{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded artifacts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ravi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:345: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "c:\\users\\ravi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:360: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '0', '.', '9', '1', '2', '8', '7', '0', '9', '2', '9', '1', '7', '5', '2', '7', '6', '9']\n",
      "[',', '0', '.', '9', '1', '2', '8', '7', '0', '9', '2', '9', '1', '7', '5', '2', '7', '6', '9']\n",
      "[',', '0', '.', '5', '7', '7', '3', '5', '0', '2', '6', '9', '1', '8', '9', '6', '2', '5', '8']\n",
      "[',', '0']\n",
      "[',', '0', '.', '7', '0', '7', '1', '0', '6', '7', '8', '1', '1', '8', '6', '5', '4', '7', '6']\n",
      "[',', '0']\n",
      "[',', '0', '.', '7', '0', '7', '1', '0', '6', '7', '8', '1', '1', '8', '6', '5', '4', '7', '6']\n",
      "[',', '0', '.', '5', '7', '7', '3', '5', '0', '2', '6', '9', '1', '8', '9', '6', '2', '5', '8']\n",
      "[',', '0', '.', '4', '0', '8', '2', '4', '8', '2', '9', '0', '4', '6', '3', '8', '6', '3', '1', '3']\n",
      "[',', '0', '.', '4', '0', '8', '2', '4', '8', '2', '9', '0', '4', '6', '3', '8', '6', '3', '1', '3']\n",
      "[',', '0', '.', '4', '0', '8', '2', '4', '8', '2', '9', '0', '4', '6', '3', '8', '6', '3', '1', '3']\n",
      "[',', '0', '.', '7', '0', '7', '1', '0', '6', '7', '8', '1', '1', '8', '6', '5', '4', '7', '6']\n",
      "[',', '0', '.', '5', '7', '7', '3', '5', '0', '2', '6', '9', '1', '8', '9', '6', '2', '5', '8']\n",
      "[',', '0', '.', '5', '7', '7', '3', '5', '0', '2', '6', '9', '1', '8', '9', '6', '2', '5', '8']\n",
      "[',', '0', '.', '5', '7', '7', '3', '5', '0', '2', '6', '9', '1', '8', '9', '6', '2', '5', '8']\n",
      "[',', '0', '.', '4', '0', '8', '2', '4', '8', '2', '9', '0', '4', '6', '3', '8', '6', '3', '1', '3']\n",
      "[',', '0', '.', '4', '0', '8', '2', '4', '8', '2', '9', '0', '4', '6', '3', '8', '6', '3', '1', '3']\n",
      "[',', '0', '.', '4', '0', '8', '2', '4', '8', '2', '9', '0', '4', '6', '3', '8', '6', '3', '1', '3']\n",
      "[',', '0', '.', '4', '0', '8', '2', '4', '8', '2', '9', '0', '4', '6', '3', '8', '6', '3', '1', '3']\n",
      "[',', '0', '.', '4', '0', '8', '2', '4', '8', '2', '9', '0', '4', '6', '3', '8', '6', '3', '1', '3']\n",
      "[',', '0', '.', '7', '0', '7', '1', '0', '6', '7', '8', '1', '1', '8', '6', '5', '4', '7', '6']\n",
      "[',', '0', '.', '4', '0', '8', '2', '4', '8', '2', '9', '0', '4', '6', '3', '8', '6', '3', '1', '3']\n",
      "[',', '0', '.', '4', '0', '8', '2', '4', '8', '2', '9', '0', '4', '6', '3', '8', '6', '3', '1', '3']\n",
      "[',', '0']\n",
      "[',', '1', '.', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ravi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:377: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "c:\\users\\ravi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\nltk\\cluster\\util.py:133: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sqrt(numpy.dot(u, u)) * sqrt(numpy.dot(v, v))))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unorderable types: list() > float()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-410f8b19250a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[0mkeywords_column_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Keywords\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[0moutput_column_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"DomainMatchScore\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m \u001b[0mrequirements_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpopulate_text_similarity_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequirements_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdomain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeywords_column_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_column_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[0moutput_column_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"DataElementsMatchScore\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-410f8b19250a>\u001b[0m in \u001b[0;36mpopulate_text_similarity_score\u001b[1;34m(artifact_df1, artifact_df2, keywords_column_name, output_column_name)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted_obj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cosine_score'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m                 \u001b[0mtop_matches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unorderable types: list() > float()"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from stop_words import get_stop_words\n",
    "import numpy\n",
    "import re\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "import websocket\n",
    "import _thread\n",
    "import time\n",
    "\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "\n",
    "\n",
    "# # 6. Data Preparation\n",
    "\n",
    "# ## 6.1 Global variables and functions\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# Name of the excel file with data in S3 Storage\n",
    "#BrdFileName = \"Banking-BRD.xlsx\"\n",
    "# Choose or get as an input as to which Domain it belongs to i.e banking, healthcare etc\n",
    "Domain = \"Banking\"\n",
    "\n",
    "# Name of the config files in Object Storage. Rule_brd will be used specifically for parsing requirement document\n",
    "configFileName = \"config/sample_config.txt\"\n",
    "BRD_configFileName = \"config/Rule_BRD.txt\"\n",
    "# Config contents\n",
    "config = None;\n",
    "\n",
    "Path = \"D:/machine learning/software-design-usingNLP-master v2/\"\n",
    "# Output excell\n",
    "\n",
    "# Requirements dataframe\n",
    "requirements_file_name = \"data/Requirements.xlsx\"\n",
    "requirements_sheet_name = \"\".join((Domain,\"-Requirements\"))\n",
    "requirements_df = None\n",
    "\n",
    "# Domain/UseCase dataframe\n",
    "domain_file_name = \"data/Domain.xlsx\"\n",
    "domain_sheet_name = \"\".join((Domain,\"-Domain\"))\n",
    "domain_df = None\n",
    "\n",
    "# DataElements dataframe\n",
    "dataelements_file_name =\"data/DataElements.xlsx\"\n",
    "dataelements_sheet_name =\"\".join((Domain,\"-Dataelements\"))\n",
    "dataelements_df = None\n",
    "\n",
    "#grammer = \"\"\"Ravi:{<VB.?>+(<TO>|<DT>|<PRP.?>|<IN>|<JJ>)*<NN.?|NNPS>+}\"\"\" \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    \"\"\" Split text into sentences.\n",
    "    \"\"\"\n",
    "    sentence_delimiters = re.compile(u'[\\\\[\\\\]\\n.!?]')\n",
    "    sentences = sentence_delimiters.split(text)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def split_into_tokens(text):\n",
    "    \"\"\" Split text into tokens.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "    \n",
    "def POS_tagging(text):\n",
    "    \"\"\" Generate Part of speech tagging of the text.\n",
    "    \"\"\"\n",
    "    POSofText = nltk.tag.pos_tag(text)\n",
    "    return POSofText\n",
    "\n",
    "\n",
    "def keyword_tagging(tag,tagtext,text):\n",
    "    \"\"\" Tag the text matching keywords.\n",
    "    \"\"\"\n",
    "    if (text.lower().find(tagtext.lower()) != -1):\n",
    "        return text[text.lower().find(tagtext.lower()):text.lower().find(tagtext.lower())+len(tagtext)]\n",
    "    else:\n",
    "        return 'UNKNOWN'\n",
    "#function not used    \n",
    "def regex_tagging(tag,regex,text):\n",
    "    \"\"\" Tag the text matching REGEX.\n",
    "    \"\"\"    \n",
    "    p = re.compile(regex, re.IGNORECASE)\n",
    "    matchtext = p.findall(text)\n",
    "    regex_list=[]    \n",
    "    if (len(matchtext)>0):\n",
    "        for regword in matchtext:\n",
    "            regex_list.append(regword)\n",
    "    return regex_list\n",
    "\n",
    "def BRD_chunk_tagging(tag,chunk,text):\n",
    "    \"\"\" Tag the text using chunking.\n",
    "    \"\"\"\n",
    "    # global grammer\n",
    "    # parsed_chink = nltk.RegexpParser(grammer)\n",
    "    # parsed_chink.parse(text).draw()\n",
    "    \n",
    "    parsed_cp = nltk.RegexpParser(chunk)\n",
    "    pos_cp = parsed_cp.parse(text)\n",
    "    #pos_cp.draw()\n",
    "    \n",
    "    #pos_cp = chunk_sentence(text) #*** use this for getting refined output after chinking but extra entities in output\n",
    "    chunk_list=[]\n",
    "    for root in pos_cp:\n",
    "        if isinstance(root, nltk.tree.Tree):               \n",
    "            if root.label() == tag:\n",
    "                chunk_word = ''\n",
    "                for child_root in root:\n",
    "                    chunk_word = chunk_word +' '+ child_root[0]\n",
    "                chunk_list.append(chunk_word)\n",
    "    #print(\"txt: \",text)\n",
    "    #print(chunk_list)\n",
    "    return chunk_list\n",
    "\n",
    "    \n",
    "def augument_SpResponse(responsejson,updateType,text,tag): # update classified text and tag in entities of response json\n",
    "    \"\"\" Update the output JSON with augumented classifications.\n",
    "    \"\"\"\n",
    "   # print(\"augument_response  response: \"+str(responsejson)+\"updateType: \"+updateType+\"text or words: \"+text+\"tag in rule brd: \"+tag )\n",
    "    if(updateType == 'keyword'):\n",
    "        if not any(d.get('text', None) == text for d in responsejson['Keywords']):\n",
    "            responsejson['Keywords'].append({\"User\":text})\n",
    "    else:\n",
    "        if not any(d.get('text', None) == text for d in responsejson['Entities']):\n",
    "            responsejson['Entities'].append({\"type\":tag,\"text\":text}) \n",
    "    #print(responsejson)\n",
    "    return responsejson\n",
    "\n",
    "def classify_BRD_text(text, config, DOC_TYPE):\n",
    "    \"\"\" Perform augumented classification of the text for BRD specifically for getting the output with action.\n",
    "    \"\"\"\n",
    "    \n",
    "    #will be used for storing initial value of response json, this is from nlu earlier\n",
    "    with open(Path+'config/output_format_BRD.json') as f:\n",
    "        responsejson = json.load(f)\n",
    "\n",
    "        tokens = split_into_tokens(text)\n",
    "\n",
    "        postags = POS_tagging(tokens)\n",
    "        #print(\"POS tags for sentence \"+str(tokens)+\"  is \"+ str(postags))\n",
    "        configjson = json.loads(config)#Rule_BRD.txt\n",
    "    \n",
    "        no_of_items = 0\n",
    "   \n",
    "    \n",
    "    for stages in configjson['configuration']['classification']['stages']:\n",
    "          #print('Stage - Performing ' + stages['name']+':')\n",
    "        for steps in stages['steps']:\n",
    "            #print(\"steps \", steps['type'])\n",
    "            if(steps['type'] == 'chunking'):\n",
    "\n",
    "                    if (DOC_TYPE=='REQ_ACTION'):\n",
    "                        tag = 'REQ_ACTION'\n",
    "                        chunktags = BRD_chunk_tagging('ACTION',steps['chunk'][0]['pattern'],postags)\n",
    "                    elif (DOC_TYPE=='DOMAIN_ACTION'):\n",
    "                        tag='DOMAIN_ACTION'\n",
    "                        chunktags = BRD_chunk_tagging('ACTION',steps['chunk'][1]['pattern'],postags)\n",
    "                    elif (DOC_TYPE=='DE_ACTION'):\n",
    "                        tag = 'DE_ACTION'\n",
    "                        chunktags = BRD_chunk_tagging('ACTION',steps['chunk'][2]['pattern'],postags)\n",
    "                    else:\n",
    "                        print(\"Unknown Document\")\n",
    "                        \n",
    "                    #print(\"chunktags \",chunktags)   \n",
    "                    if (len(chunktags)>0):\n",
    "                        for words in chunktags:\n",
    "                            #print('-------'+chunk['tag']+':'+words)\n",
    "                            responsejson= augument_SpResponse(responsejson,'ACTION',words,tag)\n",
    "                            \n",
    "            else:\n",
    "                print('UNKNOWN STEP')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return responsejson\n",
    "\n",
    "\n",
    "\n",
    "stopWords = get_stop_words('english')\n",
    "# List of words to be ignored for text similarity\n",
    "stopWords.extend([\"The\",\"This\",\"That\",\".\",\"!\",\"?\"])\n",
    "\n",
    "def compute_text_similarity(text1, text2, text1tags, text2tags):\n",
    "    \"\"\" Compute text similarity using cosine\n",
    "    \"\"\"\n",
    "    #stemming is the process for reducing inflected (or sometimes derived) words to their stem, base or root form\n",
    "    stemmer = nltk.stem.WordNetLemmatizer()\n",
    "    sentences_text1 = split_sentences(text1)\n",
    "    sentences_text2 = split_sentences(text2)\n",
    "    tokens_text1 = []\n",
    "    tokens_text2 = []\n",
    "    #print(\"sentence 1\",sentences_text1)\n",
    "    #print(\"sentence 2\",sentences_text2)\n",
    "\n",
    "    \n",
    "    for element in text1tags:\n",
    "        tokens_text1.extend(split_into_tokens(element))\n",
    "    for element in text2tags:\n",
    "        tokens_text2.extend(split_into_tokens(element))\n",
    "    \n",
    "    for sentence in sentences_text1:\n",
    "        tokenstemp = split_into_tokens(sentence.lower())\n",
    "        tokens_text1.extend(tokenstemp)\n",
    "    \n",
    "    for sentence in sentences_text2:\n",
    "        tokenstemp = split_into_tokens(sentence.lower())\n",
    "        tokens_text2.extend(tokenstemp)\n",
    "        \n",
    "       \n",
    "    if (len(text1tags) > 0):  \n",
    "        tokens_text1.extend(text1tags)\n",
    "    if (len(text2tags) > 0):    \n",
    "        tokens_text2.extend(text2tags)\n",
    "    \n",
    "    \n",
    "\n",
    "    tokens1Filtered = [stemmer.lemmatize(x)  for x in tokens_text1 if x not in stopWords]\n",
    "    \n",
    "    tokens2Filtered = [stemmer.lemmatize(x) for x in tokens_text2 if x not in stopWords]\n",
    "    \n",
    "    #  remove duplicate tokens\n",
    "    tokens1Filtered = set(tokens1Filtered)\n",
    "    tokens2Filtered = set(tokens2Filtered)\n",
    "    print(\"final tokens1 \",tokens_text1)\n",
    "    \n",
    "    print(\"final tokens2 \",tokens_text2)\n",
    "    tokensList=[]\n",
    "\n",
    "    text1vector = []\n",
    "    text2vector = []\n",
    "    \n",
    "    if len(tokens1Filtered) < len(tokens2Filtered):\n",
    "        tokensList = tokens1Filtered\n",
    "    else:\n",
    "        tokensList = tokens2Filtered\n",
    "\n",
    "    for token in tokensList:\n",
    "        if token in tokens1Filtered:\n",
    "            text1vector.append(1)\n",
    "        else:\n",
    "            text1vector.append(0)\n",
    "        if token in tokens2Filtered:\n",
    "            text2vector.append(1)\n",
    "        else:\n",
    "            text2vector.append(0)  \n",
    "\n",
    "    cosine_similarity = 1-cosine_distance(text1vector,text2vector)\n",
    "    if numpy.isnan(cosine_similarity):\n",
    "        cosine_similarity = 0\n",
    "    \n",
    "    return cosine_similarity\n",
    "             \n",
    "\n",
    "def get_file(Path):\n",
    "    dummpyfunction=\"123\"\n",
    "    \n",
    "\n",
    "\n",
    "def load_artifacts():\n",
    "    global requirements_df \n",
    "    global domain_df \n",
    "    global dataelements_df \n",
    "    global config\n",
    "    global BRD_config\n",
    "    global Path\n",
    "    \n",
    "    \n",
    "    Location = \"\".join((Path,requirements_file_name))\n",
    "    #get_file(requirements_file_name,Location)\n",
    "    excel = pd.ExcelFile(Location)\n",
    "    requirements_df = excel.parse(requirements_sheet_name)\n",
    "    Location = \"\".join((Path,domain_file_name))\n",
    "    #get_file(domain_file_name,Location)\n",
    "    excel = pd.ExcelFile(Location)\n",
    "    domain_df = excel.parse(domain_sheet_name)\n",
    "    Location = \"\".join((Path,dataelements_file_name))\n",
    "    #get_file(dataelements_file_name,Location)\n",
    "    excel = pd.ExcelFile(Location)\n",
    "    dataelements_df = excel.parse(dataelements_sheet_name)\n",
    "    rule_text = open(Path+configFileName)\n",
    "    config = rule_text.read()\n",
    "    BRD_rule_text = open(Path+BRD_configFileName)\n",
    "    BRD_config = BRD_rule_text.read()\n",
    "\n",
    "    \n",
    "def prepare_artifact_dataframes():\n",
    "    \"\"\" Prepare artifact dataframes by creating necessary output columns\n",
    "    \"\"\"\n",
    "    global requirements_df \n",
    "    global domain_df \n",
    "    global dataelements_df \n",
    "    req_cols_len = len(requirements_df.columns)\n",
    "    dom_cols_len = len(domain_df.columns)\n",
    "    dat_cols_len = len(dataelements_df.columns)\n",
    "    requirements_df.insert(req_cols_len, \"ClassifiedText\",\"\")\n",
    "    requirements_df.insert(req_cols_len+1, \"Keywords\",\"\")\n",
    "    requirements_df.insert(req_cols_len+2, \"DomainMatchScore\",\"\")\n",
    "    \n",
    "    domain_df.insert(dom_cols_len, \"ClassifiedText\",\"\")\n",
    "    domain_df.insert(dom_cols_len+1, \"Keywords\",\"\")\n",
    "    domain_df.insert(dom_cols_len+2, \"DataElementsMatchScore\",\"\")\n",
    "\n",
    "    dataelements_df.insert(dat_cols_len, \"ClassifiedText\",\"\")\n",
    "    dataelements_df.insert(dat_cols_len+1, \"Keywords\",\"\")\n",
    "    dataelements_df.insert(dat_cols_len+2, \"RequirementsMatchScore\",\"\")\n",
    "    \n",
    "\n",
    "    \n",
    "def mod_req_text_classifier_output(artifact_df, BRD_config, output_column_name,DOC_TYPE):\n",
    "    \"\"\" Add text classifier output to the artifact dataframe based on rule defined in config\n",
    "    \"\"\"\n",
    "    for index, row in artifact_df.iterrows():\n",
    "        summary = row[\"I want to <perform some task>\"]\n",
    "        modID = row[\"ID\"]\n",
    "        \n",
    "       # print(modID)\n",
    "        modID = modID.replace(\"R\",\"UC\")\n",
    "        user = row[\"As a <type of user>\"]\n",
    "        user = \"\".join((user,\" want to \"))\n",
    "        summary = \"\".join((user,summary))\n",
    "        #print(\"--------------\")\n",
    "        #print(summary)\n",
    "        classifier_journey_output = classify_BRD_text(summary, BRD_config,DOC_TYPE)\n",
    "        #print(\"classifieer ourney out\",classifier_journey_output)\n",
    "        artifact_df.set_value(index, output_column_name, classifier_journey_output)\n",
    "        \n",
    "       \n",
    "    return artifact_df \n",
    "\n",
    "\n",
    "def add_text_classifier_output(artifact_df, config, output_column_name, DOC_TYPE):\n",
    "    \"\"\" Add text classifier output to the artifact dataframe based on rule defined in config\n",
    "    \"\"\"\n",
    "    for index, row in artifact_df.iterrows():\n",
    "        summary = row[\"Description\"]\n",
    "        #print(\"--------------\")\n",
    "        #print(summary)\n",
    "        classifier_journey_output = classify_BRD_text(summary, BRD_config,DOC_TYPE)\n",
    "        #print(classifier_journey_output)\n",
    "        artifact_df.set_value(index, output_column_name, classifier_journey_output)\n",
    "    return artifact_df \n",
    "           \n",
    "def add_keywords_entities(artifact_df, classify_text_column_name, output_column_name):\n",
    "    \"\"\" Add keywords and entities to the artifact dataframe\"\"\"\n",
    "    i=0\n",
    "    for index, artifact in artifact_df.iterrows():\n",
    "        keywords_array = []\n",
    "        for row in artifact[classify_text_column_name]['Keywords']:\n",
    "            #print(\"add key word entities: classifiedtext[keywords] \",row)\n",
    "            if not row['User'] in keywords_array:\n",
    "                keywords_array.append(row['User'])\n",
    "                \n",
    "        for entities in artifact[classify_text_column_name]['Entities']:\n",
    "            #print(\"add key word entities: classifiedtext[entities] \",entities)\n",
    "            if not entities['text'] in keywords_array and entities['text']!='':\n",
    "                print(entities[text])\n",
    "                artifact_df.at(i,output_column_name) = entities[text]\n",
    "                i= i +1\n",
    "                #set_value(index, output_column_name, keywords_array)\n",
    "                #keywords_array.append(entities['text'])\n",
    "        \n",
    "        \n",
    "        #print(keywords_array)\n",
    "    return artifact_df \n",
    "\n",
    "#requirements_df, domain_df, keywords_column_name, output_column_name)\n",
    "\n",
    "def populate_text_similarity_score(artifact_df1, artifact_df2, keywords_column_name, output_column_name):\n",
    "    \"\"\" Populate text similarity score to the artifact dataframes\n",
    "    \"\"\"\n",
    "    heading1 = \"Description\"\n",
    "    heading2 = \"Description\"\n",
    "    \n",
    "    try:\n",
    "        artifact_df1[heading1]\n",
    "    except: \n",
    "        heading1 = \"I want to <perform some task>\"\n",
    "    \n",
    "    try:\n",
    "        artifact_df2[heading2]\n",
    "    except: \n",
    "        heading2 = \"I want to <perform some task>\"    \n",
    "    \n",
    "    \n",
    "    for index1, artifact1 in artifact_df1.iterrows():\n",
    "        matches = []\n",
    "        top_matches = []\n",
    "        for index2, artifact2 in artifact_df2.iterrows():\n",
    "            matches.append({'ID': artifact2['ID'], \n",
    "                            'cosine_score': 0, \n",
    "                            'SubjectID':artifact1['ID']})\n",
    "            cosine_score = compute_text_similarity(\n",
    "                #artifact1[\\'Description\\'], \n",
    "                #artifact2[\\'Description\\'], \n",
    "                artifact1[heading1],\n",
    "                artifact2[heading2],\n",
    "                artifact1['Keywords'], \n",
    "                artifact2['Keywords'])\n",
    "            matches[index2][\"cosine_score\"] = cosine_score\n",
    "       \n",
    "        sorted_obj = sorted(matches, key=lambda x : x['cosine_score'], reverse=True)\n",
    "    \n",
    "    # This is where the lower cosine value to be truncated is set and needs to be adjusted based on output\n",
    "    \n",
    "        for obj in sorted_obj:\n",
    "            if obj['cosine_score'] > 0.6:\n",
    "                top_matches.append(obj)\n",
    "               \n",
    "        artifact_df1.set_value(index1, output_column_name, top_matches)\n",
    "    return artifact_df1\n",
    "\n",
    "\n",
    "# ## 6.3 Process flow\n",
    "\n",
    "# ** Prepare data **\n",
    "# * Load artifacts from object storage and create pandas dataframes\n",
    "# * Prepare the pandas dataframes. Add additional columns required for further processing.\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "load_artifacts()\n",
    "prepare_artifact_dataframes()\n",
    "\n",
    "print(\"loaded artifacts\")\n",
    "\n",
    "# ** Run Text Classification on data **\n",
    "# * Add the text classification output to the artifact dataframes\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "DOC_TYPE = ['REQ_ACTION','DOMAIN_ACTION','DE_ACTION']\n",
    "output_column_name = \"ClassifiedText\"\n",
    "requirements_df = mod_req_text_classifier_output(requirements_df, BRD_config, output_column_name,DOC_TYPE[0])\n",
    "\n",
    "domain_df = add_text_classifier_output(domain_df,config, output_column_name,DOC_TYPE[1])\n",
    "dataelements_df = add_text_classifier_output(dataelements_df,config, output_column_name,DOC_TYPE[2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ** Populate keywords and entities **\n",
    "# * Add the keywords and entities extracted from the unstructured text to the artifact dataframes\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "classify_text_column_name = \"ClassifiedText\"\n",
    "output_column_name = \"Keywords\"\n",
    "\n",
    "\n",
    "\n",
    "requirements_df = add_keywords_entities(requirements_df, classify_text_column_name, output_column_name)\n",
    "domain_df = add_keywords_entities(domain_df, classify_text_column_name, output_column_name)\n",
    "dataelements_df = add_keywords_entities(dataelements_df, classify_text_column_name, output_column_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ** Correlate keywords between artifacts **\n",
    "# * Add the text similarity score of associated artifacts to the dataframe\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "keywords_column_name = \"Keywords\"\n",
    "output_column_name = \"DomainMatchScore\"\n",
    "requirements_df = populate_text_similarity_score(requirements_df, domain_df, keywords_column_name, output_column_name)\n",
    "\n",
    "output_column_name = \"DataElementsMatchScore\"\n",
    "domain_df = populate_text_similarity_score(domain_df, dataelements_df, keywords_column_name, output_column_name)\n",
    "\n",
    "output_column_name = \"RequirementsMatchScore\"\n",
    "dataelements_df = populate_text_similarity_score(dataelements_df, requirements_df, keywords_column_name, output_column_name)\n",
    "'''\n",
    "writer = pd.ExcelWriter(Path+'data/intermediateout.xlsx')\n",
    "requirements_df.to_excel(writer, sheet_name='Sheet1')\n",
    "domain_df.to_excel(writer, sheet_name='Sheet2')\n",
    "dataelements_df.to_excel(writer, sheet_name='Sheet3')\n",
    "writer.save()\n",
    "# # This section will be used to create the Output in excell format\n",
    "\n",
    "# In[59]:\n",
    "\n",
    "\n",
    "def extract_action(summary):\n",
    "    action_string = \"\"\n",
    "    count = 1\n",
    "    for entities in summary['Entities']:\n",
    "        #print(entities['text'])\n",
    "        if not entities['text'] in action_string:\n",
    "                action_string = action_string + entities['text']\n",
    "                count = count + 1\n",
    "                if count == 2:\n",
    "                    count = 1\n",
    "                    action_string = action_string + \",\"\n",
    "    \n",
    "    #print(action_string)\n",
    "    return action_string\n",
    "\n",
    "def lookup_use_case(temp,artifact3_df,column_name):\n",
    "    #print(artifact3_df.get_value(0,'ID'))\n",
    "    val = \"\"\n",
    "    rowNum = len(artifact3_df.index)\n",
    "    #print(rowNum)\n",
    "    for j in range(0,rowNum):\n",
    "        if temp == artifact3_df.get_value(j,'ID'):\n",
    "            val = artifact3_df.get_value(j,column_name)\n",
    "            print(val)\n",
    "    \n",
    "    return val       \n",
    "        \n",
    "    \n",
    "def extract_match(summary,no_of_matches,artifact3_df,column_name):\n",
    "    match_array_description = []\n",
    "    match_array_id = []\n",
    "    for index in range(0,no_of_matches):\n",
    "        try:\n",
    "            temp = summary[index][\"ID\"]\n",
    "            \n",
    "        except:\n",
    "            break\n",
    "    \n",
    "            \n",
    "        temp = summary[index][\"ID\"]\n",
    "        #print(temp)\n",
    "        use_case = lookup_use_case(temp,artifact3_df,column_name)\n",
    "        \n",
    "        match_array_id.append(temp)\n",
    "        #match_array_description.append(use_case + \"(\" + str(round(summary[index][\"cosine_score\"], 2)) +\")\")\n",
    "        match_array_description.append(use_case)\n",
    "        #print(use_case)\n",
    "            \n",
    "    \n",
    "    #print(\"************\")\n",
    "    #print(match_array_id)       \n",
    "    #print(\"************\")\n",
    "    return (match_array_description,match_array_id)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "def extract_action_requirements_df(artifact1_df, artifact2_df):\n",
    "    \"\"\" Add text classifier output to the artifact dataframe based on rule defined in config\n",
    "    \"\"\"\n",
    "    for index, row in artifact2_df.iterrows():\n",
    "        summary = row[\"ClassifiedText\"]\n",
    "        classifier_journey_output = extract_action(summary)\n",
    "        artifact1_df.set_value(index, 'Use Case', classifier_journey_output)\n",
    "    return artifact1_df \n",
    "\n",
    "def extract_bestmatch(artifact1_df, artifact2_df, artifact3_df, artifact4_df):\n",
    "    \"\"\" Extract best Match\n",
    "    \"\"\"\n",
    "    No_of_matches_user_function = 2\n",
    "    No_of_matches_data_elements = 5\n",
    "    best_match_output_domain_function = []\n",
    "    best_match_output_dataelement_function = []\n",
    "    print(\"in extract match\")\n",
    "     \n",
    "    for index, row in artifact2_df.iterrows():\n",
    "        temp1=\"\"\n",
    "        summary = row[\"DomainMatchScore\"]\n",
    "       \n",
    "        #print(summary)\n",
    "        (best_match_output_domain_function,best_match_output_domain_id) = extract_match(summary, No_of_matches_user_function, artifact3_df,\"User Function\")\n",
    "        #print(best_match_output_domain_id)\n",
    "        artifact1_df.set_value(index, 'Functionality', best_match_output_domain_function)\n",
    "        print(\"************\")\n",
    "        for index2 in best_match_output_domain_id:\n",
    "            #print(index2)\n",
    "        \n",
    "            row_domain = len(artifact3_df.index)\n",
    "            for p in range(0,row_domain):\n",
    "                if index2 == artifact3_df.get_value(p,'ID'):\n",
    "                    dataelement_summary = artifact3_df.get_value(p,'DataElementsMatchScore')\n",
    "                    #print(dataelement_summary)\n",
    "                    #print(\"------\")\n",
    "                    (best_match_output_dataelement_function,best_match_output_dataelement_id) = extract_match(dataelement_summary, No_of_matches_data_elements, artifact4_df, \"Short\")\n",
    "            temp1 = temp1 +','+ str(best_match_output_dataelement_function)\n",
    "            #print(\"best elemenets \",temp1)\n",
    "        \n",
    "        \n",
    "\n",
    "        artifact1_df.set_value(index, 'Attributes', temp1)\n",
    "        \n",
    "    return artifact1_df \n",
    "\n",
    "\n",
    "# In[60]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "no_of_rows_brd = len(requirements_df.index)\n",
    "\n",
    "index = range(0,no_of_rows_brd)\n",
    "columns = ['User','Use Case', 'Functionality', 'Attributes']\n",
    "\n",
    "\n",
    "SimMean = pd.DataFrame(index=index, columns=columns)\n",
    "SimMean.loc[0:no_of_rows_brd,'User'] = requirements_df.loc[0:no_of_rows_brd,'As a <type of user>'].values\n",
    "SimMean = extract_action_requirements_df(SimMean,requirements_df)\n",
    "SimMean = extract_bestmatch(SimMean,requirements_df,domain_df,dataelements_df)\n",
    "#SimMean = extract_bestmatch_domaintodataelem(SimMean,domain_df)\n",
    "SimMean.get_value(0,\"Attributes\")\n",
    "#print(SimMean)\n",
    "\n",
    "writer = pd.ExcelWriter(Path+'data/final_output_banking_2.xlsx')\n",
    "SimMean.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
