{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded artifacts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ravi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:339: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "c:\\users\\ravi\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py:354: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities:   deposit cheque\n",
      "entities:   withdraw cash\n",
      "entities:   transfer money\n",
      "entities:   pay my utility bills\n",
      "entities:   apply for a loan\n",
      "entities:   request for check books\n",
      "entities:   restock sufficient cash\n",
      "entities:   limit the cash withdrawl\n",
      "entities:   review all transactions\n",
      "entities:   review the credit history\n",
      "entities:   apply for loans\n",
      "entities:   operate locker\n",
      "     ID As a <type of user>  \\\n",
      "0   R01            Customer   \n",
      "1   R02            Customer   \n",
      "2   R03            Customer   \n",
      "3   R04            Customer   \n",
      "4   R05            Customer   \n",
      "5   R06              Banker   \n",
      "6   R07              Banker   \n",
      "7   R08              Banker   \n",
      "8   R09              Banker   \n",
      "9   R10              Banker   \n",
      "10  R11              Banker   \n",
      "11  NaN                 NaN   \n",
      "\n",
      "                        I want to <perform some task>  \\\n",
      "0   deposit cheque in customer name in customer ac...   \n",
      "1                           withdraw cash from an ATM   \n",
      "2   want to transfer money from one account to ano...   \n",
      "3                         pay my utility bills online   \n",
      "4                                    apply for a loan   \n",
      "5                             request for check books   \n",
      "6             restock sufficient cash in ATM machines   \n",
      "7          limit the cash withdrawl from ATM machines   \n",
      "8        I want to review all transactions of the day   \n",
      "9   review the credit history and the bank balance...   \n",
      "10                                     operate locker   \n",
      "11                                                NaN   \n",
      "\n",
      "                    so that I can <achieve some goal>  \\\n",
      "0                  I want to increase my bank balance   \n",
      "1            I don't have to wait in line at the Bank   \n",
      "2            I don't need to pay the amount in person   \n",
      "3   I don't need to write checks or use postal ser...   \n",
      "4                                      purchase a car   \n",
      "5           I can write the required number of checks   \n",
      "6                     the customers can withdraw cash   \n",
      "7   more customers can prevail the ATM cash and al...   \n",
      "8           I can debit and credit the right accounts   \n",
      "9       I can approve or reject the loan applications   \n",
      "10                         so that I can submit money   \n",
      "11                                                NaN   \n",
      "\n",
      "                                       ClassifiedText  \\\n",
      "0   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "1   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "2   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "3   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "4   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "5   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "6   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "7   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "8   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "9   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "10  {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "11                                                NaN   \n",
      "\n",
      "                                             Keywords DomainMatchScore  \n",
      "0       [ , d, e, p, o, s, i, t,  , c, h, e, q, u, e]                   \n",
      "1          [ , w, i, t, h, d, r, a, w,  , c, a, s, h]                   \n",
      "2       [ , t, r, a, n, s, f, e, r,  , m, o, n, e, y]                   \n",
      "3   [ , p, a, y,  , m, y,  , u, t, i, l, i, t, y, ...                   \n",
      "4   [ , a, p, p, l, y,  , f, o, r,  , a,  , l, o, ...                   \n",
      "5   [ , r, e, q, u, e, s, t,  , f, o, r,  , c, h, ...                   \n",
      "6   [ , r, e, s, t, o, c, k,  , s, u, f, f, i, c, ...                   \n",
      "7   [ , l, i, m, i, t,  , t, h, e,  , c, a, s, h, ...                   \n",
      "8   [ , r, e, v, i, e, w,  , a, l, l,  , t, r, a, ...                   \n",
      "9   [ , r, e, v, i, e, w,  , t, h, e,  , c, r, e, ...                   \n",
      "10   [ , a, p, p, l, y,  , f, o, r,  , l, o, a, n, s]                   \n",
      "11      [ , o, p, e, r, a, t, e,  , l, o, c, k, e, r]              NaN  \n",
      "entities:   entered by Customer\n",
      "entities:   withdrawing money\n",
      "entities:   checking balance\n",
      "entities:   depositing cheque\n",
      "entities:   are Account number\n",
      "entities:   allows a customer\n",
      "entities:   view the account balance information\n",
      "entities:   view transaction history\n",
      "entities:   request for account\n",
      "entities:   be logged in the internet bank\n",
      "entities:   required for function\n",
      "entities:   is account number\n",
      "entities:   transfering money\n",
      "entities:   require source account number\n",
      "entities:   account type\n",
      "entities:   manages actual process\n",
      "entities:   withdrawing cash\n",
      "entities:   based on bill type\n",
      "entities:   be withdrawn from the ATM\n",
      "entities:   is required if a customer\n",
      "entities:   apply for loan\n",
      "entities:   requires type\n",
      "entities:   account number\n",
      "entities:   is required along with loan amount\n",
      "entities:   be used by banker\n",
      "entities:   fill the cash\n",
      "entities:   checking credit history\n",
      "entities:   applying for loan\n",
      "entities:   is collected from external credit rating agency\n",
      "entities:   is the functionality\n",
      "entities:   enquiries cheque status\n",
      "entities:   allows customer\n",
      "entities:   stop cheque payment\n",
      "entities:   be delivered at home .The customer\n",
      "entities:   be logged into Banking System\n",
      "entities:   display his/her payment histor\n",
      "entities:   review old transactions\n",
      "entities:   click on Bill Payment History\n",
      "entities:   display the transaction\n",
      "entities:   is used by banker\n",
      "entities:   limit the amount\n",
      "entities:   given time\n",
      "entities:   invoke restock cash\n",
      "entities:   be able into the system\n",
      "entities:   be for a customer\n",
      "entities:   allows a customer\n",
      "entities:   view the minute balance information\n",
      "entities:   view transaction history\n",
      "entities:   request for account\n",
      "entities:   be logged in the internet banking\n",
      "entities:   be logged into Banking System\n",
      "entities:   Transfer Funds\n",
      "entities:   allows customer\n",
      "entities:   transfer funds\n",
      "entities:   â€“ own personal accounts\n",
      "entities:   Requested transfer\n",
      "entities:   take place\n",
      "entities:   specified by customer\n",
      "entities:   delete the account details\n",
      "entities:   are recorded in a table\n",
      "entities:   is any funds\n",
      "entities:   be logged into Banking System\n",
      "entities:   make payments\n",
      "entities:   include utilities\n",
      "entities:   use Online Pay Bill service\n",
      "entities:   pay bills\n",
      "entities:   debiting their account\n",
      "entities:   payee corporations\n",
      "entities:   has registered with internet banking\n",
      "entities:   using the Registered Bill\n",
      "entities:   are Enquiry Future Payment Status\n",
      "entities:   lets customer\n",
      "entities:   has scheduled any future payments\n",
      "entities:   lets customer cancels\n",
      "entities:   scheduled future payments\n",
      "entities:   be logged into Banking System.The customer\n",
      "entities:   enquiries cheque status\n",
      "entities:   allows customer\n",
      "entities:   stop cheque payment\n",
      "entities:   request for a cheque book online\n",
      "entities:   be logged into Banking System\n",
      "entities:   allows customer\n",
      "entities:   change password\n",
      "entities:   change the online\n",
      "entities:   is retained by the internet banking system\n",
      "entities:   cancel the ATM facilities\n",
      "entities:   be logged into Banking System\n",
      "entities:   logged in user finishes\n",
      "entities:   abuse his username\n",
      "entities:   is that person\n",
      "entities:   editing for the internet banking system\n",
      "entities:   check the transactions\n",
      "entities:   be valid user\n",
      "entities:   have ausername\n",
      "entities:   allows a customer\n",
      "entities:   pay Immediate\n",
      "entities:   have registered the corportations\n",
      "entities:   allows a customer\n",
      "entities:   pay Immediate\n",
      "entities:   provided Enter\n",
      "entities:   click â€™ Register\n",
      "entities:   appear the confirm message\n",
      "entities:   confirm the transaction\n",
      "entities:   Tick the box\n",
      "entities:   delete from the list\n",
      "entities:   appear message confirmation\n",
      "entities:   confirm the transaction\n",
      "entities:   creates an investigation case\n",
      "entities:   states that the fund\n",
      "entities:   transferred to Bank B\n",
      "entities:   obtains transaction details\n",
      "entities:   dealswith deposit cheque\n",
      "entities:   are Account number\n",
      "entities:   account type\n",
      "       ID              User Function  \\\n",
      "0     Fn1                 Verify PIN   \n",
      "1     Fn2              View Account    \n",
      "2     Fn3             Transfer Money   \n",
      "3     Fn4              Withdraw cash   \n",
      "4     Fn5                 Apply loan   \n",
      "5     Fn6               Restock cash   \n",
      "6     Fn7               Credit_check   \n",
      "7     Fn8            Cheque Services   \n",
      "8     Fn9        Review transactions   \n",
      "9    Fn10                 Limit Cash   \n",
      "10   Fn11                      Login   \n",
      "11   Fn12              View Account    \n",
      "12   Fn13            Transfer Funds    \n",
      "13   Fn14                 Pay Bills    \n",
      "14   Fn15            Cheque Services   \n",
      "15   Fn16                    Utility   \n",
      "16   Fn17                     Logout   \n",
      "17   Fn18              Administrator   \n",
      "18   Fn19     Pay Registered Payment   \n",
      "19   Fn20               Open Payment   \n",
      "20   Fn21      Pay Registration Bill   \n",
      "21   Fn22  Delete registration Bill    \n",
      "22   Fn23         fraudInvestigation   \n",
      "23   Fn24        Transaction Details   \n",
      "24   Fn25             Deposit Cheque   \n",
      "25    NaN                        NaN   \n",
      "26    NaN                        NaN   \n",
      "27    NaN                        NaN   \n",
      "28    NaN                        NaN   \n",
      "29    NaN                        NaN   \n",
      "..    ...                        ...   \n",
      "84    NaN                        NaN   \n",
      "85    NaN                        NaN   \n",
      "86    NaN                        NaN   \n",
      "87    NaN                        NaN   \n",
      "88    NaN                        NaN   \n",
      "89    NaN                        NaN   \n",
      "90    NaN                        NaN   \n",
      "91    NaN                        NaN   \n",
      "92    NaN                        NaN   \n",
      "93    NaN                        NaN   \n",
      "94    NaN                        NaN   \n",
      "95    NaN                        NaN   \n",
      "96    NaN                        NaN   \n",
      "97    NaN                        NaN   \n",
      "98    NaN                        NaN   \n",
      "99    NaN                        NaN   \n",
      "100   NaN                        NaN   \n",
      "101   NaN                        NaN   \n",
      "102   NaN                        NaN   \n",
      "103   NaN                        NaN   \n",
      "104   NaN                        NaN   \n",
      "105   NaN                        NaN   \n",
      "106   NaN                        NaN   \n",
      "107   NaN                        NaN   \n",
      "108   NaN                        NaN   \n",
      "109   NaN                        NaN   \n",
      "110   NaN                        NaN   \n",
      "111   NaN                        NaN   \n",
      "112   NaN                        NaN   \n",
      "113   NaN                        NaN   \n",
      "\n",
      "                                           Description  \\\n",
      "0    Function deals with Verification of PIN entere...   \n",
      "1    View Account allows a customer to view the acc...   \n",
      "2    This use case is intended for transfering mone...   \n",
      "3    This function manages actual process of withdr...   \n",
      "4    This function is required if a customer is wil...   \n",
      "5    Function will be used by banker to fill the ca...   \n",
      "6    Function deals with checking credit history of...   \n",
      "7    Cheque service is the functionality by which t...   \n",
      "8    If the customer wants to display his/her payme...   \n",
      "9    Limit cash withdrawl function is used by banke...   \n",
      "10   A customer to be able into the system, he/she ...   \n",
      "11   View Account allows a customer to view the min...   \n",
      "12   The customer must be logged into Banking Syste...   \n",
      "13   The customer most be logged into Banking Syste...   \n",
      "14   The customer most be logged into Banking Syste...   \n",
      "15   The customer most be logged into Banking Syste...   \n",
      "16   The customer must be logged into Banking Syste...   \n",
      "17   An administrator is that person who makes some...   \n",
      "18   This function allows a customer to pay Immedia...   \n",
      "19   This function allows a customer to pay Immedia...   \n",
      "20   Select Corporation Name from the list provided...   \n",
      "21   When the customer selects and clicks on Deregi...   \n",
      "22   Bank A creates an investigation case when the ...   \n",
      "23   Bank B obtains transaction details of the payment   \n",
      "24   function dealswith deposit cheque at ATM or br...   \n",
      "25                                                 NaN   \n",
      "26                                                 NaN   \n",
      "27                                                 NaN   \n",
      "28                                                 NaN   \n",
      "29                                                 NaN   \n",
      "..                                                 ...   \n",
      "84                                                 NaN   \n",
      "85                                                 NaN   \n",
      "86                                                 NaN   \n",
      "87                                                 NaN   \n",
      "88                                                 NaN   \n",
      "89                                                 NaN   \n",
      "90                                                 NaN   \n",
      "91                                                 NaN   \n",
      "92                                                 NaN   \n",
      "93                                                 NaN   \n",
      "94                                                 NaN   \n",
      "95                                                 NaN   \n",
      "96                                                 NaN   \n",
      "97                                                 NaN   \n",
      "98                                                 NaN   \n",
      "99                                                 NaN   \n",
      "100                                                NaN   \n",
      "101                                                NaN   \n",
      "102                                                NaN   \n",
      "103                                                NaN   \n",
      "104                                                NaN   \n",
      "105                                                NaN   \n",
      "106                                                NaN   \n",
      "107                                                NaN   \n",
      "108                                                NaN   \n",
      "109                                                NaN   \n",
      "110                                                NaN   \n",
      "111                                                NaN   \n",
      "112                                                NaN   \n",
      "113                                                NaN   \n",
      "\n",
      "                                        ClassifiedText  \\\n",
      "0    {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "1    {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "2    {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "3    {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "4    {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "5    {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "6    {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "7    {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "8    {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "9    {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "10   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "11   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "12   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "13   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "14   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "15   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "16   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "17   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "18   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "19   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "20   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "21   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "22   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "23   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "24   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "25                                                 NaN   \n",
      "26                                                 NaN   \n",
      "27                                                 NaN   \n",
      "28                                                 NaN   \n",
      "29                                                 NaN   \n",
      "..                                                 ...   \n",
      "84                                                 NaN   \n",
      "85                                                 NaN   \n",
      "86                                                 NaN   \n",
      "87                                                 NaN   \n",
      "88                                                 NaN   \n",
      "89                                                 NaN   \n",
      "90                                                 NaN   \n",
      "91                                                 NaN   \n",
      "92                                                 NaN   \n",
      "93                                                 NaN   \n",
      "94                                                 NaN   \n",
      "95                                                 NaN   \n",
      "96                                                 NaN   \n",
      "97                                                 NaN   \n",
      "98                                                 NaN   \n",
      "99                                                 NaN   \n",
      "100                                                NaN   \n",
      "101                                                NaN   \n",
      "102                                                NaN   \n",
      "103                                                NaN   \n",
      "104                                                NaN   \n",
      "105                                                NaN   \n",
      "106                                                NaN   \n",
      "107                                                NaN   \n",
      "108                                                NaN   \n",
      "109                                                NaN   \n",
      "110                                                NaN   \n",
      "111                                                NaN   \n",
      "112                                                NaN   \n",
      "113                                                NaN   \n",
      "\n",
      "                                              Keywords DataElementsMatchScore  \n",
      "0    [ , e, n, t, e, r, e, d,  , b, y,  , C, u, s, ...                         \n",
      "1    [ , w, i, t, h, d, r, a, w, i, n, g,  , m, o, ...                         \n",
      "2    [ , c, h, e, c, k, i, n, g,  , b, a, l, a, n, ...                         \n",
      "3    [ , d, e, p, o, s, i, t, i, n, g,  , c, h, e, ...                         \n",
      "4    [ , a, r, e,  , A, c, c, o, u, n, t,  , n, u, ...                         \n",
      "5    [ , a, l, l, o, w, s,  , a,  , c, u, s, t, o, ...                         \n",
      "6    [ , v, i, e, w,  , t, h, e,  , a, c, c, o, u, ...                         \n",
      "7    [ , v, i, e, w,  , t, r, a, n, s, a, c, t, i, ...                         \n",
      "8    [ , r, e, q, u, e, s, t,  , f, o, r,  , a, c, ...                         \n",
      "9    [ , b, e,  , l, o, g, g, e, d,  , i, n,  , t, ...                         \n",
      "10   [ , r, e, q, u, i, r, e, d,  , f, o, r,  , f, ...                         \n",
      "11   [ , i, s,  , a, c, c, o, u, n, t,  , n, u, m, ...                         \n",
      "12   [ , t, r, a, n, s, f, e, r, i, n, g,  , m, o, ...                         \n",
      "13   [ , r, e, q, u, i, r, e,  , s, o, u, r, c, e, ...                         \n",
      "14             [ , a, c, c, o, u, n, t,  , t, y, p, e]                         \n",
      "15   [ , m, a, n, a, g, e, s,  , a, c, t, u, a, l, ...                         \n",
      "16   [ , w, i, t, h, d, r, a, w, i, n, g,  , c, a, ...                         \n",
      "17   [ , b, a, s, e, d,  , o, n,  , b, i, l, l,  , ...                         \n",
      "18   [ , b, e,  , w, i, t, h, d, r, a, w, n,  , f, ...                         \n",
      "19   [ , i, s,  , r, e, q, u, i, r, e, d,  , i, f, ...                         \n",
      "20       [ , a, p, p, l, y,  , f, o, r,  , l, o, a, n]                         \n",
      "21          [ , r, e, q, u, i, r, e, s,  , t, y, p, e]                         \n",
      "22       [ , a, c, c, o, u, n, t,  , n, u, m, b, e, r]                         \n",
      "23   [ , i, s,  , r, e, q, u, i, r, e, d,  , a, l, ...                         \n",
      "24   [ , b, e,  , u, s, e, d,  , b, y,  , b, a, n, ...                         \n",
      "25          [ , f, i, l, l,  , t, h, e,  , c, a, s, h]                    NaN  \n",
      "26   [ , c, h, e, c, k, i, n, g,  , c, r, e, d, i, ...                    NaN  \n",
      "27   [ , a, p, p, l, y, i, n, g,  , f, o, r,  , l, ...                    NaN  \n",
      "28   [ , i, s,  , c, o, l, l, e, c, t, e, d,  , f, ...                    NaN  \n",
      "29   [ , i, s,  , t, h, e,  , f, u, n, c, t, i, o, ...                    NaN  \n",
      "..                                                 ...                    ...  \n",
      "84   [ , i, s,  , r, e, t, a, i, n, e, d,  , b, y, ...                    NaN  \n",
      "85   [ , c, a, n, c, e, l,  , t, h, e,  , A, T, M, ...                    NaN  \n",
      "86   [ , b, e,  , l, o, g, g, e, d,  , i, n, t, o, ...                    NaN  \n",
      "87   [ , l, o, g, g, e, d,  , i, n,  , u, s, e, r, ...                    NaN  \n",
      "88   [ , a, b, u, s, e,  , h, i, s,  , u, s, e, r, ...                    NaN  \n",
      "89       [ , i, s,  , t, h, a, t,  , p, e, r, s, o, n]                    NaN  \n",
      "90   [ , e, d, i, t, i, n, g,  , f, o, r,  , t, h, ...                    NaN  \n",
      "91   [ , c, h, e, c, k,  , t, h, e,  , t, r, a, n, ...                    NaN  \n",
      "92          [ , b, e,  , v, a, l, i, d,  , u, s, e, r]                    NaN  \n",
      "93       [ , h, a, v, e,  , a, u, s, e, r, n, a, m, e]                    NaN  \n",
      "94   [ , a, l, l, o, w, s,  , a,  , c, u, s, t, o, ...                    NaN  \n",
      "95          [ , p, a, y,  , I, m, m, e, d, i, a, t, e]                    NaN  \n",
      "96   [ , h, a, v, e,  , r, e, g, i, s, t, e, r, e, ...                    NaN  \n",
      "97   [ , a, l, l, o, w, s,  , a,  , c, u, s, t, o, ...                    NaN  \n",
      "98          [ , p, a, y,  , I, m, m, e, d, i, a, t, e]                    NaN  \n",
      "99       [ , p, r, o, v, i, d, e, d,  , E, n, t, e, r]                    NaN  \n",
      "100  [ , c, l, i, c, k,  , â€™,  , R, e, g, i, s, t, ...                    NaN  \n",
      "101  [ , a, p, p, e, a, r,  , t, h, e,  , c, o, n, ...                    NaN  \n",
      "102  [ , c, o, n, f, i, r, m,  , t, h, e,  , t, r, ...                    NaN  \n",
      "103            [ , T, i, c, k,  , t, h, e,  , b, o, x]                    NaN  \n",
      "104  [ , d, e, l, e, t, e,  , f, r, o, m,  , t, h, ...                    NaN  \n",
      "105  [ , a, p, p, e, a, r,  , m, e, s, s, a, g, e, ...                    NaN  \n",
      "106  [ , c, o, n, f, i, r, m,  , t, h, e,  , t, r, ...                    NaN  \n",
      "107  [ , c, r, e, a, t, e, s,  , a, n,  , i, n, v, ...                    NaN  \n",
      "108  [ , s, t, a, t, e, s,  , t, h, a, t,  , t, h, ...                    NaN  \n",
      "109  [ , t, r, a, n, s, f, e, r, r, e, d,  , t, o, ...                    NaN  \n",
      "110  [ , o, b, t, a, i, n, s,  , t, r, a, n, s, a, ...                    NaN  \n",
      "111  [ , d, e, a, l, s, w, i, t, h,  , d, e, p, o, ...                    NaN  \n",
      "112  [ , a, r, e,  , A, c, c, o, u, n, t,  , n, u, ...                    NaN  \n",
      "113            [ , a, c, c, o, u, n, t,  , t, y, p, e]                    NaN  \n",
      "\n",
      "[114 rows x 6 columns]\n",
      "entities:   be used for verification\n",
      "entities:   is available in the account\n",
      "entities:   be required as input\n",
      "entities:   withdrawing amount\n",
      "entities:   transfering money\n",
      "entities:   transfering money\n",
      "entities:   be used for deposit\n",
      "entities:   specifies the type\n",
      "entities:   be withdrawn from ATM\n",
      "entities:   provides the maximum limit\n",
      "entities:   be withdrawn from ATM\n",
      "entities:   denotes the amount\n",
      "entities:   be transferred to another account\n",
      "entities:   denotes the amount\n",
      "entities:   be withdrawn from the account\n",
      "entities:   provides the addresss\n",
      "entities:   applying loan\n",
      "entities:   signifies the purpose\n",
      "entities:   taking loan\n",
      "entities:   cbuying new car\n",
      "entities:   approving loan\n",
      "entities:   perform credit check score\n",
      "entities:   is obtained from external agencies\n",
      "      ID         Short                                        Description  \\\n",
      "0    DE1       Cus_Nme  Customer Name  and will be used for verificati...   \n",
      "1    DE2       Acc_num  Account number of the customer will be used to...   \n",
      "2    DE3     Amt_avail  This number signifies how much money is availa...   \n",
      "3    DE4     Debit_pin  Debit Card Number will be required as input fo...   \n",
      "4    DE5  From_AcctNum  From Account Number will be required for trans...   \n",
      "5    DE6    To_AcctNum  To Account Number will be required for transfe...   \n",
      "6    DE7   Amt_deposit  this specifies how much amount is to be deposited   \n",
      "7    DE8      Acc_type  Account Type will specify which type of accoun...   \n",
      "8    DE9     Bill_type  This specifies the type of bills that will be ...   \n",
      "9   DE10     Max_limit  This element provides the maximum limit of cas...   \n",
      "10  DE11    Amt_trnsfr  This amount denotes the amount to be transferr...   \n",
      "11  DE12      Amt_wdrl  This  denotes the amount to be withdrawn from ...   \n",
      "12  DE13     Cust_Addr  This provides the addresss for any mail commun...   \n",
      "13  DE14      Loan_Amt  How much loan amount customer is asking. For a...   \n",
      "14  DE15     Loan_purp  This signifies the purpose of taking loan like...   \n",
      "15  DE16    Cred_Score  For approving loan one has to perform credit c...   \n",
      "16   NaN           NaN                                                NaN   \n",
      "17   NaN           NaN                                                NaN   \n",
      "18   NaN           NaN                                                NaN   \n",
      "19   NaN           NaN                                                NaN   \n",
      "20   NaN           NaN                                                NaN   \n",
      "21   NaN           NaN                                                NaN   \n",
      "22   NaN           NaN                                                NaN   \n",
      "\n",
      "                                       ClassifiedText  \\\n",
      "0   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "1   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "2   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "3   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "4   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "5   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "6   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "7   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "8   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "9   {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "10  {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "11  {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "12  {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "13  {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "14  {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "15  {'Keywords': [{'User': ''}], 'Entities': [{'ty...   \n",
      "16                                                NaN   \n",
      "17                                                NaN   \n",
      "18                                                NaN   \n",
      "19                                                NaN   \n",
      "20                                                NaN   \n",
      "21                                                NaN   \n",
      "22                                                NaN   \n",
      "\n",
      "                                             Keywords RequirementsMatchScore  \n",
      "0   [ , b, e,  , u, s, e, d,  , f, o, r,  , v, e, ...                         \n",
      "1   [ , i, s,  , a, v, a, i, l, a, b, l, e,  , i, ...                         \n",
      "2   [ , b, e,  , r, e, q, u, i, r, e, d,  , a, s, ...                         \n",
      "3   [ , w, i, t, h, d, r, a, w, i, n, g,  , a, m, ...                         \n",
      "4   [ , t, r, a, n, s, f, e, r, i, n, g,  , m, o, ...                         \n",
      "5   [ , t, r, a, n, s, f, e, r, i, n, g,  , m, o, ...                         \n",
      "6   [ , b, e,  , u, s, e, d,  , f, o, r,  , d, e, ...                         \n",
      "7   [ , s, p, e, c, i, f, i, e, s,  , t, h, e,  , ...                         \n",
      "8   [ , b, e,  , w, i, t, h, d, r, a, w, n,  , f, ...                         \n",
      "9   [ , p, r, o, v, i, d, e, s,  , t, h, e,  , m, ...                         \n",
      "10  [ , b, e,  , w, i, t, h, d, r, a, w, n,  , f, ...                         \n",
      "11  [ , d, e, n, o, t, e, s,  , t, h, e,  , a, m, ...                         \n",
      "12  [ , b, e,  , t, r, a, n, s, f, e, r, r, e, d, ...                         \n",
      "13  [ , d, e, n, o, t, e, s,  , t, h, e,  , a, m, ...                         \n",
      "14  [ , b, e,  , w, i, t, h, d, r, a, w, n,  , f, ...                         \n",
      "15  [ , p, r, o, v, i, d, e, s,  , t, h, e,  , a, ...                         \n",
      "16         [ , a, p, p, l, y, i, n, g,  , l, o, a, n]                    NaN  \n",
      "17  [ , s, i, g, n, i, f, i, e, s,  , t, h, e,  , ...                    NaN  \n",
      "18               [ , t, a, k, i, n, g,  , l, o, a, n]                    NaN  \n",
      "19   [ , c, b, u, y, i, n, g,  , n, e, w,  , c, a, r]                    NaN  \n",
      "20      [ , a, p, p, r, o, v, i, n, g,  , l, o, a, n]                    NaN  \n",
      "21  [ , p, e, r, f, o, r, m,  , c, r, e, d, i, t, ...                    NaN  \n",
      "22  [ , i, s,  , o, b, t, a, i, n, e, d,  , f, r, ...                    NaN  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-acc86ca92999>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[0mkeywords_column_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Keywords\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[0moutput_column_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"DomainMatchScore\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m \u001b[0mrequirements_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpopulate_text_similarity_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequirements_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdomain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeywords_column_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_column_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[0moutput_column_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"DataElementsMatchScore\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-acc86ca92999>\u001b[0m in \u001b[0;36mpopulate_text_similarity_score\u001b[1;34m(artifact_df1, artifact_df2, keywords_column_name, output_column_name)\u001b[0m\n\u001b[0;32m    411\u001b[0m                 \u001b[0martifact2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mheading2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m                 \u001b[0martifact1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Keywords'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m                 artifact2['Keywords'])\n\u001b[0m\u001b[0;32m    414\u001b[0m             \u001b[0mmatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"cosine_score\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-acc86ca92999>\u001b[0m in \u001b[0;36mcompute_text_similarity\u001b[1;34m(text1, text2, text1tags, text2tags)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[0msentences_text1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m     \u001b[0msentences_text2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m     \u001b[0mtokens_text1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[0mtokens_text2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-acc86ca92999>\u001b[0m in \u001b[0;36msplit_sentences\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m     68\u001b[0m     \u001b[0msentence_delimiters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'[\\\\[\\\\]\\n.!?]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence_delimiters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from stop_words import get_stop_words\n",
    "import numpy\n",
    "import re\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "import websocket\n",
    "import _thread\n",
    "import time\n",
    "\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "\n",
    "\n",
    "# # 6. Data Preparation\n",
    "\n",
    "# ## 6.1 Global variables and functions\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# Name of the excel file with data in S3 Storage\n",
    "#BrdFileName = \"Banking-BRD.xlsx\"\n",
    "# Choose or get as an input as to which Domain it belongs to i.e banking, healthcare etc\n",
    "Domain = \"Banking\"\n",
    "\n",
    "# Name of the config files in Object Storage. Rule_brd will be used specifically for parsing requirement document\n",
    "configFileName = \"config/sample_config.txt\"\n",
    "BRD_configFileName = \"config/Rule_BRD.txt\"\n",
    "# Config contents\n",
    "config = None;\n",
    "\n",
    "Path = \"D:/machine learning/software-design-usingNLP-master v2/\"\n",
    "# Output excell\n",
    "\n",
    "# Requirements dataframe\n",
    "requirements_file_name = \"data/Requirements.xlsx\"\n",
    "requirements_sheet_name = \"\".join((Domain,\"-Requirements\"))\n",
    "requirements_df = None\n",
    "\n",
    "# Domain/UseCase dataframe\n",
    "domain_file_name = \"data/Domain.xlsx\"\n",
    "domain_sheet_name = \"\".join((Domain,\"-Domain\"))\n",
    "domain_df = None\n",
    "\n",
    "# DataElements dataframe\n",
    "dataelements_file_name =\"data/DataElements.xlsx\"\n",
    "dataelements_sheet_name =\"\".join((Domain,\"-Dataelements\"))\n",
    "dataelements_df = None\n",
    "\n",
    "#grammer = \"\"\"Ravi:{<VB.?>+(<TO>|<DT>|<PRP.?>|<IN>|<JJ>)*<NN.?|NNPS>+}\"\"\" \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    \"\"\" Split text into sentences.\n",
    "    \"\"\"\n",
    "    sentence_delimiters = re.compile(u'[\\\\[\\\\]\\n.!?]')\n",
    "    sentences = sentence_delimiters.split(text)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def split_into_tokens(text):\n",
    "    \"\"\" Split text into tokens.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "    \n",
    "def POS_tagging(text):\n",
    "    \"\"\" Generate Part of speech tagging of the text.\n",
    "    \"\"\"\n",
    "    POSofText = nltk.tag.pos_tag(text)\n",
    "    return POSofText\n",
    "\n",
    "\n",
    "def keyword_tagging(tag,tagtext,text):\n",
    "    \"\"\" Tag the text matching keywords.\n",
    "    \"\"\"\n",
    "    if (text.lower().find(tagtext.lower()) != -1):\n",
    "        return text[text.lower().find(tagtext.lower()):text.lower().find(tagtext.lower())+len(tagtext)]\n",
    "    else:\n",
    "        return 'UNKNOWN'\n",
    "#function not used    \n",
    "def regex_tagging(tag,regex,text):\n",
    "    \"\"\" Tag the text matching REGEX.\n",
    "    \"\"\"    \n",
    "    p = re.compile(regex, re.IGNORECASE)\n",
    "    matchtext = p.findall(text)\n",
    "    regex_list=[]    \n",
    "    if (len(matchtext)>0):\n",
    "        for regword in matchtext:\n",
    "            regex_list.append(regword)\n",
    "    return regex_list\n",
    "\n",
    "def BRD_chunk_tagging(tag,chunk,text):\n",
    "    \"\"\" Tag the text using chunking.\n",
    "    \"\"\"\n",
    "    # global grammer\n",
    "    # parsed_chink = nltk.RegexpParser(grammer)\n",
    "    # parsed_chink.parse(text).draw()\n",
    "    \n",
    "    parsed_cp = nltk.RegexpParser(chunk)\n",
    "    pos_cp = parsed_cp.parse(text)\n",
    "    #pos_cp.draw()\n",
    "    \n",
    "    #pos_cp = chunk_sentence(text) #*** use this for getting refined output after chinking but extra entities in output\n",
    "    chunk_list=[]\n",
    "    for root in pos_cp:\n",
    "        if isinstance(root, nltk.tree.Tree):               \n",
    "            if root.label() == tag:\n",
    "                chunk_word = ''\n",
    "                for child_root in root:\n",
    "                    chunk_word = chunk_word +' '+ child_root[0]\n",
    "                chunk_list.append(chunk_word)\n",
    "    #print(\"txt: \",text)\n",
    "    #print(chunk_list)\n",
    "    return chunk_list\n",
    "\n",
    "    \n",
    "def augument_SpResponse(responsejson,updateType,text,tag): # update classified text and tag in entities of response json\n",
    "    \"\"\" Update the output JSON with augumented classifications.\n",
    "    \"\"\"\n",
    "   # print(\"augument_response  response: \"+str(responsejson)+\"updateType: \"+updateType+\"text or words: \"+text+\"tag in rule brd: \"+tag )\n",
    "    if(updateType == 'keyword'):\n",
    "        if not any(d.get('text', None) == text for d in responsejson['Keywords']):\n",
    "            responsejson['Keywords'].append({\"User\":text})\n",
    "    else:\n",
    "        if not any(d.get('text', None) == text for d in responsejson['Entities']):\n",
    "            responsejson['Entities'].append({\"type\":tag,\"text\":text}) \n",
    "    #print(responsejson)\n",
    "    return responsejson\n",
    "\n",
    "def classify_BRD_text(text, config, DOC_TYPE):\n",
    "    \"\"\" Perform augumented classification of the text for BRD specifically for getting the output with action.\n",
    "    \"\"\"\n",
    "    \n",
    "    #will be used for storing initial value of response json, this is from nlu earlier\n",
    "    with open(Path+'config/output_format_BRD.json') as f:\n",
    "        responsejson = json.load(f)\n",
    "\n",
    "        tokens = split_into_tokens(text)\n",
    "\n",
    "        postags = POS_tagging(tokens)\n",
    "        #print(\"POS tags for sentence \"+str(tokens)+\"  is \"+ str(postags))\n",
    "        configjson = json.loads(config)#Rule_BRD.txt\n",
    "    \n",
    "        no_of_items = 0\n",
    "   \n",
    "    \n",
    "    for stages in configjson['configuration']['classification']['stages']:\n",
    "          #print('Stage - Performing ' + stages['name']+':')\n",
    "        for steps in stages['steps']:\n",
    "            #print(\"steps \", steps['type'])\n",
    "            if(steps['type'] == 'chunking'):\n",
    "\n",
    "                    if (DOC_TYPE=='REQ_ACTION'):\n",
    "                        tag = 'REQ_ACTION'\n",
    "                        chunktags = BRD_chunk_tagging('ACTION',steps['chunk'][0]['pattern'],postags)\n",
    "                    elif (DOC_TYPE=='DOMAIN_ACTION'):\n",
    "                        tag='DOMAIN_ACTION'\n",
    "                        chunktags = BRD_chunk_tagging('ACTION',steps['chunk'][1]['pattern'],postags)\n",
    "                    elif (DOC_TYPE=='DE_ACTION'):\n",
    "                        tag = 'DE_ACTION'\n",
    "                        chunktags = BRD_chunk_tagging('ACTION',steps['chunk'][2]['pattern'],postags)\n",
    "                    else:\n",
    "                        print(\"Unknown Document\")\n",
    "                        \n",
    "                    #print(\"chunktags \",chunktags)   \n",
    "                    if (len(chunktags)>0):\n",
    "                        for words in chunktags:\n",
    "                            #print('-------'+chunk['tag']+':'+words)\n",
    "                            responsejson= augument_SpResponse(responsejson,'ACTION',words,tag)\n",
    "                            \n",
    "            else:\n",
    "                print('UNKNOWN STEP')\n",
    "    \n",
    "    \n",
    "    \n",
    "    return responsejson\n",
    "\n",
    "\n",
    "\n",
    "stopWords = get_stop_words('english')\n",
    "# List of words to be ignored for text similarity\n",
    "stopWords.extend([\"The\",\"This\",\"That\",\".\",\"!\",\"?\"])\n",
    "\n",
    "def compute_text_similarity(text1, text2, text1tags, text2tags):\n",
    "    \"\"\" Compute text similarity using cosine\n",
    "    \"\"\"\n",
    "    #stemming is the process for reducing inflected (or sometimes derived) words to their stem, base or root form\n",
    "    stemmer = nltk.stem.WordNetLemmatizer()\n",
    "    sentences_text1 = split_sentences(text1)\n",
    "    sentences_text2 = split_sentences(text2)\n",
    "    tokens_text1 = []\n",
    "    tokens_text2 = []\n",
    "    #print(\"sentence 1\",sentences_text1)\n",
    "    #print(\"sentence 2\",sentences_text2)\n",
    "\n",
    "    \n",
    "    for element in text1tags:\n",
    "        tokens_text1.extend(split_into_tokens(element))\n",
    "    for element in text2tags:\n",
    "        tokens_text2.extend(split_into_tokens(element))\n",
    "    \n",
    "    for sentence in sentences_text1:\n",
    "        tokenstemp = split_into_tokens(sentence.lower())\n",
    "        tokens_text1.extend(tokenstemp)\n",
    "    \n",
    "    for sentence in sentences_text2:\n",
    "        tokenstemp = split_into_tokens(sentence.lower())\n",
    "        tokens_text2.extend(tokenstemp)\n",
    "        \n",
    "       \n",
    "    if (len(text1tags) > 0):  \n",
    "        tokens_text1.extend(text1tags)\n",
    "    if (len(text2tags) > 0):    \n",
    "        tokens_text2.extend(text2tags)\n",
    "    \n",
    "    \n",
    "\n",
    "    tokens1Filtered = [stemmer.lemmatize(x)  for x in tokens_text1 if x not in stopWords]\n",
    "    \n",
    "    tokens2Filtered = [stemmer.lemmatize(x) for x in tokens_text2 if x not in stopWords]\n",
    "    \n",
    "    #  remove duplicate tokens\n",
    "    tokens1Filtered = set(tokens1Filtered)\n",
    "    tokens2Filtered = set(tokens2Filtered)\n",
    "    #print(\"final tokens1 \",tokens_text1)\n",
    "    \n",
    "    #print(\"final tokens2 \",tokens_text2)\n",
    "    tokensList=[]\n",
    "\n",
    "    text1vector = []\n",
    "    text2vector = []\n",
    "    \n",
    "    if len(tokens1Filtered) < len(tokens2Filtered):\n",
    "        tokensList = tokens1Filtered\n",
    "    else:\n",
    "        tokensList = tokens2Filtered\n",
    "\n",
    "    for token in tokensList:\n",
    "        if token in tokens1Filtered:\n",
    "            text1vector.append(1)\n",
    "        else:\n",
    "            text1vector.append(0)\n",
    "        if token in tokens2Filtered:\n",
    "            text2vector.append(1)\n",
    "        else:\n",
    "            text2vector.append(0)  \n",
    "\n",
    "    cosine_similarity = 1-cosine_distance(text1vector,text2vector)\n",
    "    if numpy.isnan(cosine_similarity):\n",
    "        cosine_similarity = 0\n",
    "    \n",
    "    return cosine_similarity\n",
    "             \n",
    "\n",
    "def get_file(Path):\n",
    "    dummpyfunction=\"123\"\n",
    "    \n",
    "\n",
    "\n",
    "def load_artifacts():\n",
    "    global requirements_df \n",
    "    global domain_df \n",
    "    global dataelements_df \n",
    "    global config\n",
    "    global BRD_config\n",
    "    global Path\n",
    "    \n",
    "    \n",
    "    Location = \"\".join((Path,requirements_file_name))\n",
    "    #get_file(requirements_file_name,Location)\n",
    "    excel = pd.ExcelFile(Location)\n",
    "    requirements_df = excel.parse(requirements_sheet_name)\n",
    "    Location = \"\".join((Path,domain_file_name))\n",
    "    #get_file(domain_file_name,Location)\n",
    "    excel = pd.ExcelFile(Location)\n",
    "    domain_df = excel.parse(domain_sheet_name)\n",
    "    Location = \"\".join((Path,dataelements_file_name))\n",
    "    #get_file(dataelements_file_name,Location)\n",
    "    excel = pd.ExcelFile(Location)\n",
    "    dataelements_df = excel.parse(dataelements_sheet_name)\n",
    "    rule_text = open(Path+configFileName)\n",
    "    config = rule_text.read()\n",
    "    BRD_rule_text = open(Path+BRD_configFileName)\n",
    "    BRD_config = BRD_rule_text.read()\n",
    "\n",
    "    \n",
    "def prepare_artifact_dataframes():\n",
    "    \"\"\" Prepare artifact dataframes by creating necessary output columns\n",
    "    \"\"\"\n",
    "    global requirements_df \n",
    "    global domain_df \n",
    "    global dataelements_df \n",
    "    req_cols_len = len(requirements_df.columns)\n",
    "    dom_cols_len = len(domain_df.columns)\n",
    "    dat_cols_len = len(dataelements_df.columns)\n",
    "    requirements_df.insert(req_cols_len, \"ClassifiedText\",\"\")\n",
    "    requirements_df.insert(req_cols_len+1, \"Keywords\",\"\")\n",
    "    requirements_df.insert(req_cols_len+2, \"DomainMatchScore\",\"\")\n",
    "    \n",
    "    domain_df.insert(dom_cols_len, \"ClassifiedText\",\"\")\n",
    "    domain_df.insert(dom_cols_len+1, \"Keywords\",\"\")\n",
    "    domain_df.insert(dom_cols_len+2, \"DataElementsMatchScore\",\"\")\n",
    "\n",
    "    dataelements_df.insert(dat_cols_len, \"ClassifiedText\",\"\")\n",
    "    dataelements_df.insert(dat_cols_len+1, \"Keywords\",\"\")\n",
    "    dataelements_df.insert(dat_cols_len+2, \"RequirementsMatchScore\",\"\")\n",
    "    \n",
    "\n",
    "    \n",
    "def mod_req_text_classifier_output(artifact_df, BRD_config, output_column_name,DOC_TYPE):\n",
    "    \"\"\" Add text classifier output to the artifact dataframe based on rule defined in config\n",
    "    \"\"\"\n",
    "    for index, row in artifact_df.iterrows():\n",
    "        summary = row[\"I want to <perform some task>\"]\n",
    "        modID = row[\"ID\"]\n",
    "        \n",
    "       # print(modID)\n",
    "        modID = modID.replace(\"R\",\"UC\")\n",
    "        user = row[\"As a <type of user>\"]\n",
    "        user = \"\".join((user,\" want to \"))\n",
    "        summary = \"\".join((user,summary))\n",
    "        #print(\"--------------\")\n",
    "        #print(summary)\n",
    "        classifier_journey_output = classify_BRD_text(summary, BRD_config,DOC_TYPE)\n",
    "        #print(\"classifieer ourney out\",classifier_journey_output)\n",
    "        artifact_df.set_value(index, output_column_name, classifier_journey_output)\n",
    "        \n",
    "       \n",
    "    return artifact_df \n",
    "\n",
    "\n",
    "def add_text_classifier_output(artifact_df, config, output_column_name, DOC_TYPE):\n",
    "    \"\"\" Add text classifier output to the artifact dataframe based on rule defined in config\n",
    "    \"\"\"\n",
    "    for index, row in artifact_df.iterrows():\n",
    "        summary = row[\"Description\"]\n",
    "        #print(\"--------------\")\n",
    "        #print(summary)\n",
    "        classifier_journey_output = classify_BRD_text(summary, BRD_config,DOC_TYPE)\n",
    "        #print(classifier_journey_output)\n",
    "        artifact_df.set_value(index, output_column_name, classifier_journey_output)\n",
    "    return artifact_df \n",
    "           \n",
    "def add_keywords_entities(artifact_df, classify_text_column_name, output_column_name):\n",
    "    \"\"\" Add keywords and entities to the artifact dataframe\"\"\"\n",
    "    i=0\n",
    "    for index, artifact in artifact_df.iterrows():\n",
    "        keywords_array = []\n",
    "        for row in artifact[classify_text_column_name]['Keywords']:\n",
    "            #print(\"add key word entities: classifiedtext[keywords] \",row)\n",
    "            if not row['User'] in keywords_array:\n",
    "                keywords_array.append(row['User'])\n",
    "                \n",
    "        for entities in artifact[classify_text_column_name]['Entities']:\n",
    "            #print(\"add key word entities: classifiedtext[entities] \",entities)\n",
    "            if not entities['text'] in keywords_array and entities['text']!='':\n",
    "                print(\"entities: \" , entities['text'])\n",
    "                artifact_df.at[i,output_column_name] = list(entities['text'])\n",
    "                i= i +1\n",
    "                #set_value(index, output_column_name, keywords_array)\n",
    "                #keywords_array.append(entities['text'])\n",
    "        \n",
    "        \n",
    "        #print(keywords_array)\n",
    "    print(artifact_df)\n",
    "    return artifact_df \n",
    "\n",
    "#requirements_df, domain_df, keywords_column_name, output_column_name)\n",
    "\n",
    "def populate_text_similarity_score(artifact_df1, artifact_df2, keywords_column_name, output_column_name):\n",
    "    \"\"\" Populate text similarity score to the artifact dataframes\n",
    "    \"\"\"\n",
    "    heading1 = \"Description\"\n",
    "    heading2 = \"Description\"\n",
    "    \n",
    "    try:\n",
    "        artifact_df1[heading1]\n",
    "    except: \n",
    "        heading1 = \"I want to <perform some task>\"\n",
    "    \n",
    "    try:\n",
    "        artifact_df2[heading2]\n",
    "    except: \n",
    "        heading2 = \"I want to <perform some task>\"    \n",
    "    \n",
    "    \n",
    "    for index1, artifact1 in artifact_df1.iterrows():\n",
    "        matches = []\n",
    "        top_matches = []\n",
    "        for index2, artifact2 in artifact_df2.iterrows():\n",
    "            matches.append({'ID': artifact2['ID'], \n",
    "                            'cosine_score': 0, \n",
    "                            'SubjectID':artifact1['ID']})\n",
    "            cosine_score = compute_text_similarity(\n",
    "                #artifact1[\\'Description\\'], \n",
    "                #artifact2[\\'Description\\'], \n",
    "                artifact1[heading1],\n",
    "                artifact2[heading2],\n",
    "                artifact1['Keywords'], \n",
    "                artifact2['Keywords'])\n",
    "            matches[index2][\"cosine_score\"] = cosine_score\n",
    "       \n",
    "        sorted_obj = sorted(matches, key=lambda x : x['cosine_score'], reverse=True)\n",
    "    \n",
    "    # This is where the lower cosine value to be truncated is set and needs to be adjusted based on output\n",
    "    \n",
    "        for obj in sorted_obj:\n",
    "            if obj['cosine_score'] > 0.6:\n",
    "                top_matches.append(obj)\n",
    "               \n",
    "        artifact_df1.set_value(index1, output_column_name, top_matches)\n",
    "    return artifact_df1\n",
    "\n",
    "\n",
    "# ## 6.3 Process flow\n",
    "\n",
    "# ** Prepare data **\n",
    "# * Load artifacts from object storage and create pandas dataframes\n",
    "# * Prepare the pandas dataframes. Add additional columns required for further processing.\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "load_artifacts()\n",
    "prepare_artifact_dataframes()\n",
    "\n",
    "print(\"loaded artifacts\")\n",
    "\n",
    "# ** Run Text Classification on data **\n",
    "# * Add the text classification output to the artifact dataframes\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "DOC_TYPE = ['REQ_ACTION','DOMAIN_ACTION','DE_ACTION']\n",
    "output_column_name = \"ClassifiedText\"\n",
    "requirements_df = mod_req_text_classifier_output(requirements_df, BRD_config, output_column_name,DOC_TYPE[0])\n",
    "\n",
    "domain_df = add_text_classifier_output(domain_df,config, output_column_name,DOC_TYPE[1])\n",
    "dataelements_df = add_text_classifier_output(dataelements_df,config, output_column_name,DOC_TYPE[2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ** Populate keywords and entities **\n",
    "# * Add the keywords and entities extracted from the unstructured text to the artifact dataframes\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "classify_text_column_name = \"ClassifiedText\"\n",
    "output_column_name = \"Keywords\"\n",
    "\n",
    "\n",
    "\n",
    "requirements_df = add_keywords_entities(requirements_df, classify_text_column_name, output_column_name)\n",
    "domain_df = add_keywords_entities(domain_df, classify_text_column_name, output_column_name)\n",
    "dataelements_df = add_keywords_entities(dataelements_df, classify_text_column_name, output_column_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ** Correlate keywords between artifacts **\n",
    "# * Add the text similarity score of associated artifacts to the dataframe\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "keywords_column_name = \"Keywords\"\n",
    "output_column_name = \"DomainMatchScore\"\n",
    "requirements_df = populate_text_similarity_score(requirements_df, domain_df, keywords_column_name, output_column_name)\n",
    "\n",
    "output_column_name = \"DataElementsMatchScore\"\n",
    "domain_df = populate_text_similarity_score(domain_df, dataelements_df, keywords_column_name, output_column_name)\n",
    "\n",
    "output_column_name = \"RequirementsMatchScore\"\n",
    "dataelements_df = populate_text_similarity_score(dataelements_df, requirements_df, keywords_column_name, output_column_name)\n",
    "\n",
    "writer = pd.ExcelWriter(Path+'data/intermediateout.xlsx')\n",
    "requirements_df.to_excel(writer, sheet_name='Sheet1')\n",
    "domain_df.to_excel(writer, sheet_name='Sheet2')\n",
    "dataelements_df.to_excel(writer, sheet_name='Sheet3')\n",
    "writer.save()\n",
    "# # This section will be used to create the Output in excell format\n",
    "\n",
    "# In[59]:\n",
    "\n",
    "\n",
    "def extract_action(summary):\n",
    "    action_string = \"\"\n",
    "    count = 1\n",
    "    for entities in summary['Entities']:\n",
    "        #print(entities['text'])\n",
    "        if not entities['text'] in action_string:\n",
    "                action_string = action_string + entities['text']\n",
    "                count = count + 1\n",
    "                if count == 2:\n",
    "                    count = 1\n",
    "                    action_string = action_string + \",\"\n",
    "    \n",
    "    #print(action_string)\n",
    "    return action_string\n",
    "\n",
    "def lookup_use_case(temp,artifact3_df,column_name):\n",
    "    #print(artifact3_df.get_value(0,'ID'))\n",
    "    val = \"\"\n",
    "    rowNum = len(artifact3_df.index)\n",
    "    #print(rowNum)\n",
    "    for j in range(0,rowNum):\n",
    "        if temp == artifact3_df.get_value(j,'ID'):\n",
    "            val = artifact3_df.get_value(j,column_name)\n",
    "            #print(val)\n",
    "    \n",
    "    return val       \n",
    "        \n",
    "    \n",
    "def extract_match(summary,no_of_matches,artifact3_df,column_name):\n",
    "    match_array_description = []\n",
    "    match_array_id = []\n",
    "    for index in range(0,no_of_matches):\n",
    "        try:\n",
    "            temp = summary[index][\"ID\"]\n",
    "            \n",
    "        except:\n",
    "            break\n",
    "    \n",
    "            \n",
    "        temp = summary[index][\"ID\"]\n",
    "        #print(temp)\n",
    "        use_case = lookup_use_case(temp,artifact3_df,column_name)\n",
    "        \n",
    "        match_array_id.append(temp)\n",
    "        #match_array_description.append(use_case + \"(\" + str(round(summary[index][\"cosine_score\"], 2)) +\")\")\n",
    "        match_array_description.append(use_case)\n",
    "        #print(use_case)\n",
    "            \n",
    "    \n",
    "    #print(\"************\")\n",
    "    #print(match_array_id)       \n",
    "    #print(\"************\")\n",
    "    return (match_array_description,match_array_id)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "def extract_action_requirements_df(artifact1_df, artifact2_df):\n",
    "    \"\"\" Add text classifier output to the artifact dataframe based on rule defined in config\n",
    "    \"\"\"\n",
    "    for index, row in artifact2_df.iterrows():\n",
    "        summary = row[\"ClassifiedText\"]\n",
    "        classifier_journey_output = extract_action(summary)\n",
    "        artifact1_df.set_value(index, 'Use Case', classifier_journey_output)\n",
    "    return artifact1_df \n",
    "\n",
    "def extract_bestmatch(artifact1_df, artifact2_df, artifact3_df, artifact4_df):\n",
    "    \"\"\" Extract best Match\n",
    "    \"\"\"\n",
    "    No_of_matches_user_function = 2\n",
    "    No_of_matches_data_elements = 5\n",
    "    best_match_output_domain_function = []\n",
    "    best_match_output_dataelement_function = []\n",
    "    print(\"in extract match\")\n",
    "     \n",
    "    for index, row in artifact2_df.iterrows():\n",
    "        temp1=\"\"\n",
    "        summary = row[\"DomainMatchScore\"]\n",
    "       \n",
    "        #print(summary)\n",
    "        (best_match_output_domain_function,best_match_output_domain_id) = extract_match(summary, No_of_matches_user_function, artifact3_df,\"User Function\")\n",
    "        #print(best_match_output_domain_id)\n",
    "        artifact1_df.set_value(index, 'Functionality', best_match_output_domain_function)\n",
    "        print(\"************\")\n",
    "        for index2 in best_match_output_domain_id:\n",
    "            #print(index2)\n",
    "        \n",
    "            row_domain = len(artifact3_df.index)\n",
    "            for p in range(0,row_domain):\n",
    "                if index2 == artifact3_df.get_value(p,'ID'):\n",
    "                    dataelement_summary = artifact3_df.get_value(p,'DataElementsMatchScore')\n",
    "                    #print(dataelement_summary)\n",
    "                    #print(\"------\")\n",
    "                    (best_match_output_dataelement_function,best_match_output_dataelement_id) = extract_match(dataelement_summary, No_of_matches_data_elements, artifact4_df, \"Short\")\n",
    "            temp1 = temp1 +','+ str(best_match_output_dataelement_function)\n",
    "            #print(\"best elemenets \",temp1)\n",
    "        \n",
    "        \n",
    "\n",
    "        artifact1_df.set_value(index, 'Attributes', temp1)\n",
    "        \n",
    "    return artifact1_df \n",
    "\n",
    "\n",
    "# In[60]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "no_of_rows_brd = len(requirements_df.index)\n",
    "\n",
    "index = range(0,no_of_rows_brd)\n",
    "columns = ['User','Use Case', 'Functionality', 'Attributes']\n",
    "\n",
    "\n",
    "SimMean = pd.DataFrame(index=index, columns=columns)\n",
    "SimMean.loc[0:no_of_rows_brd,'User'] = requirements_df.loc[0:no_of_rows_brd,'As a <type of user>'].values\n",
    "SimMean = extract_action_requirements_df(SimMean,requirements_df)\n",
    "SimMean = extract_bestmatch(SimMean,requirements_df,domain_df,dataelements_df)\n",
    "#SimMean = extract_bestmatch_domaintodataelem(SimMean,domain_df)\n",
    "SimMean.get_value(0,\"Attributes\")\n",
    "#print(SimMean)\n",
    "\n",
    "writer = pd.ExcelWriter(Path+'data/final_output_banking_2.xlsx')\n",
    "SimMean.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
