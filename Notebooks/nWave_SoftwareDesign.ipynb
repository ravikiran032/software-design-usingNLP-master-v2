{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Design Using ML&AI nWave\n",
    "\n",
    "\n",
    "# 1. Setup\n",
    "\n",
    "To prepare your environment, you need to install some packages\n",
    "\n",
    "# 1.1 Install the necessary packages\n",
    "\n",
    "You need the latest versions of these packages:<br>\n",
    " \n",
    "** Spacy** a client library for NLP.<br>\n",
    "** Pandas for dataframe.<br>\n",
    "** stop_words: **List of common stop words.<br>\n",
    "** python-boto3:** is a python client for the Boto3 API used for communicating to AWS.<br>\n",
    "** websocket-client: ** is a python client for the Websockets.<br>\n",
    "** pyorient: ** is a python client for the Orient DB.<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Install NLTK: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /anaconda3/lib/python3.6/site-packages (3.3)\n",
      "Requirement not upgraded as not directly required: six in /anaconda3/lib/python3.6/site-packages (from nltk) (1.11.0)\n",
      "Requirement already up-to-date: pyorient in /Users/swaroopmishra/.local/lib/python3.6/site-packages (1.5.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk\n",
    "!pip install --upgrade pyorient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Boto3 client for AWS communication thorugh CLI **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /anaconda3/lib/python3.6/site-packages (1.7.11)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /anaconda3/lib/python3.6/site-packages (from boto3) (0.1.13)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /anaconda3/lib/python3.6/site-packages (from boto3) (0.9.3)\n",
      "Requirement already satisfied: botocore<1.11.0,>=1.10.11 in /Users/swaroopmishra/.local/lib/python3.6/site-packages (from boto3) (1.10.19)\n",
      "Requirement already satisfied: docutils>=0.10 in /anaconda3/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.11->boto3) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /anaconda3/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.11->boto3) (2.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.11.0,>=1.10.11->boto3) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Install stop_words **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stop-words in /anaconda3/lib/python3.6/site-packages (2015.2.23.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install stop-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Install websocket client: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: websocket-client in /anaconda3/lib/python3.6/site-packages (0.47.0)\r\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from websocket-client) (1.11.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install websocket-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Install pyorient: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: awscli in /Users/swaroopmishra/.local/lib/python3.6/site-packages (1.15.19)\n",
      "Requirement already satisfied: botocore==1.10.19 in /Users/swaroopmishra/.local/lib/python3.6/site-packages (from awscli) (1.10.19)\n",
      "Requirement already satisfied: docutils>=0.10 in /anaconda3/lib/python3.6/site-packages (from awscli) (0.14)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.12 in /anaconda3/lib/python3.6/site-packages (from awscli) (0.1.13)\n",
      "Requirement already satisfied: PyYAML<=3.12,>=3.10 in /anaconda3/lib/python3.6/site-packages (from awscli) (3.12)\n",
      "Requirement already satisfied: rsa<=3.5.0,>=3.1.2 in /anaconda3/lib/python3.6/site-packages (from awscli) (3.4.2)\n",
      "Requirement already satisfied: colorama<=0.3.7,>=0.2.5 in /anaconda3/lib/python3.6/site-packages (from awscli) (0.3.7)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /anaconda3/lib/python3.6/site-packages (from botocore==1.10.19->awscli) (0.9.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /anaconda3/lib/python3.6/site-packages (from botocore==1.10.19->awscli) (2.6.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /anaconda3/lib/python3.6/site-packages (from rsa<=3.5.0,>=3.1.2->awscli) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore==1.10.19->awscli) (1.11.0)\n",
      "Requirement already satisfied: pyorient in /Users/swaroopmishra/.local/lib/python3.6/site-packages (1.5.5)\n",
      "Collecting awscli\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/7d/81e59502c95100bfb9010e6e04fe6dc8f03b4c11f5c63d79b9888ad4a412/awscli-1.15.20-py2.py3-none-any.whl (1.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.3MB 1.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: PyYAML<=3.12,>=3.10 in /anaconda3/lib/python3.6/site-packages (from awscli) (3.12)\n",
      "Collecting botocore==1.10.20 (from awscli)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/bf/99c47b80476a890773d56233a890a4c30d0d5868e6c991dcc945f4735d75/botocore-1.10.20-py2.py3-none-any.whl (4.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.2MB 3.8MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement not upgraded as not directly required: colorama<=0.3.9,>=0.2.5 in /anaconda3/lib/python3.6/site-packages (from awscli) (0.3.7)\n",
      "Requirement not upgraded as not directly required: s3transfer<0.2.0,>=0.1.12 in /anaconda3/lib/python3.6/site-packages (from awscli) (0.1.13)\n",
      "Requirement not upgraded as not directly required: rsa<=3.5.0,>=3.1.2 in /anaconda3/lib/python3.6/site-packages (from awscli) (3.4.2)\n",
      "Requirement not upgraded as not directly required: docutils>=0.10 in /anaconda3/lib/python3.6/site-packages (from awscli) (0.14)\n",
      "Requirement not upgraded as not directly required: jmespath<1.0.0,>=0.7.1 in /anaconda3/lib/python3.6/site-packages (from botocore==1.10.20->awscli) (0.9.3)\n",
      "Requirement not upgraded as not directly required: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /anaconda3/lib/python3.6/site-packages (from botocore==1.10.20->awscli) (2.6.1)\n",
      "Requirement not upgraded as not directly required: pyasn1>=0.1.3 in /anaconda3/lib/python3.6/site-packages (from rsa<=3.5.0,>=3.1.2->awscli) (0.4.2)\n",
      "Requirement not upgraded as not directly required: six>=1.5 in /anaconda3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore==1.10.20->awscli) (1.11.0)\n",
      "Installing collected packages: botocore, awscli\n",
      "  Found existing installation: botocore 1.10.19\n",
      "    Uninstalling botocore-1.10.19:\n",
      "      Successfully uninstalled botocore-1.10.19\n",
      "  Found existing installation: awscli 1.15.19\n",
      "    Uninstalling awscli-1.15.19:\n",
      "      Successfully uninstalled awscli-1.15.19\n",
      "Successfully installed awscli-1.15.20 botocore-1.10.20\n"
     ]
    }
   ],
   "source": [
    "! pip install awscli\n",
    "! pip install pyorient --user\n",
    "! pip install --upgrade --user awscli\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Import packages and libraries \n",
    "\n",
    "Import the packages and libraries that you'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from stop_words import get_stop_words\n",
    "import numpy\n",
    "\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "import websocket\n",
    "import _thread\n",
    "import time\n",
    "\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Configuration\n",
    "\n",
    "Add configurable items of the notebook below\n",
    "## 2.1 Add your service credentials if any required( this is where you need to add credentials of infrastructure you are using to store data etc)\n",
    "\n",
    "\n",
    "Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the section to provide credentials for AWS S3 account\n",
    "### While sharing the notebook remove them -- will try to make this cell hidden later\n",
    "\n",
    "## Console URL :::  https://awstestconsole-swaroop.signin.aws.amazon.com/console\n",
    "## Account Id: \n",
    "## Username : \n",
    "## Password : \n",
    "## Then Navigate to the S3 section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Add your service credentials for S3\n",
    "\n",
    "You must create S3 bucket service on AWS. To access data in a file in Object Storage, you need the Object Storage authentication credentials. Insert the Object Storage authentication credentials as credentials_1 in the following cell after removing the current contents in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "credentials_1 = {\n",
    "    'ACCESS_KEY_ID': '',\n",
    "    'ACCESS_SECRET_KEY': '',\n",
    "    'BUCKET': 'software-testing-pyscript'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Spacy Text Classification  ( this section will be required if we use spacy for machine learning)\n",
    "\n",
    "Write the classification related utility functions in a modularalized form.\n",
    "\n",
    "## 3.1  REQUIREMENT Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_tagging(text):\n",
    "    \"\"\" Generate Part of speech tagging of the text.\n",
    "    \"\"\"\n",
    "    POSofText = nltk.tag.pos_tag(text)\n",
    "    return POSofText\n",
    "def split_into_tokens(text):\n",
    "    \"\"\" Split text into tokens.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('want', 'NN'), ('to', 'TO'), ('withdraw', 'VB'), ('cash', 'NN'), ('from', 'IN'), ('ATM', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "text = \"want to withdraw cash from ATM\"\n",
    "tokens = split_into_tokens(text)\n",
    "postags = POS_tagging(tokens)\n",
    "print(postags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Augumented Classification\n",
    "\n",
    "Custom classification utlity fucntions for augumenting the results of Spacy API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    \"\"\" Split text into sentences.\n",
    "    \"\"\"\n",
    "    sentence_delimiters = re.compile(u'[\\\\[\\\\]\\n.!?]')\n",
    "    sentences = sentence_delimiters.split(text)\n",
    "    return sentences\n",
    "\n",
    "def split_into_tokens(text):\n",
    "    \"\"\" Split text into tokens.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "    \n",
    "def POS_tagging(text):\n",
    "    \"\"\" Generate Part of speech tagging of the text.\n",
    "    \"\"\"\n",
    "    POSofText = nltk.tag.pos_tag(text)\n",
    "    return POSofText\n",
    "\n",
    "def keyword_tagging(tag,tagtext,text):\n",
    "    \"\"\" Tag the text matching keywords.\n",
    "    \"\"\"\n",
    "    if (text.lower().find(tagtext.lower()) != -1):\n",
    "        return text[text.lower().find(tagtext.lower()):text.lower().find(tagtext.lower())+len(tagtext)]\n",
    "    else:\n",
    "        return 'UNKNOWN'\n",
    "    \n",
    "def regex_tagging(tag,regex,text):\n",
    "    \"\"\" Tag the text matching REGEX.\n",
    "    \"\"\"    \n",
    "    p = re.compile(regex, re.IGNORECASE)\n",
    "    matchtext = p.findall(text)\n",
    "    regex_list=[]    \n",
    "    if (len(matchtext)>0):\n",
    "        for regword in matchtext:\n",
    "            regex_list.append(regword)\n",
    "    return regex_list\n",
    "\n",
    "def chunk_tagging(tag,chunk,text):\n",
    "    \"\"\" Tag the text using chunking.\n",
    "    \"\"\"\n",
    "    parsed_cp = nltk.RegexpParser(chunk)\n",
    "    pos_cp = parsed_cp.parse(text)\n",
    "    chunk_list=[]\n",
    "    for root in pos_cp:\n",
    "        if isinstance(root, nltk.tree.Tree):               \n",
    "            if root.label() == tag:\n",
    "                chunk_word = ''\n",
    "                for child_root in root:\n",
    "                    chunk_word = chunk_word +' '+ child_root[0]\n",
    "                chunk_list.append(chunk_word)\n",
    "    return chunk_list\n",
    "    \n",
    "def augument_SpResponse(responsejson,updateType,text,tag):\n",
    "    \"\"\" Update the output JSON with augumented classifications.\n",
    "    \"\"\"\n",
    "    if(updateType == 'keyword'):\n",
    "        if not any(d.get('text', None) == text for d in responsejson['keywords']):\n",
    "            responsejson['keywords'].append({\"text\":text,\"relevance\":0.5})\n",
    "    else:\n",
    "        if not any(d.get('text', None) == text for d in responsejson['entities']):\n",
    "            responsejson['entities'].append({\"type\":tag,\"text\":text,\"relevance\":0.5,\"count\":1}) \n",
    "\n",
    "def classify_text(text, config):\n",
    "    \"\"\" Perform augumented classification of the text.\n",
    "    \"\"\"\n",
    "    \n",
    "    #will be used for storing initial value of response json, this is from nlu earlier\n",
    "    with open('sample.json') as f:\n",
    "        responsejson = json.load(f)\n",
    "    \n",
    "    sentenceList = split_sentences(text) #\n",
    "    \n",
    "    tokens = split_into_tokens(text)\n",
    "    \n",
    "    postags = POS_tagging(tokens)\n",
    "    \n",
    "    configjson = json.loads(config)#load would take a file-like object, read the data from that object, and use that string to create an object:\n",
    "    \n",
    "    \n",
    "    for stages in configjson['configuration']['classification']['stages']:\n",
    "        # print('Stage - Performing ' + stages['name']+':')\n",
    "        for steps in stages['steps']:\n",
    "            # print('    Step - ' + steps['type']+':')\n",
    "            if (steps['type'] == 'keywords'):\n",
    "                for keyword in steps['keywords']:\n",
    "                    for word in sentenceList:\n",
    "                        wordtag = keyword_tagging(keyword['tag'],keyword['text'],word)\n",
    "                        if(wordtag != 'UNKNOWN'):\n",
    "                            #print('      '+keyword['tag']+':'+wordtag)\n",
    "                            augument_SpResponse(responsejson,'entities',wordtag,keyword['tag'])\n",
    "            elif(steps['type'] == 'd_regex'):\n",
    "                for regex in steps['d_regex']:\n",
    "                    for word in sentenceList:\n",
    "                        regextags = regex_tagging(regex['tag'],regex['pattern'],word)\n",
    "                        if (len(regextags)>0):\n",
    "                            for words in regextags:\n",
    "                                #print('      '+regex['tag']+':'+words)\n",
    "                                augument_SpResponse(responsejson,'entities',words,regex['tag'])\n",
    "            elif(steps['type'] == 'chunking'):\n",
    "                for chunk in steps['chunk']:\n",
    "                    chunktags = chunk_tagging(chunk['tag'],chunk['pattern'],postags)\n",
    "                    if (len(chunktags)>0):\n",
    "                        for words in chunktags:\n",
    "                            #print('      '+chunk['tag']+':'+words)\n",
    "                            augument_SpResponse(responsejson,'entities',words,chunk['tag'])\n",
    "            else:\n",
    "                print('UNKNOWN STEP')\n",
    "    \n",
    "    return responsejson\n",
    "\n",
    "def replace_unicode_strings(response):\n",
    "    \"\"\" Convert dict with unicode strings to strings.\n",
    "    \"\"\"\n",
    "    if isinstance(response, dict):\n",
    "        return {replace_unicode_strings(key): replace_unicode_strings(value) for key, value in response.iteritems()}\n",
    "    elif isinstance(response, list):\n",
    "        return [replace_unicode_strings(element) for element in response]\n",
    "    elif isinstance(response, unicode):\n",
    "        return response.encode('utf-8')\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Correlate text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = get_stop_words('english')\n",
    "# List of words to be ignored for text similarity\n",
    "stopWords.extend([\"The\",\"This\",\"That\",\".\",\"!\",\"?\"])\n",
    "\n",
    "def compute_text_similarity(text1, text2, text1tags, text2tags):\n",
    "    \"\"\" Compute text similarity using cosine\n",
    "    \"\"\"\n",
    "    #stemming is the process for reducing inflected (or sometimes derived) words to their stem, base or root form\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    sentences_text1 = split_sentences(text1)\n",
    "    sentences_text2 = split_sentences(text2)\n",
    "    tokens_text1 = []\n",
    "    tokens_text2 = []\n",
    "    \n",
    "    for sentence in sentences_text1:\n",
    "        tokenstemp = split_into_tokens(sentence.lower())\n",
    "        tokens_text1.extend(tokenstemp)\n",
    "    \n",
    "    for sentence in sentences_text2:\n",
    "        tokenstemp = split_into_tokens(sentence.lower())\n",
    "        tokens_text2.extend(tokenstemp)\n",
    "    if (len(text1tags) > 0):  \n",
    "        tokens_text1.extend(text1tags)\n",
    "    if (len(text2tags) > 0):    \n",
    "        tokens_text2.extend(text2tags)\n",
    "    \n",
    "    tokens1Filtered = [stemmer.stem(x) for x in tokens_text1 if x not in stopWords]\n",
    "    \n",
    "    tokens2Filtered = [stemmer.stem(x) for x in tokens_text2 if x not in stopWords]\n",
    "    \n",
    "    #  remove duplicate tokens\n",
    "    tokens1Filtered = set(tokens1Filtered)\n",
    "    tokens2Filtered = set(tokens2Filtered)\n",
    "   \n",
    "    tokensList=[]\n",
    "\n",
    "    text1vector = []\n",
    "    text2vector = []\n",
    "    \n",
    "    if len(tokens1Filtered) < len(tokens2Filtered):\n",
    "        tokensList = tokens1Filtered\n",
    "    else:\n",
    "        tokensList = tokens2Filtered\n",
    "\n",
    "    for token in tokensList:\n",
    "        if token in tokens1Filtered:\n",
    "            text1vector.append(1)\n",
    "        else:\n",
    "            text1vector.append(0)\n",
    "        if token in tokens2Filtered:\n",
    "            text2vector.append(1)\n",
    "        else:\n",
    "            text2vector.append(0)  \n",
    "\n",
    "    cosine_similarity = 1-cosine_distance(text1vector,text2vector)\n",
    "    if numpy.isnan(cosine_similarity):\n",
    "        cosine_similarity = 0\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Persistence and Storage\n",
    "## 5.1 Configure Object Storage Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3',\n",
    "                    aws_access_key_id=credentials_1['ACCESS_KEY_ID'],\n",
    "                    aws_secret_access_key=credentials_1['ACCESS_SECRET_KEY'],\n",
    "                    config=Config(signature_version='s3v4')\n",
    "                     )\n",
    "#Enter the path where you want to store data downlaoded from S3\n",
    "\n",
    "\n",
    "def get_file(filename,Location):\n",
    "    #s3.download_file(Bucket=credentials_1['BUCKET'],Key=filename,Filename=Location)\n",
    "    t=\"abc\"\n",
    "\n",
    "#def load_string(fileobject):\n",
    "#    '''Load the file contents into a Python string'''\n",
    "#    text = fileobject.read()\n",
    "#    return text\n",
    "\n",
    "#def load_df(fileobject,sheetname):\n",
    "#    '''Load file contents into a Pandas dataframe'''\n",
    "#    excelFile = pd.ExcelFile(fileobject)\n",
    "#    df = excelFile.parse(sheetname)\n",
    "#    return df\n",
    "\n",
    "#def put_file(filename, filecontents):\n",
    "#    '''Write file to Cloud Object Storage'''\n",
    "#    resp = s3.put_object(Bucket=credentials_1['BUCKET'], Key=filename, Body=filecontents)\n",
    "    #resp = s3.Bucket(Bucket=credentials_1['BUCKET']).put_object(Key=filename, Body=filecontents)\n",
    "#    return resp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 OrientDB client - functions to connect, store and retrieve data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Connect to OrientDB **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyorient' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-586f403fcbc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyorient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrientDB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"localhost\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2424\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpassw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msession_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pyorient' is not defined"
     ]
    }
   ],
   "source": [
    "client = pyorient.OrientDB(host=\"localhost\", port=2424)\n",
    "user = \"\"\n",
    "passw = \"\"\n",
    "session_id = client.connect(user, passw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** OrientDB Core functions **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(dbname, username, password):\n",
    "    \"\"\" Create a database\n",
    "    \"\"\"\n",
    "    client.db_create( dbname, pyorient.DB_TYPE_GRAPH, pyorient.STORAGE_TYPE_MEMORY )\n",
    "    print(dbname  + \" created and opened successfully\")\n",
    "        \n",
    "def drop_database(dbname):\n",
    "    \"\"\" Drop a database\n",
    "    \"\"\"\n",
    "    if client.db_exists( dbname, pyorient.STORAGE_TYPE_MEMORY ):\n",
    "        client.db_drop(dbname)\n",
    "    \n",
    "def create_class(classname):\n",
    "    \"\"\" Create a class\n",
    "    \"\"\"\n",
    "    command = \"create class \"+classname + \" extends V\"\n",
    "    client.command(command)\n",
    "    \n",
    "def create_record(classname, entityname, attributes):\n",
    "    \"\"\" Create a record\n",
    "    \"\"\"\n",
    "    command = \"insert into \" + classname + \" set \" \n",
    "    attrstring = \"\"\n",
    "    for index,key in enumerate(attributes):\n",
    "        attrstring = attrstring + key + \" = '\"+ attributes[key] + \"'\"\n",
    "        if index != len(attributes) -1:\n",
    "            attrstring = attrstring +\",\"\n",
    "    command = command + attrstring\n",
    "    client.command(command)\n",
    "    \n",
    "def create_defect_testcase_edge(defectid, testcaseid, attributes):\n",
    "    \"\"\" Create an edge between a defect and a testcase\n",
    "    \"\"\"\n",
    "    command = \"create edge linkedtestcases from (select from Defect where ID = \" + \"'\" + defectid + \"') to (select from Testcase where ID = \" + \"'\" + testcaseid + \"')\" \n",
    "    if len(attributes) > 0:\n",
    "        command = command + \" set \"\n",
    "    attrstring = \"\"\n",
    "    for index,key in enumerate(attributes):\n",
    "        val = attributes[key]\n",
    "        if not isinstance(val, str):\n",
    "            val = str(val)\n",
    "        attrstring = attrstring + key + \" = '\"+ val + \"'\"\n",
    "        if index != len(attributes) -1:\n",
    "            attrstring = attrstring +\",\"\n",
    "    command = command + attrstring\n",
    "    client.command(command)    \n",
    "    \n",
    "def create_testcase_requirement_edge(testcaseid, reqid, attributes):\n",
    "    \"\"\" Create an edge between a testcase and a requirement\n",
    "    \"\"\"\n",
    "    command = \"create edge linkedrequirements from (select from Testcase where ID = \"+ \"'\" + testcaseid+\"') to (select from Requirement where ID = \"+\"'\"+reqid+\"')\" \n",
    "    if len(attributes) > 0:\n",
    "        command = command + \" set \"\n",
    "    attrstring = \"\"\n",
    "    for index,key in enumerate(attributes):\n",
    "        val = attributes[key]\n",
    "        if not isinstance(val, str):\n",
    "            val = str(val)\n",
    "        attrstring = attrstring + key + \" = '\"+ val + \"'\"\n",
    "        if index != len(attributes) -1:\n",
    "            attrstring = attrstring +\",\"\n",
    "    command = command + attrstring\n",
    "    client.command(command)  \n",
    "\n",
    "    \n",
    "def create_requirement_defect_edge(reqid, defectid, attributes):\n",
    "    \"\"\" Create an edge between a requirement and a defect\n",
    "    \"\"\"\n",
    "    command = \"create edge linkeddefects from (select from Requirement where ID = \"+ \"'\" + reqid+\"') to (select from Defect where ID = \"+\"'\"+defectid+\"')\" \n",
    "    \n",
    "    if len(attributes) > 0:\n",
    "         command = command + \" set \"\n",
    "    attrstring = \"\"\n",
    "    for index,key in enumerate(attributes):\n",
    "        val = attributes[key]\n",
    "        if not isinstance(val, str):\n",
    "            val = str(val)\n",
    "        attrstring = attrstring + key + \" = '\"+ val + \"'\"\n",
    "        if index != len(attributes) -1:\n",
    "            attrstring = attrstring +\",\"\n",
    "    command = command + attrstring\n",
    "    client.command(command) \n",
    "    \n",
    "def execute_query(query):\n",
    "    \"\"\" Execute a query\n",
    "    \"\"\"\n",
    "    return client.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** OrientDB Insights **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_testcases(defectid):\n",
    "    \"\"\" Get the related testcases for a defect\n",
    "    \"\"\"\n",
    "    testcasesQuery = \"select * from ( select expand( out('linkedtestcases')) from Defect where ID = '\" + defectid +\"' )\"\n",
    "    testcases = execute_query(testcasesQuery)\n",
    "    scoresQuery = \"select expand(out_linkedtestcases) from Defect where ID = '\"+defectid+\"'\"\n",
    "    scores = execute_query(scoresQuery)\n",
    "    testcaseList =[]\n",
    "    scoresList= []\n",
    "    for testcase in testcases:\n",
    "        testcaseList.append(testcase.ID)\n",
    "    for score in scores:\n",
    "        scoresList.append(score.score)\n",
    "    result = {}\n",
    "    length = len(testcaseList)\n",
    "    for i in range(0, length):\n",
    "        result[testcaseList[i]] = scoresList[i]\n",
    "    return result\n",
    "\n",
    "def get_related_requirements(testcaseid):\n",
    "    \"\"\" Get the related requirements for a testcase\n",
    "    \"\"\"\n",
    "    requirementsQuery = \"select * from ( select expand( out('linkedrequirements') ) from Testcase where ID = '\" + testcaseid +\"' )\"\n",
    "    requirements = execute_query(requirementsQuery)\n",
    "    print requirements\n",
    "    scoresQuery = \"select expand(out_linkedrequirements) from Testcase where ID = '\"+testcaseid+\"'\"\n",
    "    scores = execute_query(scoresQuery)\n",
    "    requirementsList =[]\n",
    "    scoresList= []\n",
    "    for requirement in requirements:\n",
    "        requirementsList.append(requirement.ID)\n",
    "    for score in scores:\n",
    "        scoresList.append(score.score)\n",
    "    result = {}\n",
    "    length = len(requirementsList)\n",
    "    print requirementsList, scoresList\n",
    "    for i in range(0, length):\n",
    "        result[requirementsList[i]] = scoresList[i]\n",
    "    return result\n",
    "\n",
    "def get_related_defects(reqid):\n",
    "    \"\"\" Get the related defects for a requirement\n",
    "    \"\"\"\n",
    "    defectsQuery = \"select * from ( select expand( out('linkeddefects')) from Requirement where ID = '\" + reqid +\"' )\"\n",
    "    defects = execute_query(defectsQuery)\n",
    "    scoresQuery = \"select expand(out_linkeddefects) from Requirement where ID = '\"+reqid+\"'\"\n",
    "    scores = execute_query(scoresQuery)\n",
    "    defectsList =[]\n",
    "    scoresList= []\n",
    "    for defect in defects:\n",
    "        defectsList.append(defect.ID)\n",
    "    for score in scores:\n",
    "        scoresList.append(score.score)\n",
    "    result = {}\n",
    "    length = len(defectsList)\n",
    "    for i in range(0, length):\n",
    "        result[defectsList[i]] = scoresList[i]\n",
    "    return result\n",
    "\n",
    "def build_format_defects_list(defectsResult):\n",
    "    \"\"\" Build and format the OrientDB query results for defects\n",
    "    \"\"\"\n",
    "    defects = []\n",
    "    for defect in defectsResult:\n",
    "        detail = {}\n",
    "        detail['ID'] = defect.ID\n",
    "        detail['Severity'] = defect.Severity\n",
    "        detail['Description'] = defect.Description\n",
    "        defects.append(detail)\n",
    "    return defects\n",
    "\n",
    "def build_format_testcases_list(testcasesResult):\n",
    "    \"\"\" Build and format the OrientDB query results for testcases\n",
    "    \"\"\"\n",
    "    testcases = []\n",
    "    for testcase in testcasesResult:\n",
    "        detail = {}\n",
    "        detail['ID'] = testcase.ID\n",
    "        detail['Category'] = testcase.Category\n",
    "        detail['Description'] = testcase.Description\n",
    "        testcases.append(detail)\n",
    "    return testcases  \n",
    "\n",
    "def build_format_requirements_list(requirementsResult):\n",
    "    \"\"\" Build and format the OrientDB query results for requirements\n",
    "    \"\"\"\n",
    "    requirements = []\n",
    "    for requirement in requirementsResult:\n",
    "        detail = {}\n",
    "        detail['ID'] =requirement.ID\n",
    "        detail['Description'] = requirement.Description\n",
    "        detail['Priority'] = requirement.Priority\n",
    "        requirements.append(detail)\n",
    "    return requirements  \n",
    "\n",
    "def get_defects():\n",
    "    \"\"\" Get all defects\n",
    "    \"\"\"\n",
    "    defectsQuery = \"select * from Defect\"\n",
    "    defectsResult = execute_query(defectsQuery)\n",
    "    defects = build_format_defects_list(defectsResult)\n",
    "    return defects\n",
    "\n",
    "def get_testcases():\n",
    "    \"\"\" Get all testcases\n",
    "    \"\"\"\n",
    "    testcasesQuery = \"select * from Testcase\"\n",
    "    testcasesResult = execute_query(testcasesQuery)\n",
    "    testcases = build_format_testcases_list(testcasesResult)\n",
    "    return testcases\n",
    "\n",
    "def get_requirements():\n",
    "    \"\"\" Get all requirements\n",
    "    \"\"\"\n",
    "    requirementsQuery = \"select * from Requirement\"\n",
    "    requirementsResult =  execute_query(requirementsQuery)\n",
    "    requirements = build_format_requirements_list(requirementsResult)\n",
    "    return requirements  \n",
    "\n",
    "def get_defects_severity(severity):\n",
    "    \"\"\" Get defects of a given severity\n",
    "    \"\"\"\n",
    "    query = \"select * from Defect where Severity = \" + str(severity)\n",
    "    queryResult =  execute_query(query)\n",
    "    defects = build_format_defects_list(queryResult)    \n",
    "    return defects\n",
    "\n",
    "def get_testcases_category(category):\n",
    "    \"\"\" Get testcases of a given category\n",
    "    \"\"\"\n",
    "    testcasesQuery = \"select * from Testcase where Category = '\"+str(category)+\"'\"\n",
    "    testcasesResult = execute_query(testcasesQuery)\n",
    "    testcases = build_format_testcases_list(testcasesResult)\n",
    "    return testcases\n",
    "\n",
    "def get_testcases_zero_defects():\n",
    "    \"\"\" Get testcases that did not generate any defects\n",
    "    \"\"\"\n",
    "    testcasesQuery = \"Select * from Testcase where in('linkedtestcases').size() = 0\"\n",
    "    testcasesResult = execute_query(testcasesQuery)\n",
    "    testcases = build_format_testcases_list(testcasesResult)\n",
    "    print testcases\n",
    "    return testcases\n",
    "\n",
    "def get_defects_zero_testcases():\n",
    "    \"\"\" Get defects that have no associated testcases\n",
    "    \"\"\"\n",
    "    query = \"Select * from Defect where out('linkedtestcases').size() = 0\"\n",
    "    queryResult =  execute_query(query)\n",
    "    defects = build_format_defects_list(queryResult)   \n",
    "    print defects\n",
    "    return defects\n",
    "\n",
    "def get_requirements_zero_defect():\n",
    "    \"\"\" Get requirements that have no defects\n",
    "    \"\"\"\n",
    "    query = \"Select * from Requirement where out('linkeddefects').size() = 0\"\n",
    "    requirementsResult =  execute_query(query)\n",
    "    requirements = build_format_requirements_list(requirementsResult)\n",
    "    return requirements  \n",
    "\n",
    "def get_requirements_zero_testcases():\n",
    "    \"\"\" Get requirements that have no associated testcases\n",
    "    \"\"\"\n",
    "    query = \"Select * from Requirement where in('linkedrequirements').size() = 0\"\n",
    "    requirementsResult =  execute_query(query)\n",
    "    requirements = build_format_requirements_list(requirementsResult)\n",
    "    return requirements  \n",
    "\n",
    "def get_requirement_defects(numdefects):\n",
    "    \"\"\" Get requirements that have more than a given number of defects\n",
    "    \"\"\"\n",
    "    query = \"select ID,Description,Priority from Requirement where out('linkeddefects').size() >= \" + str(numdefects)\n",
    "    requirementsResult =  execute_query(query)\n",
    "    requirements = build_format_requirements_list(requirementsResult)\n",
    "    for requirement in requirements:\n",
    "        num = len(get_related_defects(requirement['ID']))\n",
    "        requirement['defectcount'] = num\n",
    "    return requirements  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Global variables and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the excel file with data in S3 Storage\n",
    "BrdFileName = \"Banking-BRD.xlsx\"\n",
    "# Choose or get as an input as to which Domain it belongs to i.e banking, healthcare etc\n",
    "Domain = \"Banking\"\n",
    "\n",
    "# Name of the config file in Object Storage\n",
    "configFileName = \"sample_config.txt\"\n",
    "\n",
    "# Config contents\n",
    "config = None;\n",
    "\n",
    "Path = \".//test/\"\n",
    "\n",
    "# Requirements dataframe\n",
    "requirements_file_name = \"Requirements.xlsx\"\n",
    "requirements_sheet_name = \"\".join((Domain,\"-Requirements\"))\n",
    "requirements_df = None\n",
    "\n",
    "# Domain/UseCase dataframe\n",
    "domain_file_name = \"Domain.xlsx\"\n",
    "domain_sheet_name = \"\".join((Domain,\"-Domain\"))\n",
    "domain_df = None\n",
    "\n",
    "# DataElements dataframe\n",
    "dataelements_file_name =\"DataElements.xlsx\"\n",
    "dataelements_sheet_name =\"\".join((Domain,\"-Dataelements\"))\n",
    "dataelements_df = None\n",
    "\n",
    "def load_artifacts():\n",
    "    global requirements_df \n",
    "    global domain_df \n",
    "    global dataelements_df \n",
    "    global config\n",
    "    global Path\n",
    "    Location = \"\".join((Path,requirements_file_name))\n",
    "    get_file(requirements_file_name,Location)\n",
    "    excel = pd.ExcelFile(Location)\n",
    "    requirements_df = excel.parse(requirements_sheet_name)\n",
    "    Location = \"\".join((Path,domain_file_name))\n",
    "    get_file(domain_file_name,Location)\n",
    "    excel = pd.ExcelFile(Location)\n",
    "    domain_df = excel.parse(domain_sheet_name)\n",
    "    Location = \"\".join((Path,dataelements_file_name))\n",
    "    get_file(dataelements_file_name,Location)\n",
    "    excel = pd.ExcelFile(Location)\n",
    "    dataelements_df = excel.parse(dataelements_sheet_name)\n",
    "    rule_text = open(configFileName)\n",
    "    config = rule_text.read()\n",
    "    \n",
    "\n",
    "def prepare_artifact_dataframes():\n",
    "    \"\"\" Prepare artifact dataframes by creating necessary output columns\n",
    "    \"\"\"\n",
    "    global requirements_df \n",
    "    global domain_df \n",
    "    global dataelements_df \n",
    "    req_cols_len = len(requirements_df.columns)\n",
    "    dom_cols_len = len(domain_df.columns)\n",
    "    dat_cols_len = len(dataelements_df.columns)\n",
    "    requirements_df.insert(req_cols_len, \"ClassifiedText\",\"\")\n",
    "    requirements_df.insert(req_cols_len+1, \"Keywords\",\"\")\n",
    "    requirements_df.insert(req_cols_len+2, \"DomainMatchScore\",\"\")\n",
    "    \n",
    "    domain_df.insert(dom_cols_len, \"ClassifiedText\",\"\")\n",
    "    domain_df.insert(dom_cols_len+1, \"Keywords\",\"\")\n",
    "    domain_df.insert(dom_cols_len+2, \"DataElementsMatchScore\",\"\")\n",
    "\n",
    "    dataelements_df.insert(dat_cols_len, \"ClassifiedText\",\"\")\n",
    "    dataelements_df.insert(dat_cols_len+1, \"Keywords\",\"\")\n",
    "    dataelements_df.insert(dat_cols_len+2, \"RequirementsMatchScore\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Utility functions for Engineering Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_req_text_classifier_output(artifact_df, config, output_column_name):\n",
    "    \"\"\" Add text classifier output to the artifact dataframe based on rule defined in config\n",
    "    \"\"\"\n",
    "    for index, row in artifact_df.iterrows():\n",
    "        summary = row[\"I want to <perform some task>\"]\n",
    "        print(\"--------------\")\n",
    "        print(summary)\n",
    "        #classifier_journey_output = mod_classify_text(summary, config)\n",
    "        #print(classifier_journey_output)\n",
    "        #artifact_df.set_value(index, output_column_name, classifier_journey_output)\n",
    "    return artifact_df \n",
    "\n",
    "\n",
    "def add_text_classifier_output(artifact_df, config, output_column_name):\n",
    "    \"\"\" Add text classifier output to the artifact dataframe based on rule defined in config\n",
    "    \"\"\"\n",
    "    for index, row in artifact_df.iterrows():\n",
    "        summary = row[\"Description\"]\n",
    "        #print(\"--------------\")\n",
    "        #print(summary)\n",
    "        classifier_journey_output = classify_text(summary, config)\n",
    "        print(classifier_journey_output)\n",
    "        artifact_df.set_value(index, output_column_name, classifier_journey_output)\n",
    "    return artifact_df \n",
    "           \n",
    "def add_keywords_entities(artifact_df, classify_text_column_name, output_column_name):\n",
    "    \"\"\" Add keywords and entities to the artifact dataframe\"\"\"\n",
    "    for index, artifact in artifact_df.iterrows():\n",
    "        #print(\"-----------\")\n",
    "        #print(artifact)\n",
    "        keywords_array = []\n",
    "        for row in artifact[classify_text_column_name]['keywords']:\n",
    "            if not row['text'] in keywords_array:\n",
    "                keywords_array.append(row['text'])\n",
    "                \n",
    "        for entities in artifact[classify_text_column_name]['entities']:\n",
    "            if not entities['text'] in keywords_array:\n",
    "                keywords_array.append(entities['text'])\n",
    "            if not entities['type'] in keywords_array:\n",
    "                keywords_array.append(entities['type'])\n",
    "        artifact_df.set_value(index, output_column_name, keywords_array)\n",
    "        print(keywords_array)\n",
    "    return artifact_df \n",
    "\n",
    "def populate_text_similarity_score(artifact_df1, artifact_df2, keywords_column_name, output_column_name):\n",
    "    \"\"\" Populate text similarity score to the artifact dataframes\n",
    "    \"\"\"\n",
    "    for index1, artifact1 in artifact_df1.iterrows():\n",
    "        matches = []\n",
    "        top_matches = []\n",
    "        for index2, artifact2 in artifact_df2.iterrows():\n",
    "            matches.append({'ID': artifact2['ID'], \n",
    "                            'cosine_score': 0, \n",
    "                            'SubjectID':artifact1['ID']})\n",
    "            cosine_score = compute_text_similarity(\n",
    "                artifact1['Description'], \n",
    "                artifact2['Description'], \n",
    "                artifact1['Keywords'], \n",
    "                artifact2['Keywords'])\n",
    "            matches[index2][\"cosine_score\"] = cosine_score\n",
    "       \n",
    "        sorted_obj = sorted(matches, key=lambda x : x['cosine_score'], reverse=True)\n",
    "    \n",
    "    # This is where the lower cosine value to be truncated is set and needs to be adjusted based on output\n",
    "    \n",
    "        for obj in sorted_obj:\n",
    "            if obj['cosine_score'] > 0.4:\n",
    "                top_matches.append(obj)\n",
    "               \n",
    "        artifact_df1.set_value(index1, output_column_name, top_matches)\n",
    "    return artifact_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Process flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Prepare data **\n",
    "* Load artifacts from object storage and create pandas dataframes\n",
    "* Prepare the pandas dataframes. Add additional columns required for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>As a &lt;type of user&gt;</th>\n",
       "      <th>I want to &lt;perform some task&gt;</th>\n",
       "      <th>so that I can &lt;achieve some goal&gt;</th>\n",
       "      <th>ClassifiedText</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>DomainMatchScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R01</td>\n",
       "      <td>Customer</td>\n",
       "      <td>deposit check</td>\n",
       "      <td>I want to increase my bank balance</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R02</td>\n",
       "      <td>Customer</td>\n",
       "      <td>withdraw cash from an ATM</td>\n",
       "      <td>I don't have to wait in line at the Bank</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R03</td>\n",
       "      <td>Customer</td>\n",
       "      <td>want to transfer money from one account to ano...</td>\n",
       "      <td>I don't need to pay the amount in person</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R04</td>\n",
       "      <td>Customer</td>\n",
       "      <td>pay my utility bills online</td>\n",
       "      <td>I don't need to write checks or use postal ser...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R05</td>\n",
       "      <td>Customer</td>\n",
       "      <td>apply for a loan</td>\n",
       "      <td>purchase a car</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID As a <type of user>                      I want to <perform some task>  \\\n",
       "0  R01            Customer                                      deposit check   \n",
       "1  R02            Customer                          withdraw cash from an ATM   \n",
       "2  R03            Customer  want to transfer money from one account to ano...   \n",
       "3  R04            Customer                        pay my utility bills online   \n",
       "4  R05            Customer                                   apply for a loan   \n",
       "\n",
       "                   so that I can <achieve some goal> ClassifiedText Keywords  \\\n",
       "0                 I want to increase my bank balance                           \n",
       "1           I don't have to wait in line at the Bank                           \n",
       "2           I don't need to pay the amount in person                           \n",
       "3  I don't need to write checks or use postal ser...                           \n",
       "4                                     purchase a car                           \n",
       "\n",
       "  DomainMatchScore  \n",
       "0                   \n",
       "1                   \n",
       "2                   \n",
       "3                   \n",
       "4                   "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_artifacts()\n",
    "prepare_artifact_dataframes()\n",
    "requirements_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Run Text Classification on data **\n",
    "* Add the text classification output to the artifact dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "deposit check\n",
      "--------------\n",
      "withdraw cash from an ATM\n",
      "--------------\n",
      "want to transfer money from one account to another\n",
      "--------------\n",
      "pay my utility bills online\n",
      "--------------\n",
      "apply for a loan\n",
      "--------------\n",
      "request for check books\n",
      "--------------\n",
      "restock sufficient cash in ATM machines\n"
     ]
    }
   ],
   "source": [
    "output_column_name = \"ClassifiedText\"\n",
    "requirements_df = mod_req_text_classifier_output(requirements_df, config, output_column_name)\n",
    "#requirements_df = add_text_classifier_output(requirements_df,config, output_column_name)\n",
    "#domain_df = add_text_classifier_output(domain_df,config, output_column_name)\n",
    "#dataelements_df = add_text_classifier_output(dataelements_df,config, output_column_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Populate keywords and entities **\n",
    "* Add the keywords and entities extracted from the unstructured text to the artifact dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', ' A customer', 'NP', ' cheque', ' the balance', ' like', 'VERB', ' deposit', ' increase']\n",
      "['', ' cash', 'NP', ' line', ' Customer', 'NAME', ' ATM', ' withdraw', 'VERB', ' wait']\n",
      "['', ' money', 'NP', ' account', ' no physical transfer', ' like', 'VERB', ' transfer']\n",
      "['', ' a customer', 'NP', ' the bank', ' order', ' mail', ' postal service', ' checksto', 'VERB', ' write', ' use']\n",
      "['', ' a loan', 'NP', ' bank', ' a car', ' Customer', 'NAME', ' apply', 'VERB', ' purchase']\n",
      "['', ' sufficient cash', 'NP', ' Banker', 'NAME', ' ATM', ' be', 'VERB', ' restock']\n",
      "['', ' the credit', 'NP', ' history', ' the bank', ' balance', ' banker', ' the loan', ' Banker', 'NAME', ' be', 'VERB', ' review', ' help', ' decide', ' approve', ' reject']\n",
      "['', ' Function', 'NP', ' money', ' balance', ' cheque', ' The input', ' debit card number', ' number', ' Verification', 'NAME', ' PIN', ' Customer', ' ATM', ' be', 'VERB']\n",
      "['', ' This use', 'NP', ' case', ' the current account', ' balance', ' transferingmoney', ' other account', ' any amount', ' function', ' account number', ' account', ' name', ' Input', 'NAME']\n",
      "['', ' The use', 'NP', ' case', ' blank cheque', ' customer', ' number', ' Account', 'NAME', ' Name', ' Customer', ' Adddress', ' require', 'VERB']\n",
      "['', ' This use', 'NP', ' case', ' money', ' account', ' source', ' number', ' target', ' code', ' transfer', ' date', ' type', ' IFSC', 'NAME', ' require', 'VERB', ' say', ' be']\n",
      "['', ' This function', 'NP', ' the deposit', ' money', ' form', ' cheque', ' an account', ' type', ' atleast customer', ' name', ' number', ' amount', ' require', 'VERB', ' be']\n",
      "['', ' This function', 'NP', ' actual process', ' cash', ' bill', ' type', ' maximum amount', ' limit', ' ATM', 'NAME', ' be', 'VERB']\n",
      "['', ' This function', 'NP', ' a customer', ' loan', ' type', ' car', ' house', ' number', ' amount', ' apply', 'VERB']\n",
      "['', ' Function', 'NP', ' banker', ' the cash', ' required bill', ' necessary amount', ' cash', ' ATM', 'NAME', ' be', 'VERB', ' fill']\n",
      "['', ' Function', 'NP', ' credit', ' history', ' customer', ' account', ' number', ' loan', ' The data', ' external credit', ' rating', ' agency', ' score']\n",
      "['', ' the name', 'NP', ' the customer', ' verification', ' deposit', ' withdrawal', ' transfer', ' money', ' Customer Name', 'NAME', ' be', 'VERB']\n",
      "['', ' the account', 'NP', ' number', ' all banking', ' be', 'VERB']\n",
      "['', ' This number', 'NP', ' much money', ' the account']\n",
      "['', ' input', 'NP', ' amount', ' Debit Card Number', 'NAME', ' ATM', ' be', 'VERB']\n",
      "['', ' money', 'NP', ' source', ' account', ' Account Number', 'NAME', ' be', 'VERB']\n",
      "['', ' money', 'NP', ' destination', ' account', ' Account Number', 'NAME', ' be', 'VERB']\n",
      "['', ' much amount', 'NP', ' be', 'VERB']\n",
      "['', ' type', 'NP', ' account', ' Account Type', 'NAME', ' specify', 'VERB']\n",
      "['', ' the type', 'NP', ' ATM', 'NAME', ' be', 'VERB']\n",
      "['', ' This element', 'NP', ' the maximum limit', ' cash', ' ATM', 'NAME', ' be', 'VERB']\n",
      "['', ' This amount', 'NP', ' the amount', ' another account', ' be', 'VERB']\n",
      "['', ' This amount', 'NP', ' the amount', ' the account', ' be', 'VERB']\n",
      "['', ' the addresss', 'NP', ' any mail', ' communication', ' customer']\n",
      "['', ' much loan', 'NP', ' amount', ' customer', ' loan', ' have', 'VERB']\n",
      "['', ' the purpose', 'NP', ' loan', ' new car', ' house']\n",
      "['', ' loan', 'NP', ' the credit', ' score', ' check', 'VERB']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "classify_text_column_name = \"ClassifiedText\"\n",
    "output_column_name = \"Keywords\"\n",
    "requirements_df = add_keywords_entities(requirements_df, classify_text_column_name, output_column_name)\n",
    "domain_df = add_keywords_entities(domain_df, classify_text_column_name, output_column_name)\n",
    "dataelements_df = add_keywords_entities(dataelements_df, classify_text_column_name, output_column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Correlate keywords between artifacts **\n",
    "* Add the text similarity score of associated artifacts to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>User</th>\n",
       "      <th>Description</th>\n",
       "      <th>ClassifiedText</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>DomainMatchScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R01</td>\n",
       "      <td>Customer</td>\n",
       "      <td>A customer would like to deposit cheque so tha...</td>\n",
       "      <td>{'keywords': [{'text': '', 'relevance': 0}], '...</td>\n",
       "      <td>[,  A customer, NP,  cheque,  the balance,  li...</td>\n",
       "      <td>[{'ID': 'U1', 'cosine_score': 0.70710678118654...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R02</td>\n",
       "      <td>Customer</td>\n",
       "      <td>I am a Customer and I want to withdraw cash fr...</td>\n",
       "      <td>{'keywords': [{'text': '', 'relevance': 0}], '...</td>\n",
       "      <td>[,  cash, NP,  line,  Customer, NAME,  ATM,  w...</td>\n",
       "      <td>[{'ID': 'U1', 'cosine_score': 0.67082039324993...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R03</td>\n",
       "      <td>Customer</td>\n",
       "      <td>Customers would like to transfer money from on...</td>\n",
       "      <td>{'keywords': [{'text': '', 'relevance': 0}], '...</td>\n",
       "      <td>[,  money, NP,  account,  no physical transfer...</td>\n",
       "      <td>[{'ID': 'U4', 'cosine_score': 0.82915619758885...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R04</td>\n",
       "      <td>Customer</td>\n",
       "      <td>I am a customer at the bank, and I want to or...</td>\n",
       "      <td>{'keywords': [{'text': '', 'relevance': 0}], '...</td>\n",
       "      <td>[,  a customer, NP,  the bank,  order,  mail, ...</td>\n",
       "      <td>[{'ID': 'U1', 'cosine_score': 0.50917507721731...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R05</td>\n",
       "      <td>Customer</td>\n",
       "      <td>Customer need to  apply for a loan from bank s...</td>\n",
       "      <td>{'keywords': [{'text': '', 'relevance': 0}], '...</td>\n",
       "      <td>[,  a loan, NP,  bank,  a car,  Customer, NAME...</td>\n",
       "      <td>[{'ID': 'U7', 'cosine_score': 0.66666666666666...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID      User                                        Description  \\\n",
       "0  R01  Customer  A customer would like to deposit cheque so tha...   \n",
       "1  R02  Customer  I am a Customer and I want to withdraw cash fr...   \n",
       "2  R03  Customer  Customers would like to transfer money from on...   \n",
       "3  R04  Customer   I am a customer at the bank, and I want to or...   \n",
       "4  R05  Customer  Customer need to  apply for a loan from bank s...   \n",
       "\n",
       "                                      ClassifiedText  \\\n",
       "0  {'keywords': [{'text': '', 'relevance': 0}], '...   \n",
       "1  {'keywords': [{'text': '', 'relevance': 0}], '...   \n",
       "2  {'keywords': [{'text': '', 'relevance': 0}], '...   \n",
       "3  {'keywords': [{'text': '', 'relevance': 0}], '...   \n",
       "4  {'keywords': [{'text': '', 'relevance': 0}], '...   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  [,  A customer, NP,  cheque,  the balance,  li...   \n",
       "1  [,  cash, NP,  line,  Customer, NAME,  ATM,  w...   \n",
       "2  [,  money, NP,  account,  no physical transfer...   \n",
       "3  [,  a customer, NP,  the bank,  order,  mail, ...   \n",
       "4  [,  a loan, NP,  bank,  a car,  Customer, NAME...   \n",
       "\n",
       "                                    DomainMatchScore  \n",
       "0  [{'ID': 'U1', 'cosine_score': 0.70710678118654...  \n",
       "1  [{'ID': 'U1', 'cosine_score': 0.67082039324993...  \n",
       "2  [{'ID': 'U4', 'cosine_score': 0.82915619758885...  \n",
       "3  [{'ID': 'U1', 'cosine_score': 0.50917507721731...  \n",
       "4  [{'ID': 'U7', 'cosine_score': 0.66666666666666...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requirements_df.head()\n",
    "#domain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:55: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "keywords_column_name = \"Keywords\"\n",
    "output_column_name = \"DomainMatchScore\"\n",
    "requirements_df = populate_text_similarity_score(requirements_df, domain_df, keywords_column_name, output_column_name)\n",
    "\n",
    "output_column_name = \"DataElementsMatchScore\"\n",
    "domain_df = populate_text_similarity_score(domain_df, dataelements_df, keywords_column_name, output_column_name)\n",
    "\n",
    "output_column_name = \"RequirementsMatchScore\"\n",
    "dataelements_df = populate_text_similarity_score(dataelements_df, requirements_df, keywords_column_name, output_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'ID': 'U4', 'SubjectID': 'R03', 'cosine_score': 0.82915619758885},\n",
       " {'ID': 'U1', 'SubjectID': 'R03', 'cosine_score': 0.6614378277661476},\n",
       " {'ID': 'U5', 'SubjectID': 'R03', 'cosine_score': 0.6614378277661476},\n",
       " {'ID': 'U3', 'SubjectID': 'R03', 'cosine_score': 0.6123724356957946},\n",
       " {'ID': 'U7', 'SubjectID': 'R03', 'cosine_score': 0.6123724356957946},\n",
       " {'ID': 'U9', 'SubjectID': 'R03', 'cosine_score': 0.5590169943749475},\n",
       " {'ID': 'U2', 'SubjectID': 'R03', 'cosine_score': 0.5},\n",
       " {'ID': 'U6', 'SubjectID': 'R03', 'cosine_score': 0.4330127018922194},\n",
       " {'ID': 'U8', 'SubjectID': 'R03', 'cosine_score': 0.4330127018922194}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(requirements_df)\n",
    "#domain_df.head()\n",
    "#dataelements_df.head()\n",
    "#requirements_df.head()\n",
    "requirements_df.get_value(2,'DomainMatchScore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'ID': 'DE5', 'SubjectID': 'U4', 'cosine_score': 0.9682458365518541},\n",
       " {'ID': 'DE6', 'SubjectID': 'U4', 'cosine_score': 0.9013878188659974},\n",
       " {'ID': 'DE8', 'SubjectID': 'U4', 'cosine_score': 0.8660254037844386},\n",
       " {'ID': 'DE2', 'SubjectID': 'U4', 'cosine_score': 0.8017837257372732},\n",
       " {'ID': 'DE11', 'SubjectID': 'U4', 'cosine_score': 0.7637626158259734},\n",
       " {'ID': 'DE9', 'SubjectID': 'U4', 'cosine_score': 0.7337993857053429},\n",
       " {'ID': 'DE1', 'SubjectID': 'U4', 'cosine_score': 0.7223151185146154},\n",
       " {'ID': 'DE3', 'SubjectID': 'U4', 'cosine_score': 0.674199862463242},\n",
       " {'ID': 'DE12', 'SubjectID': 'U4', 'cosine_score': 0.674199862463242},\n",
       " {'ID': 'DE4', 'SubjectID': 'U4', 'cosine_score': 0.6666666666666666},\n",
       " {'ID': 'DE7', 'SubjectID': 'U4', 'cosine_score': 0.6666666666666666},\n",
       " {'ID': 'DE10', 'SubjectID': 'U4', 'cosine_score': 0.5423261445466404},\n",
       " {'ID': 'DE16', 'SubjectID': 'U4', 'cosine_score': 0.5},\n",
       " {'ID': 'DE14', 'SubjectID': 'U4', 'cosine_score': 0.447213595499958},\n",
       " {'ID': 'DE13', 'SubjectID': 'U4', 'cosine_score': 0.4264014327112209}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_df.get_value(3,'DataElementsMatchScore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataelements_df.get_value(1,'RequirementsMatchScore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \"*\"**********************************************************\"\n",
    "# Next steps :\n",
    "\n",
    "* Populate the correct wording in 3 sheets to provide more accurate and insightful data\n",
    "* Use OrientdB to graph the result of cosine\n",
    "* Use Node Red to start a UI dashboard\n",
    "* Optmize code to reduce memory usage\n",
    "* Move the components like Notebook,OrientDB etc to EC2 AWS ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Utility functions to store entities and relations in Orient DB **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_requirements(requirements_df):\n",
    "    \"\"\" Store requirements into the database\n",
    "    \"\"\"\n",
    "    for index, row in requirements_df.iterrows():\n",
    "        attrs = {}\n",
    "        reqid = row[\"ID\"]\n",
    "        attrs[\"Description\"] = row[\"Description\"].replace('\\n', ' ').replace('\\r', '')\n",
    "        attrs[\"ID\"] = reqid\n",
    "        attrs[\"User\"]= str(row[\"User\"])\n",
    "        create_record(requirement_classname, reqid, attrs)    \n",
    "        \n",
    "def store_domain(domain_df):  \n",
    "    \"\"\" Store domain which has user functions into the database\n",
    "    \"\"\"\n",
    "    for index, row in domain_df.iterrows():\n",
    "        attrs = {}\n",
    "        tcaseid = row[\"ID\"]\n",
    "        attrs[\"Description\"] = row[\"Description\"].replace('\\n', ' ').replace('\\r', '')\n",
    "        attrs[\"ID\"] = tcaseid\n",
    "        attrs[\"User Fuction\"] = str(row[\"Use CASE\"])\n",
    "        create_record(domain_classname, tcaseid, attrs)\n",
    "        \n",
    "def store_dataelements(dataelements_df):\n",
    "    \"\"\" Store data elements or attributes into the database\n",
    "    \"\"\"\n",
    "    for index, row in dataelements_df.iterrows():\n",
    "        attrs = {}\n",
    "        defid = row[\"ID\"]\n",
    "        attrs[\"Description\"] = row[\"Description\"].replace('\\n', ' ').replace('\\r', '')\n",
    "        attrs[\"ID\"] = defid\n",
    "        attrs[\"Short form\"] = str(row[\"Short\"])\n",
    "        create_record(dataelement_classname, defid, attrs)\n",
    "        \n",
    "def store_testcases_requirement_mapping(testcases_df):\n",
    "    \"\"\" Store the related requirements for testcases into the database\n",
    "    \"\"\"\n",
    "    for index, row in testcases_df.iterrows():\n",
    "        tcaseid = row[\"ID\"]\n",
    "        requirements = row[\"RequirementsMatchScore\"]\n",
    "        for requirement in requirements:\n",
    "            reqid = requirement[\"ID\"]\n",
    "            attributes = {}\n",
    "            attributes['score'] = requirement['cosine_score']\n",
    "            create_testcase_requirement_edge(tcaseid,reqid, attributes)\n",
    "            \n",
    "def store_defect_testcase_mapping(defects_df):\n",
    "    \"\"\" Store the related testcases for the defects into the database\n",
    "    \"\"\"\n",
    "    for index, row in defects_df.iterrows():\n",
    "        defid = row[\"ID\"]\n",
    "        testcases = row[\"TestCasesMatchScore\"]\n",
    "        for testcase in testcases:\n",
    "            testcaseid = testcase[\"ID\"]\n",
    "            attributes = {}\n",
    "            attributes['score'] = testcase[\"cosine_score\"]\n",
    "            create_defect_testcase_edge(defid,testcaseid, attributes)\n",
    "            \n",
    "def store_requirement_defect_mapping(requirements_df):\n",
    "    \"\"\" Store the related defects for the requirements in the database\n",
    "    \"\"\"\n",
    "    for index, row in requirements_df.iterrows():\n",
    "        reqid = row[\"ID\"]\n",
    "        defects = row[\"DefectsMatchScore\"]\n",
    "        for defect in defects:\n",
    "            defectid = defect[\"ID\"]\n",
    "            cosine_score =  defect[\"cosine_score\"]\n",
    "            attributes = {}\n",
    "            attributes['score'] = cosine_score\n",
    "            create_requirement_defect_edge(reqid,defectid, attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Store artifacts data and relations into OrientDB **\n",
    "* Drop and create a database\n",
    "* Create classes for each category of artifact\n",
    "* Store artifact data\n",
    "* Store artifact relations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "PyOrientSQLParsingException",
     "evalue": "com.orientechnologies.orient.core.sql.OCommandSQLParsingException - Error parsing query:\ninsert into DataElements set Description = 'Customer Name will be used as part of authenting user and to greet customer',ID = 'DE1',Short form = 'Cus_Nme'\n                                                                                                                                             ^\nEncountered \" <IDENTIFIER> \"form \"\" at line 1, column 139.\nWas expecting one of:\n    \"=\" ...\n    \"=\" ...\n    \r\n\tDB name=\"SoftwareDesignAI\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPyOrientSQLParsingException\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-2d57379ba4d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#store_requirements(requirements_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mstore_dataelements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataelements_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mstore_domain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-74867bc8fd95>\u001b[0m in \u001b[0;36mstore_dataelements\u001b[0;34m(dataelements_df)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ID\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Short form\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Short\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mcreate_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataelement_classname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstore_testcases_requirement_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestcases_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-9b2dd22686a7>\u001b[0m in \u001b[0;36mcreate_record\u001b[0;34m(classname, entityname, attributes)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mattrstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrstring\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommand\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattrstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_defect_testcase_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefectid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestcaseid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyorient/orient.py\u001b[0m in \u001b[0;36mcommand\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CommandMessage\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mQUERY_CMD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyorient/messages/commands.py\u001b[0m in \u001b[0;36mfetch_response\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# decode header only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mCommandMessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_command_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mQUERY_ASYNC\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyorient/messages/base.py\u001b[0m in \u001b[0;36mfetch_response\u001b[0;34m(self, *_continue)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# already fetched, get last results as cache info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_body\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_streams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyorient/messages/base.py\u001b[0m in \u001b[0;36m_decode_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decode_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pyorient/messages/base.py\u001b[0m in \u001b[0;36m_decode_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m             raise PyOrientCommandException(\n\u001b[1;32m    181\u001b[0m                 \u001b[0mexception_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'utf8'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0;34m[\u001b[0m \u001b[0mexception_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'utf8'\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             )\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPyOrientSQLParsingException\u001b[0m: com.orientechnologies.orient.core.sql.OCommandSQLParsingException - Error parsing query:\ninsert into DataElements set Description = 'Customer Name will be used as part of authenting user and to greet customer',ID = 'DE1',Short form = 'Cus_Nme'\n                                                                                                                                             ^\nEncountered \" <IDENTIFIER> \"form \"\" at line 1, column 139.\nWas expecting one of:\n    \"=\" ...\n    \"=\" ...\n    \r\n\tDB name=\"SoftwareDesignAI\""
     ]
    }
   ],
   "source": [
    "#drop_database(\"SoftwareDesign\")\n",
    "#create_database(\"SoftwareDesignAI\", \"\", \"admin\")\n",
    "\n",
    "requirement_classname = \"Requirements\"\n",
    "domain_classname = \"Domain\"\n",
    "dataelement_classname = \"DataElements\"\n",
    "\n",
    "#create_class(requirement_classname)\n",
    "#create_class(domain_classname)\n",
    "#create_class(dataelement_classname)\n",
    "\n",
    "#store_requirements(requirements_df)\n",
    "store_dataelements(dataelements_df)\n",
    "store_domain(domain_df)\n",
    "\n",
    "#store_testcases_requirement_mapping(testcases_df)\n",
    "#store_defect_testcase_mapping(defects_df)\n",
    "#store_requirement_defect_mapping(requirements_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This will be used for line by line checking of code for correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "index1 = 0\n",
    "index2 = 1\n",
    "matches.append({'ID': \"U2\", \n",
    "                'cosine_score': 0, \n",
    "                'SubjectID':\"R01\"})\n",
    "cosine_score = 0.7\n",
    "matches[index2][\"cosine_score\"] = cosine_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ID': 'U1', 'cosine_score': 0.4, 'SubjectID': 'R01'}, {'ID': 'U2', 'cosine_score': 0.7, 'SubjectID': 'R01'}]\n"
     ]
    }
   ],
   "source": [
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ID': 'U3', 'cosine_score': 0.9, 'SubjectID': 'R03'}, {'ID': 'U4', 'cosine_score': 0.9, 'SubjectID': 'R04'}, {'ID': 'U1', 'cosine_score': 0.8, 'SubjectID': 'R01'}, {'ID': 'U2', 'cosine_score': 0.7, 'SubjectID': 'R02'}, {'ID': 'U5', 'cosine_score': 0.4, 'SubjectID': 'R05'}]\n"
     ]
    }
   ],
   "source": [
    "sorted_obj = sorted(matches, key=lambda x : x['cosine_score'], reverse=True)\n",
    "print(sorted_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in sorted_obj:\n",
    "            if obj['cosine_score'] > 0.4:\n",
    "                top_matches.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-259-382d6133efcf>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-259-382d6133efcf>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    stopWords.extend([\"The\",\"This\",\"That\",\".\",\"!\",\"?\"])print(top_matches)\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "stopWords = get_stop_words('english')\n",
    "# List of words to be ignored for text similarity\n",
    "stopWords.extend([\"The\",\"This\",\"That\",\".\",\"!\",\"?\"])print(top_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "cosine similarity :\n"
     ]
    }
   ],
   "source": [
    "stopWords = get_stop_words('english')\n",
    "# List of words to be ignored for text similarity\n",
    "stopWords.extend([\"The\",\"This\",\"That\",\".\",\"!\",\"?\"])\n",
    "\n",
    "# this depicts cosine similarity\n",
    "\n",
    "text1 = \"A customer would like to deposit cheque at the ATM so that he can increase the balance.\"\n",
    "text1tags = ['', ' A customer', 'NP', ' cheque', ' the balance', ' like', 'VERB', ' deposit', ' increase', ' ATM']\n",
    "\n",
    "#text2 = \"This Use case deals with Verification of PIN entered by Customer at ATM. One has to validate the pin using the customer number and the debit card number\"\n",
    "#text2tags = ['', ' case', 'NP', ' the pin', ' the customer', ' number', ' the debit', ' card', ' Use', 'NAME', ' Verification', ' PIN', ' Customer', ' ATM', ' validate', 'VERB']\n",
    "\n",
    "#text1 = \"I am a Customer and I want to withdraw cash from an ATM so that I don’t have to wait in line\"\n",
    "#text1tags = ['', ' cash', 'NP', ' line', ' Customer', 'NAME', ' ATM', ' withdraw', 'VERB', ' wait']\n",
    "\n",
    "#text2 = \"I am a Customer and I want to withdraw cash from an ATM so that I don’t have to wait in line\"\n",
    "#text2tags = ['', ' cash', 'NP', ' line', ' Customer', 'NAME', ' ATM', ' withdraw', 'VERB', ' wait']\n",
    "\n",
    "text2 = \"Customer Name will be used as part of authenting user and to greet customer\"\n",
    "text2tags = ['', ' part', 'NP', ' user', ' customer', ' Customer Name', 'NAME', ' be', 'VERB', ' greet']\n",
    "\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "sentences_text1 = split_sentences(text1)\n",
    "#print(sentences_text1)\n",
    "sentences_text2 = split_sentences(text2)\n",
    "tokens_text1 = []\n",
    "tokens_text2 = []\n",
    "\n",
    "for sentence in sentences_text1:\n",
    "        tokenstemp = split_into_tokens(sentence.lower())\n",
    "        #print(tokenstemp)\n",
    "        tokens_text1.extend(tokenstemp)\n",
    "#print(tokens_text1)\n",
    "\n",
    "for sentence in sentences_text2:\n",
    "        tokenstemp = split_into_tokens(sentence.lower())\n",
    "        tokens_text2.extend(tokenstemp)\n",
    "\n",
    "if (len(text1tags) > 0):  \n",
    "        tokens_text1.extend(text1tags)\n",
    "if (len(text2tags) > 0):    \n",
    "        tokens_text2.extend(text2tags)\n",
    "#print(tokens_text1)\n",
    "        \n",
    "tokens1Filtered = [stemmer.stem(x) for x in tokens_text1 if x not in stopWords]\n",
    "#print(tokens1Filtered)    \n",
    "tokens2Filtered = [stemmer.stem(x) for x in tokens_text2 if x not in stopWords]\n",
    "    \n",
    "    #  remove duplicate tokens\n",
    "tokens1Filtered = set(tokens1Filtered)\n",
    "tokens2Filtered = set(tokens2Filtered)\n",
    "#print(tokens1Filtered)\n",
    "\n",
    "tokensList=[]\n",
    "\n",
    "text1vector = []\n",
    "text2vector = []\n",
    "    \n",
    "if len(tokens1Filtered) < len(tokens2Filtered):\n",
    "    tokensList = tokens1Filtered\n",
    "else:\n",
    "    tokensList = tokens2Filtered\n",
    "\n",
    "#print(tokensList)\n",
    "for token in tokensList:\n",
    "    if token in tokens1Filtered:\n",
    "        text1vector.append(1)\n",
    "    else:\n",
    "        text1vector.append(0)\n",
    "    if token in tokens2Filtered:\n",
    "        text2vector.append(1)\n",
    "    else:\n",
    "        text2vector.append(0)         \n",
    "#print(text1vector)  \n",
    "print(text2vector)\n",
    "cosine_similarity = 1-cosine_distance(text1vector,text2vector) \n",
    "\n",
    "print(\"cosine similarity :\")\n",
    "#print(cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Story ID</th>\n",
       "      <th>User</th>\n",
       "      <th>Description</th>\n",
       "      <th>ClassifiedText</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>DataelementMatchScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R01</td>\n",
       "      <td>Customer</td>\n",
       "      <td>A customer would like to deposit cheque so tha...</td>\n",
       "      <td>{'keywords': [{'text': '', 'relevance': 0}], '...</td>\n",
       "      <td>[,  A customer, NP,  cheque,  the balance,  li...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R02</td>\n",
       "      <td>Customer</td>\n",
       "      <td>I am a Customer and I want to withdraw cash fr...</td>\n",
       "      <td>{'keywords': [{'text': '', 'relevance': 0}], '...</td>\n",
       "      <td>[,  cash, NP,  line,  Customer, NAME,  ATM,  w...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R03</td>\n",
       "      <td>Customer</td>\n",
       "      <td>Customers would like to transfer money from on...</td>\n",
       "      <td>{'keywords': [{'text': '', 'relevance': 0}], '...</td>\n",
       "      <td>[,  money, NP,  account,  no physical transfer...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R04</td>\n",
       "      <td>Customer</td>\n",
       "      <td>My name is Ryan and I am a customer at the ban...</td>\n",
       "      <td>{'keywords': [{'text': '', 'relevance': 0}], '...</td>\n",
       "      <td>[,  name, NP,  a customer,  the bank,  order, ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R05</td>\n",
       "      <td>Customer</td>\n",
       "      <td>Customer will need to have a feature to apply ...</td>\n",
       "      <td>{'keywords': [{'text': '', 'relevance': 0}], '...</td>\n",
       "      <td>[,  a feature, NP,  a loan,  bank,  a car,  Cu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  User Story ID      User                                        Description  \\\n",
       "0           R01  Customer  A customer would like to deposit cheque so tha...   \n",
       "1           R02  Customer  I am a Customer and I want to withdraw cash fr...   \n",
       "2           R03  Customer  Customers would like to transfer money from on...   \n",
       "3           R04  Customer  My name is Ryan and I am a customer at the ban...   \n",
       "4           R05  Customer  Customer will need to have a feature to apply ...   \n",
       "\n",
       "                                      ClassifiedText  \\\n",
       "0  {'keywords': [{'text': '', 'relevance': 0}], '...   \n",
       "1  {'keywords': [{'text': '', 'relevance': 0}], '...   \n",
       "2  {'keywords': [{'text': '', 'relevance': 0}], '...   \n",
       "3  {'keywords': [{'text': '', 'relevance': 0}], '...   \n",
       "4  {'keywords': [{'text': '', 'relevance': 0}], '...   \n",
       "\n",
       "                                            Keywords DataelementMatchScore  \n",
       "0  [,  A customer, NP,  cheque,  the balance,  li...                        \n",
       "1  [,  cash, NP,  line,  Customer, NAME,  ATM,  w...                        \n",
       "2  [,  money, NP,  account,  no physical transfer...                        \n",
       "3  [,  name, NP,  a customer,  the bank,  order, ...                        \n",
       "4  [,  a feature, NP,  a loan,  bank,  a car,  Cu...                        "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requirements_df.head()\n",
    "#domain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'usage': {'text_characters': 502, 'features': 2, 'text_units': 1}, 'keywords': [{'text': 'A', 'relevance': 0}], 'language': 'en', 'entities': [{'type': 'Person', 'text': 'Stephen Hawking', 'relevance': 0.846941, 'count': 5}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('sample.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augument_SpResponse(responsejson,updateType,text,tag):\n",
    "    \"\"\" Update the NLU response JSON with augumented classifications.\n",
    "    \"\"\"\n",
    "    if(updateType == 'keyword'):\n",
    "        if not any(d.get('text', None) == text for d in responsejson['keywords']):\n",
    "            responsejson['keywords'].append({\"text\":text,\"relevance\":0.5})\n",
    "    else:\n",
    "        if not any(d.get('text', None) == text for d in responsejson['entities']):\n",
    "            responsejson['entities'].append({\"type\":tag,\"text\":text,\"relevance\":0.5,\"count\":1})        \n",
    "    return responsejson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "responsejson = augument_SpResponse(data,'entities','Ryan','Person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'usage': {'text_characters': 502, 'features': 2, 'text_units': 1}, 'keywords': [{'text': 'A', 'relevance': 0}], 'language': 'en', 'entities': [{'type': 'Person', 'text': 'Stephen Hawking', 'relevance': 0.846941, 'count': 5}, {'type': 'Person', 'text': 'Ryan', 'relevance': 0.5, 'count': 1}]}\n"
     ]
    }
   ],
   "source": [
    "#print(responsejson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select*\n"
     ]
    }
   ],
   "source": [
    "x =\"select*\"\n",
    "\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(nlu_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *****   The Following code for uploading and downloading is working\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** this will upload the file UserStroies-V0.1.xlsx in the S3\n",
    "First delete the file if you want to see it in S3 \n",
    "While downloading the png it will be downloaded as \"MYLOCALIMAGE.PNG\" in the location where you started the jupyter\n",
    "Credentials for S3 are provided above in section 2.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done uploading\n",
      "Done downloading\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "import pandas as pd\n",
    "data = None\n",
    "\n",
    "ACCESS_KEY_ID = ''\n",
    "ACCESS_SECRET_KEY = ''\n",
    "BUCKET_NAME = 'software-testing-pyscript'\n",
    "KEY = 'Banking-BRD.xlsx' # replace with your object key\n",
    "\n",
    "\n",
    "data = open('Banking-BRD.xlsx', 'rb' )\n",
    "\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=ACCESS_SECRET_KEY,\n",
    "    config=Config(signature_version='s3v4')\n",
    ")\n",
    "\n",
    "#s3.put_object(Bucket=BUCKET_NAME, Key=KEY, Body=data)\n",
    "\n",
    "print(\"Done uploading\")\n",
    "\n",
    "\n",
    "\n",
    "s3.download_file(BUCKET_NAME,KEY,\".//test/MYLOCALEXCELBRD_mod.xlsx\")\n",
    "    \n",
    "print(\"Done downloading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User Story ID</th>\n",
       "      <th>User</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R01</td>\n",
       "      <td>Customer</td>\n",
       "      <td>A customer would like to deposit cheque so tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R02</td>\n",
       "      <td>Customer</td>\n",
       "      <td>I am a Customer and I want to withdraw cash fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R03</td>\n",
       "      <td>Customer</td>\n",
       "      <td>Customers would like to transfer money from on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R04</td>\n",
       "      <td>Customer</td>\n",
       "      <td>My name is Ryan and I am a customer at the ban...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R05</td>\n",
       "      <td>Customer</td>\n",
       "      <td>Customer will need to have a feature to apply ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  User Story ID      User                                        Description\n",
       "0           R01  Customer  A customer would like to deposit cheque so tha...\n",
       "1           R02  Customer  I am a Customer and I want to withdraw cash fr...\n",
       "2           R03  Customer  Customers would like to transfer money from on...\n",
       "3           R04  Customer  My name is Ryan and I am a customer at the ban...\n",
       "4           R05  Customer  Customer will need to have a feature to apply ..."
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"MYLOCALEXCELBRD_mod.xlsx\",\"Banking-Requirements\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Banking-Requirements']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xl = pd.ExcelFile(\"MYLOCALEXCELBRD_mod.xlsx\")\n",
    "xl.sheet_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object DataFrame.iterrows at 0x1a133226d0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = xl.parse(\"Banking-Requirements\")\n",
    "df.iterrows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"configuration\": {\\n    \"classification\": {\\n      \"stages\": [\\n        {\\n          \"name\": \"Base Tagging\",\\n          \"steps\": [\\n            {\\n              \"type\": \"keywords\",\\n              \"keywords\": [\\n                {\\n                  \"tag\": \"chart\",\\n                  \"text\": \"bar\"\\n                },\\n                { \\n                  \"tag\": \"chart\",\\n                  \"text\": \"line\"\\n                },\\n                {\\n                  \"tag\": \"chart\",\\n                  \"text\": \"pie\"\\n                },\\n                {\\n                  \"tag\": \"UI\",\\n                  \"text\": \"visualization\"\\n                },\\n                {\\n                  \"tag\": \"edition\",\\n                  \"text\": \"editions\"\\n                },\\n                {\\n                  \"tag\": \"country\",\\n                  \"text\": \"countries\"\\n                },\\n                {\\n                  \"tag\": \"medal\",\\n                  \"text\": \"medals\"\\n                },\\n                {\\n                  \"tag\": \"edition\",\\n                  \"text\": \"years\"\\n                },\\n                {\\n                  \"tag\": \"login\",\\n                  \"text\": \"authentication\"\\n                },\\n                {\\n                  \"tag\": \"login\",\\n                  \"text\": \"password\"\\n                },\\n                {\\n                  \"tag\": \"login\",\\n                  \"text\": \"username\"\\n                },\\n                {\\n                  \"tag\": \"login\",\\n                  \"text\": \"credentials\"\\n                },\\n                {\\n                  \"tag\": \"websocket\",\\n                  \"text\": \"socket\"\\n                }\\n              ]\\n            },\\n            {\\n              \"type\": \"d_regex\",\\n              \"d_regex\": [\\n                {\\n                  \"tag\": \"Number\",\\n                  \"pattern\": \"[0-9]{10}\"\\n                }\\n              ]\\n            }\\n          ]\\n        }\\n      ]\\n    }\\n  }\\n}\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = open(\"sample_config.txt\")\n",
    "\n",
    "config.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from xlrd import open_workbook\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "class BRD:\n",
    "    def __init__(self, usrStryID, asa, action, goal):\n",
    "        self.usrStryID = usrStryID\n",
    "        self.asa = asa\n",
    "        self.action = action\n",
    "        self.goal = goal\n",
    "        \n",
    "        #Reads the spreadsheet from the file location\n",
    "#wb = open_workbook(\"MYLOCALEXCEL.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stop-words in /anaconda3/lib/python3.6/site-packages (2015.2.23.1)\n",
      "\u001b[31mboto3 1.7.11 has requirement botocore<1.11.0,>=1.10.11, but you'll have botocore 1.10.10 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    " # Prepares the list of Stop words which can be ignored like the, can , am etc\n",
    "!pip install stop-words\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "stopWords = get_stop_words('english')\n",
    "# List of words to be ignored for text similarity\n",
    "stopWords.extend([\"The\",\"This\",\"That\",\".\",\"!\",\"?\"])\n",
    "#stop_words = set(stopwords.words('english'))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/swaroopmishra/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/swaroopmishra/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Customer']\n",
      "*********************************************\n",
      "Tagged: [('Customer', 'NN')]\n",
      "----> (S Customer/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')])]\n",
      "===> (S Customer/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')])]\n",
      "['A customer would like to deposit cheque so that he can increase the balance']\n",
      "*********************************************\n",
      "Tagged: [('A', 'DT'), ('customer', 'NN'), ('would', 'MD'), ('like', 'VB'), ('to', 'TO'), ('deposit', 'VB'), ('cheque', 'NN'), ('so', 'RB'), ('that', 'IN'), ('he', 'PRP'), ('can', 'MD'), ('increase', 'VB'), ('the', 'DT'), ('balance', 'NN')]\n",
      "----> (S\n",
      "  A/DT\n",
      "  customer/NN\n",
      "  would/MD\n",
      "  (Chunk like/VB)\n",
      "  to/TO\n",
      "  (Chunk deposit/VB)\n",
      "  cheque/NN\n",
      "  so/RB\n",
      "  that/IN\n",
      "  he/PRP\n",
      "  can/MD\n",
      "  (Chunk increase/VB)\n",
      "  the/DT\n",
      "  balance/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')]), Tree('S', [('A', 'DT'), ('customer', 'NN'), ('would', 'MD'), Tree('Chunk', [('like', 'VB')]), ('to', 'TO'), Tree('Chunk', [('deposit', 'VB')]), ('cheque', 'NN'), ('so', 'RB'), ('that', 'IN'), ('he', 'PRP'), ('can', 'MD'), Tree('Chunk', [('increase', 'VB')]), ('the', 'DT'), ('balance', 'NN')])]\n",
      "===> (S\n",
      "  A/DT\n",
      "  customer/NN\n",
      "  would/MD\n",
      "  like/VB\n",
      "  to/TO\n",
      "  deposit/VB\n",
      "  cheque/NN\n",
      "  so/RB\n",
      "  that/IN\n",
      "  he/PRP\n",
      "  can/MD\n",
      "  increase/VB\n",
      "  the/DT\n",
      "  balance/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')]), Tree('S', [('A', 'DT'), ('customer', 'NN'), ('would', 'MD'), Tree('Chunk', [('like', 'VB')]), ('to', 'TO'), Tree('Chunk', [('deposit', 'VB')]), ('cheque', 'NN'), ('so', 'RB'), ('that', 'IN'), ('he', 'PRP'), ('can', 'MD'), Tree('Chunk', [('increase', 'VB')]), ('the', 'DT'), ('balance', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')]), Tree('S', [('A', 'DT'), ('customer', 'NN'), ('would', 'MD'), ('like', 'VB'), ('to', 'TO'), ('deposit', 'VB'), ('cheque', 'NN'), ('so', 'RB'), ('that', 'IN'), ('he', 'PRP'), ('can', 'MD'), ('increase', 'VB'), ('the', 'DT'), ('balance', 'NN')])]\n",
      "Filtered Sentence--> ['Customer', 'A', 'customer', 'like', 'deposit', 'cheque', 'can', 'increase', 'balance']\n",
      "['Customer']\n",
      "*********************************************\n",
      "Tagged: [('Customer', 'NN')]\n",
      "----> (S Customer/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')])]\n",
      "===> (S Customer/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')])]\n",
      "['I am a Customer and I want to withdraw cash from an ATM so that I don’t have to wait in line']\n",
      "*********************************************\n",
      "Tagged: [('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('Customer', 'NNP'), ('and', 'CC'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('withdraw', 'VB'), ('cash', 'NN'), ('from', 'IN'), ('an', 'DT'), ('ATM', 'NNP'), ('so', 'IN'), ('that', 'IN'), ('I', 'PRP'), ('don', 'VBP'), ('’', 'JJ'), ('t', 'NNS'), ('have', 'VBP'), ('to', 'TO'), ('wait', 'VB'), ('in', 'IN'), ('line', 'NN')]\n",
      "----> (S\n",
      "  I/PRP\n",
      "  (Chunk am/VBP)\n",
      "  a/DT\n",
      "  Customer/NNP\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  (Chunk want/VBP)\n",
      "  to/TO\n",
      "  (Chunk withdraw/VB)\n",
      "  cash/NN\n",
      "  from/IN\n",
      "  an/DT\n",
      "  ATM/NNP\n",
      "  so/IN\n",
      "  that/IN\n",
      "  I/PRP\n",
      "  (Chunk don/VBP)\n",
      "  ’/JJ\n",
      "  t/NNS\n",
      "  (Chunk have/VBP)\n",
      "  to/TO\n",
      "  (Chunk wait/VB)\n",
      "  in/IN\n",
      "  line/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')]), Tree('S', [('I', 'PRP'), Tree('Chunk', [('am', 'VBP')]), ('a', 'DT'), ('Customer', 'NNP'), ('and', 'CC'), ('I', 'PRP'), Tree('Chunk', [('want', 'VBP')]), ('to', 'TO'), Tree('Chunk', [('withdraw', 'VB')]), ('cash', 'NN'), ('from', 'IN'), ('an', 'DT'), ('ATM', 'NNP'), ('so', 'IN'), ('that', 'IN'), ('I', 'PRP'), Tree('Chunk', [('don', 'VBP')]), ('’', 'JJ'), ('t', 'NNS'), Tree('Chunk', [('have', 'VBP')]), ('to', 'TO'), Tree('Chunk', [('wait', 'VB')]), ('in', 'IN'), ('line', 'NN')])]\n",
      "===> (S\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  a/DT\n",
      "  (Chunk Customer/NNP)\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  want/VBP\n",
      "  to/TO\n",
      "  withdraw/VB\n",
      "  cash/NN\n",
      "  from/IN\n",
      "  an/DT\n",
      "  (Chunk ATM/NNP)\n",
      "  so/IN\n",
      "  that/IN\n",
      "  I/PRP\n",
      "  don/VBP\n",
      "  ’/JJ\n",
      "  t/NNS\n",
      "  have/VBP\n",
      "  to/TO\n",
      "  wait/VB\n",
      "  in/IN\n",
      "  line/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')]), Tree('S', [('I', 'PRP'), Tree('Chunk', [('am', 'VBP')]), ('a', 'DT'), ('Customer', 'NNP'), ('and', 'CC'), ('I', 'PRP'), Tree('Chunk', [('want', 'VBP')]), ('to', 'TO'), Tree('Chunk', [('withdraw', 'VB')]), ('cash', 'NN'), ('from', 'IN'), ('an', 'DT'), ('ATM', 'NNP'), ('so', 'IN'), ('that', 'IN'), ('I', 'PRP'), Tree('Chunk', [('don', 'VBP')]), ('’', 'JJ'), ('t', 'NNS'), Tree('Chunk', [('have', 'VBP')]), ('to', 'TO'), Tree('Chunk', [('wait', 'VB')]), ('in', 'IN'), ('line', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')]), Tree('S', [('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), Tree('Chunk', [('Customer', 'NNP')]), ('and', 'CC'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('withdraw', 'VB'), ('cash', 'NN'), ('from', 'IN'), ('an', 'DT'), Tree('Chunk', [('ATM', 'NNP')]), ('so', 'IN'), ('that', 'IN'), ('I', 'PRP'), ('don', 'VBP'), ('’', 'JJ'), ('t', 'NNS'), ('have', 'VBP'), ('to', 'TO'), ('wait', 'VB'), ('in', 'IN'), ('line', 'NN')])]\n",
      "Filtered Sentence--> ['Customer', 'I', 'Customer', 'I', 'want', 'withdraw', 'cash', 'ATM', 'I', 'don', '’', 't', 'wait', 'line']\n",
      "['Customer']\n",
      "*********************************************\n",
      "Tagged: [('Customer', 'NN')]\n",
      "----> (S Customer/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')])]\n",
      "===> (S Customer/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')])]\n",
      "['Customers would like to transfer money from one account to another so that there is no physical transfer of money']\n",
      "*********************************************\n",
      "Tagged: [('Customers', 'NNS'), ('would', 'MD'), ('like', 'VB'), ('to', 'TO'), ('transfer', 'VB'), ('money', 'NN'), ('from', 'IN'), ('one', 'CD'), ('account', 'NN'), ('to', 'TO'), ('another', 'DT'), ('so', 'RB'), ('that', 'IN'), ('there', 'EX'), ('is', 'VBZ'), ('no', 'DT'), ('physical', 'JJ'), ('transfer', 'NN'), ('of', 'IN'), ('money', 'NN')]\n",
      "----> (S\n",
      "  Customers/NNS\n",
      "  would/MD\n",
      "  (Chunk like/VB)\n",
      "  to/TO\n",
      "  (Chunk transfer/VB)\n",
      "  money/NN\n",
      "  from/IN\n",
      "  one/CD\n",
      "  account/NN\n",
      "  to/TO\n",
      "  another/DT\n",
      "  so/RB\n",
      "  that/IN\n",
      "  there/EX\n",
      "  (Chunk is/VBZ)\n",
      "  no/DT\n",
      "  physical/JJ\n",
      "  transfer/NN\n",
      "  of/IN\n",
      "  money/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')]), Tree('S', [('Customers', 'NNS'), ('would', 'MD'), Tree('Chunk', [('like', 'VB')]), ('to', 'TO'), Tree('Chunk', [('transfer', 'VB')]), ('money', 'NN'), ('from', 'IN'), ('one', 'CD'), ('account', 'NN'), ('to', 'TO'), ('another', 'DT'), ('so', 'RB'), ('that', 'IN'), ('there', 'EX'), Tree('Chunk', [('is', 'VBZ')]), ('no', 'DT'), ('physical', 'JJ'), ('transfer', 'NN'), ('of', 'IN'), ('money', 'NN')])]\n",
      "===> (S\n",
      "  Customers/NNS\n",
      "  would/MD\n",
      "  like/VB\n",
      "  to/TO\n",
      "  transfer/VB\n",
      "  money/NN\n",
      "  from/IN\n",
      "  one/CD\n",
      "  account/NN\n",
      "  to/TO\n",
      "  another/DT\n",
      "  so/RB\n",
      "  that/IN\n",
      "  there/EX\n",
      "  is/VBZ\n",
      "  no/DT\n",
      "  physical/JJ\n",
      "  transfer/NN\n",
      "  of/IN\n",
      "  money/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')]), Tree('S', [('Customers', 'NNS'), ('would', 'MD'), Tree('Chunk', [('like', 'VB')]), ('to', 'TO'), Tree('Chunk', [('transfer', 'VB')]), ('money', 'NN'), ('from', 'IN'), ('one', 'CD'), ('account', 'NN'), ('to', 'TO'), ('another', 'DT'), ('so', 'RB'), ('that', 'IN'), ('there', 'EX'), Tree('Chunk', [('is', 'VBZ')]), ('no', 'DT'), ('physical', 'JJ'), ('transfer', 'NN'), ('of', 'IN'), ('money', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')]), Tree('S', [('Customers', 'NNS'), ('would', 'MD'), ('like', 'VB'), ('to', 'TO'), ('transfer', 'VB'), ('money', 'NN'), ('from', 'IN'), ('one', 'CD'), ('account', 'NN'), ('to', 'TO'), ('another', 'DT'), ('so', 'RB'), ('that', 'IN'), ('there', 'EX'), ('is', 'VBZ'), ('no', 'DT'), ('physical', 'JJ'), ('transfer', 'NN'), ('of', 'IN'), ('money', 'NN')])]\n",
      "Filtered Sentence--> ['Customer', 'Customers', 'like', 'transfer', 'money', 'one', 'account', 'another', 'physical', 'transfer', 'money']\n",
      "['Customer']\n",
      "*********************************************\n",
      "Tagged: [('Customer', 'NN')]\n",
      "----> (S Customer/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')])]\n",
      "===> (S Customer/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')])]\n",
      "['My name is Ryan and I am a customer at the bank, and I want to order checks so that I don’t need to write checks or use postal service']\n",
      "*********************************************\n",
      "Tagged: [('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Ryan', 'NNP'), ('and', 'CC'), ('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('customer', 'NN'), ('at', 'IN'), ('the', 'DT'), ('bank', 'NN'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('order', 'NN'), ('checks', 'NNS'), ('so', 'IN'), ('that', 'IN'), ('I', 'PRP'), ('don', 'VBP'), ('’', 'JJ'), ('t', 'NNS'), ('need', 'VBP'), ('to', 'TO'), ('write', 'VB'), ('checks', 'NNS'), ('or', 'CC'), ('use', 'VB'), ('postal', 'JJ'), ('service', 'NN')]\n",
      "----> (S\n",
      "  My/PRP$\n",
      "  name/NN\n",
      "  (Chunk is/VBZ)\n",
      "  Ryan/NNP\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  (Chunk am/VBP)\n",
      "  a/DT\n",
      "  customer/NN\n",
      "  at/IN\n",
      "  the/DT\n",
      "  bank/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  (Chunk want/VBP)\n",
      "  to/TO\n",
      "  order/NN\n",
      "  checks/NNS\n",
      "  so/IN\n",
      "  that/IN\n",
      "  I/PRP\n",
      "  (Chunk don/VBP)\n",
      "  ’/JJ\n",
      "  t/NNS\n",
      "  (Chunk need/VBP)\n",
      "  to/TO\n",
      "  (Chunk write/VB)\n",
      "  checks/NNS\n",
      "  or/CC\n",
      "  (Chunk use/VB)\n",
      "  postal/JJ\n",
      "  service/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')]), Tree('S', [('My', 'PRP$'), ('name', 'NN'), Tree('Chunk', [('is', 'VBZ')]), ('Ryan', 'NNP'), ('and', 'CC'), ('I', 'PRP'), Tree('Chunk', [('am', 'VBP')]), ('a', 'DT'), ('customer', 'NN'), ('at', 'IN'), ('the', 'DT'), ('bank', 'NN'), (',', ','), ('and', 'CC'), ('I', 'PRP'), Tree('Chunk', [('want', 'VBP')]), ('to', 'TO'), ('order', 'NN'), ('checks', 'NNS'), ('so', 'IN'), ('that', 'IN'), ('I', 'PRP'), Tree('Chunk', [('don', 'VBP')]), ('’', 'JJ'), ('t', 'NNS'), Tree('Chunk', [('need', 'VBP')]), ('to', 'TO'), Tree('Chunk', [('write', 'VB')]), ('checks', 'NNS'), ('or', 'CC'), Tree('Chunk', [('use', 'VB')]), ('postal', 'JJ'), ('service', 'NN')])]\n",
      "===> (S\n",
      "  My/PRP$\n",
      "  name/NN\n",
      "  is/VBZ\n",
      "  (Chunk Ryan/NNP)\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  a/DT\n",
      "  customer/NN\n",
      "  at/IN\n",
      "  the/DT\n",
      "  bank/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  want/VBP\n",
      "  to/TO\n",
      "  order/NN\n",
      "  checks/NNS\n",
      "  so/IN\n",
      "  that/IN\n",
      "  I/PRP\n",
      "  don/VBP\n",
      "  ’/JJ\n",
      "  t/NNS\n",
      "  need/VBP\n",
      "  to/TO\n",
      "  write/VB\n",
      "  checks/NNS\n",
      "  or/CC\n",
      "  use/VB\n",
      "  postal/JJ\n",
      "  service/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')]), Tree('S', [('My', 'PRP$'), ('name', 'NN'), Tree('Chunk', [('is', 'VBZ')]), ('Ryan', 'NNP'), ('and', 'CC'), ('I', 'PRP'), Tree('Chunk', [('am', 'VBP')]), ('a', 'DT'), ('customer', 'NN'), ('at', 'IN'), ('the', 'DT'), ('bank', 'NN'), (',', ','), ('and', 'CC'), ('I', 'PRP'), Tree('Chunk', [('want', 'VBP')]), ('to', 'TO'), ('order', 'NN'), ('checks', 'NNS'), ('so', 'IN'), ('that', 'IN'), ('I', 'PRP'), Tree('Chunk', [('don', 'VBP')]), ('’', 'JJ'), ('t', 'NNS'), Tree('Chunk', [('need', 'VBP')]), ('to', 'TO'), Tree('Chunk', [('write', 'VB')]), ('checks', 'NNS'), ('or', 'CC'), Tree('Chunk', [('use', 'VB')]), ('postal', 'JJ'), ('service', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')]), Tree('S', [('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), Tree('Chunk', [('Ryan', 'NNP')]), ('and', 'CC'), ('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('customer', 'NN'), ('at', 'IN'), ('the', 'DT'), ('bank', 'NN'), (',', ','), ('and', 'CC'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('order', 'NN'), ('checks', 'NNS'), ('so', 'IN'), ('that', 'IN'), ('I', 'PRP'), ('don', 'VBP'), ('’', 'JJ'), ('t', 'NNS'), ('need', 'VBP'), ('to', 'TO'), ('write', 'VB'), ('checks', 'NNS'), ('or', 'CC'), ('use', 'VB'), ('postal', 'JJ'), ('service', 'NN')])]\n",
      "Filtered Sentence--> ['Customer', 'My', 'name', 'Ryan', 'I', 'customer', 'bank', ',', 'I', 'want', 'order', 'checks', 'I', 'don', '’', 't', 'need', 'write', 'checks', 'use', 'postal', 'service']\n",
      "['Customer']\n",
      "*********************************************\n",
      "Tagged: [('Customer', 'NN')]\n",
      "----> (S Customer/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')])]\n",
      "===> (S Customer/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')])]\n",
      "['Customer will need to have a feature to apply for a loan from bank so that he can purchase a car']\n",
      "*********************************************\n",
      "Tagged: [('Customer', 'NNP'), ('will', 'MD'), ('need', 'VB'), ('to', 'TO'), ('have', 'VB'), ('a', 'DT'), ('feature', 'NN'), ('to', 'TO'), ('apply', 'VB'), ('for', 'IN'), ('a', 'DT'), ('loan', 'NN'), ('from', 'IN'), ('bank', 'NN'), ('so', 'IN'), ('that', 'IN'), ('he', 'PRP'), ('can', 'MD'), ('purchase', 'VB'), ('a', 'DT'), ('car', 'NN')]\n",
      "----> (S\n",
      "  Customer/NNP\n",
      "  will/MD\n",
      "  (Chunk need/VB)\n",
      "  to/TO\n",
      "  (Chunk have/VB)\n",
      "  a/DT\n",
      "  feature/NN\n",
      "  to/TO\n",
      "  (Chunk apply/VB)\n",
      "  for/IN\n",
      "  a/DT\n",
      "  loan/NN\n",
      "  from/IN\n",
      "  bank/NN\n",
      "  so/IN\n",
      "  that/IN\n",
      "  he/PRP\n",
      "  can/MD\n",
      "  (Chunk purchase/VB)\n",
      "  a/DT\n",
      "  car/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Customer', 'NN')]), Tree('S', [('Customer', 'NNP'), ('will', 'MD'), Tree('Chunk', [('need', 'VB')]), ('to', 'TO'), Tree('Chunk', [('have', 'VB')]), ('a', 'DT'), ('feature', 'NN'), ('to', 'TO'), Tree('Chunk', [('apply', 'VB')]), ('for', 'IN'), ('a', 'DT'), ('loan', 'NN'), ('from', 'IN'), ('bank', 'NN'), ('so', 'IN'), ('that', 'IN'), ('he', 'PRP'), ('can', 'MD'), Tree('Chunk', [('purchase', 'VB')]), ('a', 'DT'), ('car', 'NN')])]\n",
      "===> (S\n",
      "  (Chunk Customer/NNP)\n",
      "  will/MD\n",
      "  need/VB\n",
      "  to/TO\n",
      "  have/VB\n",
      "  a/DT\n",
      "  feature/NN\n",
      "  to/TO\n",
      "  apply/VB\n",
      "  for/IN\n",
      "  a/DT\n",
      "  loan/NN\n",
      "  from/IN\n",
      "  bank/NN\n",
      "  so/IN\n",
      "  that/IN\n",
      "  he/PRP\n",
      "  can/MD\n",
      "  purchase/VB\n",
      "  a/DT\n",
      "  car/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Customer', 'NN')]), Tree('S', [('Customer', 'NNP'), ('will', 'MD'), Tree('Chunk', [('need', 'VB')]), ('to', 'TO'), Tree('Chunk', [('have', 'VB')]), ('a', 'DT'), ('feature', 'NN'), ('to', 'TO'), Tree('Chunk', [('apply', 'VB')]), ('for', 'IN'), ('a', 'DT'), ('loan', 'NN'), ('from', 'IN'), ('bank', 'NN'), ('so', 'IN'), ('that', 'IN'), ('he', 'PRP'), ('can', 'MD'), Tree('Chunk', [('purchase', 'VB')]), ('a', 'DT'), ('car', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Customer', 'NN')]), Tree('S', [Tree('Chunk', [('Customer', 'NNP')]), ('will', 'MD'), ('need', 'VB'), ('to', 'TO'), ('have', 'VB'), ('a', 'DT'), ('feature', 'NN'), ('to', 'TO'), ('apply', 'VB'), ('for', 'IN'), ('a', 'DT'), ('loan', 'NN'), ('from', 'IN'), ('bank', 'NN'), ('so', 'IN'), ('that', 'IN'), ('he', 'PRP'), ('can', 'MD'), ('purchase', 'VB'), ('a', 'DT'), ('car', 'NN')])]\n",
      "Filtered Sentence--> ['Customer', 'Customer', 'will', 'need', 'feature', 'apply', 'loan', 'bank', 'can', 'purchase', 'car']\n",
      "['Banker']\n",
      "*********************************************\n",
      "Tagged: [('Banker', 'NN')]\n",
      "----> (S Banker/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Banker', 'NN')])]\n",
      "===> (S Banker/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Banker', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Banker', 'NN')])]\n",
      "['Banker should be able to restock sufficient cash in ATM.', 'This will help customer to withdraw cash.']\n",
      "*********************************************\n",
      "Tagged: [('Banker', 'NNP'), ('should', 'MD'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('restock', 'VB'), ('sufficient', 'JJ'), ('cash', 'NN'), ('in', 'IN'), ('ATM', 'NNP'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), ('help', 'VB'), ('customer', 'NN'), ('to', 'TO'), ('withdraw', 'VB'), ('cash', 'NN'), ('.', '.')]\n",
      "----> (S\n",
      "  Banker/NNP\n",
      "  should/MD\n",
      "  (Chunk be/VB)\n",
      "  able/JJ\n",
      "  to/TO\n",
      "  (Chunk restock/VB)\n",
      "  sufficient/JJ\n",
      "  cash/NN\n",
      "  in/IN\n",
      "  ATM/NNP\n",
      "  ./.\n",
      "  This/DT\n",
      "  will/MD\n",
      "  (Chunk help/VB)\n",
      "  customer/NN\n",
      "  to/TO\n",
      "  (Chunk withdraw/VB)\n",
      "  cash/NN\n",
      "  ./.)\n",
      "Grouped VERBS :: [Tree('S', [('Banker', 'NN')]), Tree('S', [('Banker', 'NNP'), ('should', 'MD'), Tree('Chunk', [('be', 'VB')]), ('able', 'JJ'), ('to', 'TO'), Tree('Chunk', [('restock', 'VB')]), ('sufficient', 'JJ'), ('cash', 'NN'), ('in', 'IN'), ('ATM', 'NNP'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), Tree('Chunk', [('help', 'VB')]), ('customer', 'NN'), ('to', 'TO'), Tree('Chunk', [('withdraw', 'VB')]), ('cash', 'NN'), ('.', '.')])]\n",
      "===> (S\n",
      "  (Chunk Banker/NNP)\n",
      "  should/MD\n",
      "  be/VB\n",
      "  able/JJ\n",
      "  to/TO\n",
      "  restock/VB\n",
      "  sufficient/JJ\n",
      "  cash/NN\n",
      "  in/IN\n",
      "  (Chunk ATM/NNP)\n",
      "  ./.\n",
      "  This/DT\n",
      "  will/MD\n",
      "  help/VB\n",
      "  customer/NN\n",
      "  to/TO\n",
      "  withdraw/VB\n",
      "  cash/NN\n",
      "  ./.)\n",
      "Chunked Verbs:: [Tree('S', [('Banker', 'NN')]), Tree('S', [('Banker', 'NNP'), ('should', 'MD'), Tree('Chunk', [('be', 'VB')]), ('able', 'JJ'), ('to', 'TO'), Tree('Chunk', [('restock', 'VB')]), ('sufficient', 'JJ'), ('cash', 'NN'), ('in', 'IN'), ('ATM', 'NNP'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), Tree('Chunk', [('help', 'VB')]), ('customer', 'NN'), ('to', 'TO'), Tree('Chunk', [('withdraw', 'VB')]), ('cash', 'NN'), ('.', '.')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Banker', 'NN')]), Tree('S', [Tree('Chunk', [('Banker', 'NNP')]), ('should', 'MD'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('restock', 'VB'), ('sufficient', 'JJ'), ('cash', 'NN'), ('in', 'IN'), Tree('Chunk', [('ATM', 'NNP')]), ('.', '.'), ('This', 'DT'), ('will', 'MD'), ('help', 'VB'), ('customer', 'NN'), ('to', 'TO'), ('withdraw', 'VB'), ('cash', 'NN'), ('.', '.')])]\n",
      "Filtered Sentence--> ['Banker', 'Banker', 'able', 'restock', 'sufficient', 'cash', 'ATM', 'will', 'help', 'customer', 'withdraw', 'cash']\n",
      "['Banker']\n",
      "*********************************************\n",
      "Tagged: [('Banker', 'NN')]\n",
      "----> (S Banker/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Banker', 'NN')])]\n",
      "===> (S Banker/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Banker', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Banker', 'NN')])]\n",
      "['Banker will need to limit the cash withdrwal from ATM.', 'This will help more customers can prevail the ATM cash and also prevent fradulent activities']\n",
      "*********************************************\n",
      "Tagged: [('Banker', 'NNP'), ('will', 'MD'), ('need', 'VB'), ('to', 'TO'), ('limit', 'VB'), ('the', 'DT'), ('cash', 'NN'), ('withdrwal', 'NN'), ('from', 'IN'), ('ATM', 'NNP'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), ('help', 'VB'), ('more', 'JJR'), ('customers', 'NNS'), ('can', 'MD'), ('prevail', 'VB'), ('the', 'DT'), ('ATM', 'NNP'), ('cash', 'NN'), ('and', 'CC'), ('also', 'RB'), ('prevent', 'JJ'), ('fradulent', 'NN'), ('activities', 'NNS')]\n",
      "----> (S\n",
      "  Banker/NNP\n",
      "  will/MD\n",
      "  (Chunk need/VB)\n",
      "  to/TO\n",
      "  (Chunk limit/VB)\n",
      "  the/DT\n",
      "  cash/NN\n",
      "  withdrwal/NN\n",
      "  from/IN\n",
      "  ATM/NNP\n",
      "  ./.\n",
      "  This/DT\n",
      "  will/MD\n",
      "  (Chunk help/VB)\n",
      "  more/JJR\n",
      "  customers/NNS\n",
      "  can/MD\n",
      "  (Chunk prevail/VB)\n",
      "  the/DT\n",
      "  ATM/NNP\n",
      "  cash/NN\n",
      "  and/CC\n",
      "  also/RB\n",
      "  prevent/JJ\n",
      "  fradulent/NN\n",
      "  activities/NNS)\n",
      "Grouped VERBS :: [Tree('S', [('Banker', 'NN')]), Tree('S', [('Banker', 'NNP'), ('will', 'MD'), Tree('Chunk', [('need', 'VB')]), ('to', 'TO'), Tree('Chunk', [('limit', 'VB')]), ('the', 'DT'), ('cash', 'NN'), ('withdrwal', 'NN'), ('from', 'IN'), ('ATM', 'NNP'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), Tree('Chunk', [('help', 'VB')]), ('more', 'JJR'), ('customers', 'NNS'), ('can', 'MD'), Tree('Chunk', [('prevail', 'VB')]), ('the', 'DT'), ('ATM', 'NNP'), ('cash', 'NN'), ('and', 'CC'), ('also', 'RB'), ('prevent', 'JJ'), ('fradulent', 'NN'), ('activities', 'NNS')])]\n",
      "===> (S\n",
      "  (Chunk Banker/NNP)\n",
      "  will/MD\n",
      "  need/VB\n",
      "  to/TO\n",
      "  limit/VB\n",
      "  the/DT\n",
      "  cash/NN\n",
      "  withdrwal/NN\n",
      "  from/IN\n",
      "  (Chunk ATM/NNP)\n",
      "  ./.\n",
      "  This/DT\n",
      "  will/MD\n",
      "  help/VB\n",
      "  more/JJR\n",
      "  customers/NNS\n",
      "  can/MD\n",
      "  prevail/VB\n",
      "  the/DT\n",
      "  (Chunk ATM/NNP)\n",
      "  cash/NN\n",
      "  and/CC\n",
      "  also/RB\n",
      "  prevent/JJ\n",
      "  fradulent/NN\n",
      "  activities/NNS)\n",
      "Chunked Verbs:: [Tree('S', [('Banker', 'NN')]), Tree('S', [('Banker', 'NNP'), ('will', 'MD'), Tree('Chunk', [('need', 'VB')]), ('to', 'TO'), Tree('Chunk', [('limit', 'VB')]), ('the', 'DT'), ('cash', 'NN'), ('withdrwal', 'NN'), ('from', 'IN'), ('ATM', 'NNP'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), Tree('Chunk', [('help', 'VB')]), ('more', 'JJR'), ('customers', 'NNS'), ('can', 'MD'), Tree('Chunk', [('prevail', 'VB')]), ('the', 'DT'), ('ATM', 'NNP'), ('cash', 'NN'), ('and', 'CC'), ('also', 'RB'), ('prevent', 'JJ'), ('fradulent', 'NN'), ('activities', 'NNS')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Banker', 'NN')]), Tree('S', [Tree('Chunk', [('Banker', 'NNP')]), ('will', 'MD'), ('need', 'VB'), ('to', 'TO'), ('limit', 'VB'), ('the', 'DT'), ('cash', 'NN'), ('withdrwal', 'NN'), ('from', 'IN'), Tree('Chunk', [('ATM', 'NNP')]), ('.', '.'), ('This', 'DT'), ('will', 'MD'), ('help', 'VB'), ('more', 'JJR'), ('customers', 'NNS'), ('can', 'MD'), ('prevail', 'VB'), ('the', 'DT'), Tree('Chunk', [('ATM', 'NNP')]), ('cash', 'NN'), ('and', 'CC'), ('also', 'RB'), ('prevent', 'JJ'), ('fradulent', 'NN'), ('activities', 'NNS')])]\n",
      "Filtered Sentence--> ['Banker', 'Banker', 'will', 'need', 'limit', 'cash', 'withdrwal', 'ATM', 'will', 'help', 'customers', 'can', 'prevail', 'ATM', 'cash', 'also', 'prevent', 'fradulent', 'activities']\n",
      "['Banker']\n",
      "*********************************************\n",
      "Tagged: [('Banker', 'NN')]\n",
      "----> (S Banker/NN)\n",
      "Grouped VERBS :: [Tree('S', [('Banker', 'NN')])]\n",
      "===> (S Banker/NN)\n",
      "Chunked Verbs:: [Tree('S', [('Banker', 'NN')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Banker', 'NN')])]\n",
      "['Banker should be able to review the credit history and the bank balance of customers that apply for loans .', 'This will help banker to decide wether to approve or reject the loan applications']\n",
      "*********************************************\n",
      "Tagged: [('Banker', 'NNP'), ('should', 'MD'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('review', 'VB'), ('the', 'DT'), ('credit', 'NN'), ('history', 'NN'), ('and', 'CC'), ('the', 'DT'), ('bank', 'NN'), ('balance', 'NN'), ('of', 'IN'), ('customers', 'NNS'), ('that', 'WDT'), ('apply', 'VBP'), ('for', 'IN'), ('loans', 'NNS'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), ('help', 'VB'), ('banker', 'NN'), ('to', 'TO'), ('decide', 'VB'), ('wether', 'JJR'), ('to', 'TO'), ('approve', 'VB'), ('or', 'CC'), ('reject', 'VB'), ('the', 'DT'), ('loan', 'NN'), ('applications', 'NNS')]\n",
      "----> (S\n",
      "  Banker/NNP\n",
      "  should/MD\n",
      "  (Chunk be/VB)\n",
      "  able/JJ\n",
      "  to/TO\n",
      "  (Chunk review/VB)\n",
      "  the/DT\n",
      "  credit/NN\n",
      "  history/NN\n",
      "  and/CC\n",
      "  the/DT\n",
      "  bank/NN\n",
      "  balance/NN\n",
      "  of/IN\n",
      "  customers/NNS\n",
      "  that/WDT\n",
      "  (Chunk apply/VBP)\n",
      "  for/IN\n",
      "  loans/NNS\n",
      "  ./.\n",
      "  This/DT\n",
      "  will/MD\n",
      "  (Chunk help/VB)\n",
      "  banker/NN\n",
      "  to/TO\n",
      "  (Chunk decide/VB)\n",
      "  wether/JJR\n",
      "  to/TO\n",
      "  (Chunk approve/VB)\n",
      "  or/CC\n",
      "  (Chunk reject/VB)\n",
      "  the/DT\n",
      "  loan/NN\n",
      "  applications/NNS)\n",
      "Grouped VERBS :: [Tree('S', [('Banker', 'NN')]), Tree('S', [('Banker', 'NNP'), ('should', 'MD'), Tree('Chunk', [('be', 'VB')]), ('able', 'JJ'), ('to', 'TO'), Tree('Chunk', [('review', 'VB')]), ('the', 'DT'), ('credit', 'NN'), ('history', 'NN'), ('and', 'CC'), ('the', 'DT'), ('bank', 'NN'), ('balance', 'NN'), ('of', 'IN'), ('customers', 'NNS'), ('that', 'WDT'), Tree('Chunk', [('apply', 'VBP')]), ('for', 'IN'), ('loans', 'NNS'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), Tree('Chunk', [('help', 'VB')]), ('banker', 'NN'), ('to', 'TO'), Tree('Chunk', [('decide', 'VB')]), ('wether', 'JJR'), ('to', 'TO'), Tree('Chunk', [('approve', 'VB')]), ('or', 'CC'), Tree('Chunk', [('reject', 'VB')]), ('the', 'DT'), ('loan', 'NN'), ('applications', 'NNS')])]\n",
      "===> (S\n",
      "  (Chunk Banker/NNP)\n",
      "  should/MD\n",
      "  be/VB\n",
      "  able/JJ\n",
      "  to/TO\n",
      "  review/VB\n",
      "  the/DT\n",
      "  credit/NN\n",
      "  history/NN\n",
      "  and/CC\n",
      "  the/DT\n",
      "  bank/NN\n",
      "  balance/NN\n",
      "  of/IN\n",
      "  customers/NNS\n",
      "  that/WDT\n",
      "  apply/VBP\n",
      "  for/IN\n",
      "  loans/NNS\n",
      "  ./.\n",
      "  This/DT\n",
      "  will/MD\n",
      "  help/VB\n",
      "  banker/NN\n",
      "  to/TO\n",
      "  decide/VB\n",
      "  wether/JJR\n",
      "  to/TO\n",
      "  approve/VB\n",
      "  or/CC\n",
      "  reject/VB\n",
      "  the/DT\n",
      "  loan/NN\n",
      "  applications/NNS)\n",
      "Chunked Verbs:: [Tree('S', [('Banker', 'NN')]), Tree('S', [('Banker', 'NNP'), ('should', 'MD'), Tree('Chunk', [('be', 'VB')]), ('able', 'JJ'), ('to', 'TO'), Tree('Chunk', [('review', 'VB')]), ('the', 'DT'), ('credit', 'NN'), ('history', 'NN'), ('and', 'CC'), ('the', 'DT'), ('bank', 'NN'), ('balance', 'NN'), ('of', 'IN'), ('customers', 'NNS'), ('that', 'WDT'), Tree('Chunk', [('apply', 'VBP')]), ('for', 'IN'), ('loans', 'NNS'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), Tree('Chunk', [('help', 'VB')]), ('banker', 'NN'), ('to', 'TO'), Tree('Chunk', [('decide', 'VB')]), ('wether', 'JJR'), ('to', 'TO'), Tree('Chunk', [('approve', 'VB')]), ('or', 'CC'), Tree('Chunk', [('reject', 'VB')]), ('the', 'DT'), ('loan', 'NN'), ('applications', 'NNS')])]\n",
      "Chunked ProNOuns:: [Tree('S', [('Banker', 'NN')]), Tree('S', [Tree('Chunk', [('Banker', 'NNP')]), ('should', 'MD'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('review', 'VB'), ('the', 'DT'), ('credit', 'NN'), ('history', 'NN'), ('and', 'CC'), ('the', 'DT'), ('bank', 'NN'), ('balance', 'NN'), ('of', 'IN'), ('customers', 'NNS'), ('that', 'WDT'), ('apply', 'VBP'), ('for', 'IN'), ('loans', 'NNS'), ('.', '.'), ('This', 'DT'), ('will', 'MD'), ('help', 'VB'), ('banker', 'NN'), ('to', 'TO'), ('decide', 'VB'), ('wether', 'JJR'), ('to', 'TO'), ('approve', 'VB'), ('or', 'CC'), ('reject', 'VB'), ('the', 'DT'), ('loan', 'NN'), ('applications', 'NNS')])]\n",
      "Filtered Sentence--> ['Banker', 'Banker', 'able', 'review', 'credit', 'history', 'bank', 'balance', 'customers', 'apply', 'loans', 'will', 'help', 'banker', 'decide', 'wether', 'approve', 'reject', 'loan', 'applications']\n"
     ]
    }
   ],
   "source": [
    "wb = open_workbook(\"MYLOCALEXCELBRD_mod.xlsx\")\n",
    "# FOr each sheet in Spreadsheet\n",
    "for sheet in wb.sheets():\n",
    "        numberRows = sheet.nrows\n",
    "        numberCols = sheet.ncols\n",
    "        \n",
    "        items = []\n",
    "        rows = []\n",
    "        # For each row in a workbook\n",
    "        for row in range(1, numberRows):\n",
    "            values =  []\n",
    "            filtered_Sentence = []\n",
    "            verbs = []\n",
    "            proNouns = []\n",
    "            # For each columns in the workbook\n",
    "            for col in range(1, numberCols):\n",
    "                value = (sheet.cell(row, col).value)\n",
    "                values.append(value)\n",
    "                # tokenize the words in the sentence\n",
    "                print(sent_tokenize(value))\n",
    "                print(\"*********************************************\")\n",
    "                words = word_tokenize(value)\n",
    "                # Words are tagged so, that they can be identified which one is verb/Noun/ Pronoun\n",
    "                tagged = nltk.pos_tag(words)\n",
    "                print (\"Tagged:\",tagged)\n",
    "                #Retrieves Verb from the Sentence or tokens\n",
    "                chunkVerb = r\"\"\"Chunk: {<VB.?>*} \"\"\"\n",
    "                chunkProNoun = r\"\"\"Chunk: {<NNP.?>*} \"\"\"\n",
    "                \n",
    "                chunkParser = nltk.RegexpParser(chunkVerb)\n",
    "                chunked = chunkParser.parse(tagged)\n",
    "                print (\"---->\",chunked)\n",
    "                verbs.append(chunked)\n",
    "                print (\"Grouped VERBS ::\", verbs)\n",
    "                chunkParser = nltk.RegexpParser(chunkProNoun)\n",
    "                chunked = chunkParser.parse(tagged)\n",
    "                print(\"===>\",chunked)\n",
    "                proNouns.append(chunked)\n",
    "                print (\"Chunked Verbs::\",verbs)\n",
    "                print (\"Chunked ProNOuns::\",proNouns)\n",
    "                \n",
    "                #chunked.draw()\n",
    "                \n",
    "               \n",
    "                for w in words:\n",
    "                    if w not in stopWords:\n",
    "                        filtered_Sentence.append(w)\n",
    "            print(\"Filtered Sentence-->\",filtered_Sentence)\n",
    "                \n",
    "\n",
    "        break;\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Utility functions for Engineering Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Process flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Prepare data **\n",
    "* Load artifacts from object storage and create pandas dataframes\n",
    "* Prepare the pandas dataframes. Add additional columns required for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirements.xlsx\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './/test/Requirements.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ed233fa7233d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mload_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#prepare_artifact_dataframes()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-bcd74ccafd6a>\u001b[0m in \u001b[0;36mload_artifacts\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;31m# we had to use so many variables as we were not able to run python 3 with boto3 get_object()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mrequirements_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrequirements_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrequirements_sheet_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mdomain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdomain_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdomain_sheet_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdataelements_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataelements_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataelements_sheet_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, skiprows, skip_footer, index_col, names, usecols, parse_dates, date_parser, na_values, thousands, convert_float, converters, dtype, true_values, false_values, engine, squeeze, **kwds)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     return io._parse_excel(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, **kwds)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './/test/Requirements.xlsx'"
     ]
    }
   ],
   "source": [
    "load_artifacts()\n",
    "#prepare_artifact_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Run Spacy Text Classifier on data **\n",
    "* Add the text classification output to the artifact dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_column_name = \"ClassifiedText\"\n",
    "#defects_df = add_text_classifier_output(defects_df,config, output_column_name)\n",
    "#testcases_df = add_text_classifier_output(testcases_df,config, output_column_name)\n",
    "#requirements_df = add_text_classifier_output(requirements_df,config, output_column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Populate keywords and entities **\n",
    "* Add the keywords and entities extracted from the unstructured text to the artifact dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Correlate keywords between artifacts **\n",
    "* Add the text similarity score of associated artifacts to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Utility functions to store entities and relations in Orient DB **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Transform results for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Expose integration point with a websocket client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Start websocket client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_websocket_listener()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
